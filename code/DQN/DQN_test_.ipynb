{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "np.random.seed(3)\n",
    "tf.set_random_seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../cs282-f17-xuefeng-yi-david/data/train_scaled_encoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../cs282-f17-xuefeng-yi-david/data/test_scaled_encoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define an action mapping - how to get an id representing the action from the (iv, vaso) tuple\n",
    "action_map = {}\n",
    "count = 0\n",
    "for iv in range(5):\n",
    "    for vaso in range(5):\n",
    "        action_map[(iv, vaso)] = count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obser2traj(df):\n",
    "    trajectory = []\n",
    "    for i, obser in df.iterrows():\n",
    "        if i + 1 == df.shape[0]:\n",
    "            break\n",
    "        if obser['icustayid'] == df.loc[i+1, 'icustayid']:\n",
    "            next_s = df.iloc[i+1, :200]\n",
    "            r = df.loc[i+1, 'reward']\n",
    "            iv, vaso = obser['iv_input'], obser['vaso_input']\n",
    "            a = action_map[ (iv, vaso) ]\n",
    "            trajectory += [ obser[:200].tolist() + [a, r] + next_s.tolist() ]\n",
    "    return np.array(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>vaso_input</th>\n",
       "      <th>iv_input</th>\n",
       "      <th>reward</th>\n",
       "      <th>icustayid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029027</td>\n",
       "      <td>0.061212</td>\n",
       "      <td>0.224086</td>\n",
       "      <td>0.079448</td>\n",
       "      <td>0.052524</td>\n",
       "      <td>0.077151</td>\n",
       "      <td>0.103817</td>\n",
       "      <td>0.063714</td>\n",
       "      <td>0.032102</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083823</td>\n",
       "      <td>0.053838</td>\n",
       "      <td>0.081020</td>\n",
       "      <td>0.057297</td>\n",
       "      <td>0.030372</td>\n",
       "      <td>0.047363</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.031518</td>\n",
       "      <td>0.078677</td>\n",
       "      <td>0.120369</td>\n",
       "      <td>0.040228</td>\n",
       "      <td>0.047623</td>\n",
       "      <td>0.104624</td>\n",
       "      <td>0.093498</td>\n",
       "      <td>0.073046</td>\n",
       "      <td>0.035958</td>\n",
       "      <td>0.056553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088714</td>\n",
       "      <td>0.073047</td>\n",
       "      <td>0.083202</td>\n",
       "      <td>0.043883</td>\n",
       "      <td>0.036930</td>\n",
       "      <td>0.057726</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.030942</td>\n",
       "      <td>0.083416</td>\n",
       "      <td>0.106333</td>\n",
       "      <td>0.040481</td>\n",
       "      <td>0.039120</td>\n",
       "      <td>0.098819</td>\n",
       "      <td>0.077658</td>\n",
       "      <td>0.074696</td>\n",
       "      <td>0.041053</td>\n",
       "      <td>0.053553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092754</td>\n",
       "      <td>0.079161</td>\n",
       "      <td>0.082560</td>\n",
       "      <td>0.041910</td>\n",
       "      <td>0.036587</td>\n",
       "      <td>0.067986</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.042970</td>\n",
       "      <td>0.050963</td>\n",
       "      <td>0.090261</td>\n",
       "      <td>0.093201</td>\n",
       "      <td>0.031332</td>\n",
       "      <td>0.057674</td>\n",
       "      <td>0.032040</td>\n",
       "      <td>0.054802</td>\n",
       "      <td>0.061447</td>\n",
       "      <td>0.053960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098049</td>\n",
       "      <td>0.074196</td>\n",
       "      <td>0.088113</td>\n",
       "      <td>0.045784</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>0.129189</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064572</td>\n",
       "      <td>0.084793</td>\n",
       "      <td>0.065976</td>\n",
       "      <td>0.070753</td>\n",
       "      <td>0.063990</td>\n",
       "      <td>0.051591</td>\n",
       "      <td>0.050112</td>\n",
       "      <td>0.049624</td>\n",
       "      <td>0.092619</td>\n",
       "      <td>0.080058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032011</td>\n",
       "      <td>0.081824</td>\n",
       "      <td>0.037519</td>\n",
       "      <td>0.061251</td>\n",
       "      <td>0.071471</td>\n",
       "      <td>0.039540</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.029027  0.061212  0.224086  0.079448  0.052524  0.077151  0.103817   \n",
       "1  0.031518  0.078677  0.120369  0.040228  0.047623  0.104624  0.093498   \n",
       "2  0.030942  0.083416  0.106333  0.040481  0.039120  0.098819  0.077658   \n",
       "3  0.042970  0.050963  0.090261  0.093201  0.031332  0.057674  0.032040   \n",
       "4  0.064572  0.084793  0.065976  0.070753  0.063990  0.051591  0.050112   \n",
       "\n",
       "          7         8         9    ...           194       195       196  \\\n",
       "0  0.063714  0.032102  0.042459    ...      0.083823  0.053838  0.081020   \n",
       "1  0.073046  0.035958  0.056553    ...      0.088714  0.073047  0.083202   \n",
       "2  0.074696  0.041053  0.053553    ...      0.092754  0.079161  0.082560   \n",
       "3  0.054802  0.061447  0.053960    ...      0.098049  0.074196  0.088113   \n",
       "4  0.049624  0.092619  0.080058    ...      0.032011  0.081824  0.037519   \n",
       "\n",
       "        197       198       199  vaso_input  iv_input  reward  icustayid  \n",
       "0  0.057297  0.030372  0.047363           0       0.0     0.0       12.0  \n",
       "1  0.043883  0.036930  0.057726           0       0.0     0.0       12.0  \n",
       "2  0.041910  0.036587  0.067986           0       0.0     0.0       12.0  \n",
       "3  0.045784  0.041096  0.129189           0       0.0    15.0       12.0  \n",
       "4  0.061251  0.071471  0.039540           0       4.0     0.0       14.0  \n",
       "\n",
       "[5 rows x 204 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_trajectory = obser2traj(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_trajectory = obser2traj(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "      <th>400</th>\n",
       "      <th>401</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029027</td>\n",
       "      <td>0.061212</td>\n",
       "      <td>0.224086</td>\n",
       "      <td>0.079448</td>\n",
       "      <td>0.052524</td>\n",
       "      <td>0.077151</td>\n",
       "      <td>0.103817</td>\n",
       "      <td>0.063714</td>\n",
       "      <td>0.032102</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039969</td>\n",
       "      <td>0.095765</td>\n",
       "      <td>0.042694</td>\n",
       "      <td>0.056065</td>\n",
       "      <td>0.088714</td>\n",
       "      <td>0.073047</td>\n",
       "      <td>0.083202</td>\n",
       "      <td>0.043883</td>\n",
       "      <td>0.036930</td>\n",
       "      <td>0.057726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.031518</td>\n",
       "      <td>0.078677</td>\n",
       "      <td>0.120369</td>\n",
       "      <td>0.040228</td>\n",
       "      <td>0.047623</td>\n",
       "      <td>0.104624</td>\n",
       "      <td>0.093498</td>\n",
       "      <td>0.073046</td>\n",
       "      <td>0.035958</td>\n",
       "      <td>0.056553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036153</td>\n",
       "      <td>0.091474</td>\n",
       "      <td>0.040516</td>\n",
       "      <td>0.047595</td>\n",
       "      <td>0.092754</td>\n",
       "      <td>0.079161</td>\n",
       "      <td>0.082560</td>\n",
       "      <td>0.041910</td>\n",
       "      <td>0.036587</td>\n",
       "      <td>0.067986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.030942</td>\n",
       "      <td>0.083416</td>\n",
       "      <td>0.106333</td>\n",
       "      <td>0.040481</td>\n",
       "      <td>0.039120</td>\n",
       "      <td>0.098819</td>\n",
       "      <td>0.077658</td>\n",
       "      <td>0.074696</td>\n",
       "      <td>0.041053</td>\n",
       "      <td>0.053553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039244</td>\n",
       "      <td>0.076928</td>\n",
       "      <td>0.035620</td>\n",
       "      <td>0.049354</td>\n",
       "      <td>0.098049</td>\n",
       "      <td>0.074196</td>\n",
       "      <td>0.088113</td>\n",
       "      <td>0.045784</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>0.129189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.064572</td>\n",
       "      <td>0.084793</td>\n",
       "      <td>0.065976</td>\n",
       "      <td>0.070753</td>\n",
       "      <td>0.063990</td>\n",
       "      <td>0.051591</td>\n",
       "      <td>0.050112</td>\n",
       "      <td>0.049624</td>\n",
       "      <td>0.092619</td>\n",
       "      <td>0.080058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064808</td>\n",
       "      <td>0.046426</td>\n",
       "      <td>0.089100</td>\n",
       "      <td>0.049091</td>\n",
       "      <td>0.033054</td>\n",
       "      <td>0.075375</td>\n",
       "      <td>0.040486</td>\n",
       "      <td>0.053812</td>\n",
       "      <td>0.067138</td>\n",
       "      <td>0.043424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.074025</td>\n",
       "      <td>0.079019</td>\n",
       "      <td>0.069797</td>\n",
       "      <td>0.068576</td>\n",
       "      <td>0.069831</td>\n",
       "      <td>0.043737</td>\n",
       "      <td>0.056884</td>\n",
       "      <td>0.073837</td>\n",
       "      <td>0.073740</td>\n",
       "      <td>0.075134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061861</td>\n",
       "      <td>0.046455</td>\n",
       "      <td>0.084397</td>\n",
       "      <td>0.048288</td>\n",
       "      <td>0.033664</td>\n",
       "      <td>0.081671</td>\n",
       "      <td>0.043017</td>\n",
       "      <td>0.054578</td>\n",
       "      <td>0.068782</td>\n",
       "      <td>0.045069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 402 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.029027  0.061212  0.224086  0.079448  0.052524  0.077151  0.103817   \n",
       "1  0.031518  0.078677  0.120369  0.040228  0.047623  0.104624  0.093498   \n",
       "2  0.030942  0.083416  0.106333  0.040481  0.039120  0.098819  0.077658   \n",
       "3  0.064572  0.084793  0.065976  0.070753  0.063990  0.051591  0.050112   \n",
       "4  0.074025  0.079019  0.069797  0.068576  0.069831  0.043737  0.056884   \n",
       "\n",
       "        7         8         9      ...          392       393       394  \\\n",
       "0  0.063714  0.032102  0.042459    ...     0.039969  0.095765  0.042694   \n",
       "1  0.073046  0.035958  0.056553    ...     0.036153  0.091474  0.040516   \n",
       "2  0.074696  0.041053  0.053553    ...     0.039244  0.076928  0.035620   \n",
       "3  0.049624  0.092619  0.080058    ...     0.064808  0.046426  0.089100   \n",
       "4  0.073837  0.073740  0.075134    ...     0.061861  0.046455  0.084397   \n",
       "\n",
       "        395       396       397       398       399       400       401  \n",
       "0  0.056065  0.088714  0.073047  0.083202  0.043883  0.036930  0.057726  \n",
       "1  0.047595  0.092754  0.079161  0.082560  0.041910  0.036587  0.067986  \n",
       "2  0.049354  0.098049  0.074196  0.088113  0.045784  0.041096  0.129189  \n",
       "3  0.049091  0.033054  0.075375  0.040486  0.053812  0.067138  0.043424  \n",
       "4  0.048288  0.033664  0.081671  0.043017  0.054578  0.068782  0.045069  \n",
       "\n",
       "[5 rows x 402 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_trajectory).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "      <th>400</th>\n",
       "      <th>401</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.044787</td>\n",
       "      <td>0.024944</td>\n",
       "      <td>0.059501</td>\n",
       "      <td>0.186372</td>\n",
       "      <td>0.099174</td>\n",
       "      <td>0.010116</td>\n",
       "      <td>0.015910</td>\n",
       "      <td>0.031792</td>\n",
       "      <td>0.063170</td>\n",
       "      <td>0.050696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021701</td>\n",
       "      <td>0.038107</td>\n",
       "      <td>0.061600</td>\n",
       "      <td>0.108691</td>\n",
       "      <td>0.037210</td>\n",
       "      <td>0.051388</td>\n",
       "      <td>0.044145</td>\n",
       "      <td>0.039312</td>\n",
       "      <td>0.096143</td>\n",
       "      <td>0.013473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.048257</td>\n",
       "      <td>0.029508</td>\n",
       "      <td>0.033046</td>\n",
       "      <td>0.102542</td>\n",
       "      <td>0.107019</td>\n",
       "      <td>0.015018</td>\n",
       "      <td>0.014971</td>\n",
       "      <td>0.034197</td>\n",
       "      <td>0.057205</td>\n",
       "      <td>0.072662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022810</td>\n",
       "      <td>0.041783</td>\n",
       "      <td>0.065208</td>\n",
       "      <td>0.082646</td>\n",
       "      <td>0.037256</td>\n",
       "      <td>0.055131</td>\n",
       "      <td>0.042269</td>\n",
       "      <td>0.039004</td>\n",
       "      <td>0.103266</td>\n",
       "      <td>0.015244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.041906</td>\n",
       "      <td>0.028968</td>\n",
       "      <td>0.028705</td>\n",
       "      <td>0.111719</td>\n",
       "      <td>0.098205</td>\n",
       "      <td>0.013579</td>\n",
       "      <td>0.012648</td>\n",
       "      <td>0.027884</td>\n",
       "      <td>0.074506</td>\n",
       "      <td>0.081935</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020972</td>\n",
       "      <td>0.042322</td>\n",
       "      <td>0.064329</td>\n",
       "      <td>0.083004</td>\n",
       "      <td>0.037022</td>\n",
       "      <td>0.057600</td>\n",
       "      <td>0.039294</td>\n",
       "      <td>0.038362</td>\n",
       "      <td>0.117384</td>\n",
       "      <td>0.015660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.040383</td>\n",
       "      <td>0.029019</td>\n",
       "      <td>0.029167</td>\n",
       "      <td>0.131740</td>\n",
       "      <td>0.099840</td>\n",
       "      <td>0.013389</td>\n",
       "      <td>0.012455</td>\n",
       "      <td>0.028260</td>\n",
       "      <td>0.072425</td>\n",
       "      <td>0.088626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021475</td>\n",
       "      <td>0.043898</td>\n",
       "      <td>0.068749</td>\n",
       "      <td>0.089298</td>\n",
       "      <td>0.036816</td>\n",
       "      <td>0.058196</td>\n",
       "      <td>0.038383</td>\n",
       "      <td>0.038041</td>\n",
       "      <td>0.123431</td>\n",
       "      <td>0.014925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.045685</td>\n",
       "      <td>0.029040</td>\n",
       "      <td>0.027817</td>\n",
       "      <td>0.137894</td>\n",
       "      <td>0.106901</td>\n",
       "      <td>0.013968</td>\n",
       "      <td>0.012697</td>\n",
       "      <td>0.028393</td>\n",
       "      <td>0.073113</td>\n",
       "      <td>0.088150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020937</td>\n",
       "      <td>0.044473</td>\n",
       "      <td>0.070230</td>\n",
       "      <td>0.085311</td>\n",
       "      <td>0.037636</td>\n",
       "      <td>0.060717</td>\n",
       "      <td>0.035866</td>\n",
       "      <td>0.037669</td>\n",
       "      <td>0.124448</td>\n",
       "      <td>0.015627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 402 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.044787  0.024944  0.059501  0.186372  0.099174  0.010116  0.015910   \n",
       "1  0.048257  0.029508  0.033046  0.102542  0.107019  0.015018  0.014971   \n",
       "2  0.041906  0.028968  0.028705  0.111719  0.098205  0.013579  0.012648   \n",
       "3  0.040383  0.029019  0.029167  0.131740  0.099840  0.013389  0.012455   \n",
       "4  0.045685  0.029040  0.027817  0.137894  0.106901  0.013968  0.012697   \n",
       "\n",
       "        7         8         9      ...          392       393       394  \\\n",
       "0  0.031792  0.063170  0.050696    ...     0.021701  0.038107  0.061600   \n",
       "1  0.034197  0.057205  0.072662    ...     0.022810  0.041783  0.065208   \n",
       "2  0.027884  0.074506  0.081935    ...     0.020972  0.042322  0.064329   \n",
       "3  0.028260  0.072425  0.088626    ...     0.021475  0.043898  0.068749   \n",
       "4  0.028393  0.073113  0.088150    ...     0.020937  0.044473  0.070230   \n",
       "\n",
       "        395       396       397       398       399       400       401  \n",
       "0  0.108691  0.037210  0.051388  0.044145  0.039312  0.096143  0.013473  \n",
       "1  0.082646  0.037256  0.055131  0.042269  0.039004  0.103266  0.015244  \n",
       "2  0.083004  0.037022  0.057600  0.039294  0.038362  0.117384  0.015660  \n",
       "3  0.089298  0.036816  0.058196  0.038383  0.038041  0.123431  0.014925  \n",
       "4  0.085311  0.037636  0.060717  0.035866  0.037669  0.124448  0.015627  \n",
       "\n",
       "[5 rows x 402 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_trajectory).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SumTree:\n",
    "    data_pointer = 0\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity  # for all priority values\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)  # for all transitions\n",
    "    \n",
    "    def add(self, p, data):\n",
    "        tree_idx = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data  # update data_frame\n",
    "        self.update(tree_idx, p)  # update tree_frame\n",
    "\n",
    "        self.data_pointer += 1\n",
    "        if self.data_pointer >= self.capacity:  # replace when exceed the capacity\n",
    "            self.data_pointer = 0\n",
    "\n",
    "    def update(self, tree_idx, p):\n",
    "        change = p - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = p\n",
    "        # then propagate the change through tree\n",
    "        while tree_idx != 0:    # this method is faster than the recursive loop in the reference code\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_leaf(self, v):\n",
    "        \"\"\"\n",
    "        Tree structure and array storage:\n",
    "        Tree index:\n",
    "             0         -> storing priority sum\n",
    "            / \\\n",
    "          1     2\n",
    "         / \\   / \\\n",
    "        3   4 5   6    -> storing priority for transitions\n",
    "        Array type for storing:\n",
    "        [0,1,2,3,4,5,6]\n",
    "        \"\"\"\n",
    "        parent_idx = 0\n",
    "        while True:     # the while loop is faster than the method in the reference code\n",
    "            cl_idx = 2 * parent_idx + 1         # this leaf's left and right kids\n",
    "            cr_idx = cl_idx + 1\n",
    "            if cl_idx >= len(self.tree):        # reach bottom, end search\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            else:       # downward search, always search for a higher priority node\n",
    "                if v <= self.tree[cl_idx]:\n",
    "                    parent_idx = cl_idx\n",
    "                else:\n",
    "                    v -= self.tree[cl_idx]\n",
    "                    parent_idx = cr_idx\n",
    "\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
    "\n",
    "    @property\n",
    "    def total_p(self):\n",
    "        return self.tree[0]  # the root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    \"\"\"\n",
    "    epsilon = 0.01  # small amount to avoid zero priority\n",
    "    alpha = 0.6  # [0~1] convert the importance of TD error to priority\n",
    "    beta = 0.9  # importance-sampling, from initial value increasing to 1\n",
    "    beta_increment_per_sampling = 0.001\n",
    "    abs_err_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    def store(self, transition):\n",
    "        max_p = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        if max_p == 0:\n",
    "            max_p = self.abs_err_upper\n",
    "        self.tree.add(max_p, transition)   # set the max p for new p\n",
    "\n",
    "    def sample(self, n):\n",
    "        b_idx, b_memory, ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, self.tree.data[0].size)), np.empty((n, 1))\n",
    "        pri_seg = self.tree.total_p / n       # priority segment\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])  # max = 1\n",
    "\n",
    "        min_prob = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_p     # for later calculate ISweight\n",
    "        for i in range(n):\n",
    "            a, b = pri_seg * i, pri_seg * (i + 1)\n",
    "            v = np.random.uniform(a, b)\n",
    "            idx, p, data = self.tree.get_leaf(v)\n",
    "            prob = p / self.tree.total_p\n",
    "            ISWeights[i, 0] = np.power(prob/min_prob, -self.beta)\n",
    "            b_idx[i], b_memory[i, :] = idx, data\n",
    "        return b_idx, b_memory, ISWeights\n",
    "\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.epsilon  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.abs_err_upper)\n",
    "        ps = np.power(clipped_errors, self.alpha)\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_path = \"./dqn_test/xavier2.5m/ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class DQN:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions,\n",
    "        num_features,\n",
    "        hidden_units = 128,\n",
    "        learning_rate=0.0001,\n",
    "        reward_discount = 0.99,\n",
    "        epsilon_greedy = 0.9,\n",
    "        lamda = 5,\n",
    "        e_increment = None,\n",
    "        replace_target_iter = 200,\n",
    "        mem_size = 2000,\n",
    "        batch_size = 30,\n",
    "        Rmax = 15,\n",
    "        dueling = False,\n",
    "        prioritized = True,\n",
    "        output_graph = False,\n",
    "        output_path = None,\n",
    "        sess = None\n",
    "    ):\n",
    "        # network layer sizes\n",
    "        self.num_actions, self.num_features = num_actions, num_features\n",
    "        self.hidden_units = hidden_units\n",
    "        # training \n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_discount\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        self.memory_size = mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.lamda = lamda\n",
    "        self.phase = True\n",
    "        # epsilon\n",
    "        self.e_increment = e_increment\n",
    "        self.e_max = epsilon_greedy\n",
    "        self.epsilon = 0 if e_increment else (1 - self.e_max)\n",
    "        \n",
    "        self.dueling = dueling\n",
    "        self.prioritized = prioritized\n",
    "        self.learn_step_counter = 0\n",
    "        # reward\n",
    "        self.rmax = Rmax\n",
    "        \n",
    "        # s, a, r ,s_\n",
    "        if self.prioritized:\n",
    "            self.memory = Memory(capacity=self.memory_size)\n",
    "        else:\n",
    "            self.memory = np.zeros((self.memory_size, 2 * self.num_features + self.num_actions + 1))\n",
    "        \n",
    "        #build net\n",
    "        self.__build_net__()\n",
    "        # sync target network\n",
    "        self.sync_target_op = [tf.assign(t, e) for t, e in zip(tf.get_collection('target_net_params'), \\\n",
    "                                                               tf.get_collection('eval_net_params'))]\n",
    "        \n",
    "        if not sess:\n",
    "            self.sess = tf.Session()\n",
    "        else:\n",
    "            self.sess = sess\n",
    "        \n",
    "        if output_graph:\n",
    "            if not output_path: output_path = ''\n",
    "            else: output_path += '/'\n",
    "            self.writer = tf.summary.FileWriter(\"logs/\" + output_path, self.sess.graph)\n",
    "        \n",
    "        self.saver = tf.train.Saver(tf.global_variables())\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def __build_net__(self):\n",
    "        \n",
    "        def build_layers(s, c_names, summary=True):\n",
    "            # use xavier_initializer with normal distribution\n",
    "            w_init, b_init = tf.contrib.layers.xavier_initializer(), tf.zeros_initializer()\n",
    "            # tf.random_normal_initializer(0., 0.3)\n",
    "            # tf.contrib.layers.xavier_initializer()\n",
    "            # tf.contrib.layers.xavier_initializer(uniform=False)\n",
    "            # inpyt layer\n",
    "            with tf.variable_scope('l1'):\n",
    "                w1 = tf.get_variable('w1', [self.num_features, self.hidden_units], initializer=w_init, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, self.hidden_units], initializer=b_init, collections=c_names)\n",
    "                # l1 = tf.nn.relu(tf.matmul(s, w1) + b1)\n",
    "                l1 = tf.matmul(s, w1) + b1\n",
    "            \n",
    "            with tf.variable_scope('l1_bn_ac'):\n",
    "                l1_bn = tf.layers.batch_normalization(l1, training=self.phase)\n",
    "                l1_ac = tf.maximum(l1_bn, l1_bn * 0.5)\n",
    "            \n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [self.hidden_units, self.hidden_units], initializer=w_init, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.hidden_units], initializer=b_init, collections=c_names)\n",
    "                # l2 = tf.nn.relu(tf.matmul(l1_ac, w2) + b2)\n",
    "                l2 = tf.matmul(l1_ac, w2) + b2\n",
    "            \n",
    "            with tf.variable_scope('l2_bn_ac'):\n",
    "                l2_bn = tf.layers.batch_normalization(l2, training=self.phase)\n",
    "                l2_ac = tf.maximum(l2_bn, l2_bn * 0.5)\n",
    "            \n",
    "            if self.dueling:\n",
    "                # state value\n",
    "                with tf.variable_scope('Value'):\n",
    "                    w3 = tf.get_variable('w3', [self.hidden_units, 1], initializer=w_init, collections=c_names)\n",
    "                    b3 = tf.get_variable('b3', [1, 1], initializer=b_init, collections=c_names)\n",
    "                    self.V = tf.matmul(l2_ac, w3) + b3\n",
    "                # action value\n",
    "                with tf.variable_scope('Advantage'):\n",
    "                    w3 = tf.get_variable('w3', [self.hidden_units, self.num_actions], initializer=w_init, collections=c_names)\n",
    "                    b3 = tf.get_variable('b3', [1, self.num_actions], initializer=b_init, collections=c_names)\n",
    "                    self.A = tf.matmul(l2_ac, w3) + b3\n",
    "                # output Q value layer\n",
    "                with tf.variable_scope('Q'):\n",
    "                    # Q = V(s) + A(s,a)\n",
    "                    out = self.V + (self.A - tf.reduce_mean(self.A, axis=1, keep_dims=True))\n",
    "            else:\n",
    "                # output Q value layer\n",
    "                with tf.variable_scope('Q'):\n",
    "                    w3 = tf.get_variable('w3', [self.hidden_units, self.num_actions], initializer=w_init, collections=c_names)\n",
    "                    b3 = tf.get_variable('b3', [1, self.num_actions], initializer=b_init, collections=c_names)\n",
    "                    out = tf.matmul(l2_ac, w3) + b3\n",
    "            \n",
    "            return out\n",
    "        \n",
    "        ## ------------------ build evaluate_net ------------------\n",
    "        # input, i.e state\n",
    "        self.s = tf.placeholder(tf.float32, [None, self.num_features], name='s')\n",
    "        self.q_target = tf.placeholder(tf.float32, [None, self.num_actions], name='q_target')\n",
    "        \n",
    "        if self.prioritized:\n",
    "            self.ISWeights = tf.placeholder(tf.float32, [None, 1], name='IS_weights')\n",
    "        \n",
    "        with tf.variable_scope('eval_net'):\n",
    "            self.q_eval = build_layers(self.s, ['eval_net_params', tf.GraphKeys.GLOBAL_VARIABLES])\n",
    "        \n",
    "        with tf.variable_scope('loss'):\n",
    "            \n",
    "            if self.prioritized:\n",
    "                self.abs_errors = tf.reduce_sum(tf.abs(self.q_target - self.q_eval), axis=1)\n",
    "                self.loss = tf.reduce_mean(self.ISWeights * tf.squared_difference(self.q_target, self.q_eval)) + \\\n",
    "                            self.lamda * tf.reduce_sum(tf.maximum(tf.abs(self.q_eval) - self.rmax, 0))\n",
    "\n",
    "            else:\n",
    "                self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval)) + \\\n",
    "                            self.lamda * tf.reduce_sum(tf.maximum(tf.abs(self.q_eval) - self.rmax, 0))\n",
    "        \n",
    "        with tf.variable_scope('train'):\n",
    "            self._train_op = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "        \n",
    "        with tf.variable_scope('predict'):\n",
    "            self.predict = tf.argmax(self.q_eval, 1, name='predict')\n",
    "        \n",
    "        ## ------------------ build target_net ------------------\n",
    "        self.s_ = tf.placeholder(tf.float32, [None, self.num_features], name='s_')\n",
    "        \n",
    "        with tf.variable_scope('target_net'):\n",
    "            self.q_next = build_layers(self.s_, ['target_net_params', tf.GraphKeys.GLOBAL_VARIABLES], summary=False)\n",
    "        \n",
    "        ## ------------------ summary ------------------\n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "        self.merged = tf.summary.merge_all()\n",
    "        \n",
    "    def store_transition(self, histories):\n",
    "        if self.prioritized:\n",
    "            for history in histories:\n",
    "                self.memory.store(history)\n",
    "        else:\n",
    "            self.memory = histories\n",
    "\n",
    "    def learn(self):\n",
    "        self.phase = True\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0 and self.learn_step_counter != 0:\n",
    "            self.sess.run(self.sync_target_op)\n",
    "            # print('target_params_synced')\n",
    "        \n",
    "        if self.prioritized:\n",
    "            tree_idx, batch_memory, ISWeights = self.memory.sample(self.batch_size)\n",
    "        else:\n",
    "            sample_index = np.random.choice(self.memory_size, size=self.batch_size)\n",
    "            batch_memory = self.memory[sample_index, :]\n",
    "        \n",
    "        # next observation\n",
    "        q_next, q_eval = self.sess.run([self.q_next, self.q_eval], \n",
    "                               feed_dict={\n",
    "                                   self.s_: batch_memory[:, -self.num_features:],\n",
    "                                   self.s: batch_memory[:, :self.num_features]})\n",
    "        \n",
    "        q_next[q_next > self.rmax] = self.rmax\n",
    "        q_next[q_next < -self.rmax] = - self.rmax\n",
    "        \n",
    "        q_target = q_eval.copy()\n",
    "        \n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        eval_act_index = batch_memory[:, self.num_features].astype(int)\n",
    "        reward = batch_memory[:, self.num_features + 1]\n",
    "        \n",
    "        q_target[batch_index, eval_act_index] = reward + self.gamma * np.max(q_next, axis=1)\n",
    "        \n",
    "        if self.prioritized:\n",
    "            _, abs_errors, cost, summary = self.sess.run([self._train_op, self.abs_errors, self.loss, self.merged],\n",
    "                                         feed_dict={self.s: batch_memory[:, :self.num_features],\n",
    "                                                    self.q_target: q_target,\n",
    "                                                    self.ISWeights: ISWeights })\n",
    "            self.memory.batch_update(tree_idx, abs_errors) \n",
    "        else:\n",
    "            _, cost, summary = self.sess.run([self._train_op, self.loss, self.merged],\n",
    "                                         feed_dict={self.s: batch_memory[:, :self.num_features],\n",
    "                                                    self.q_target: q_target })\n",
    "        \n",
    "        self.writer.add_summary(summary, self.learn_step_counter)\n",
    "        \n",
    "        if self.learn_step_counter % 1000 == 0 and self.learn_step_counter > 0:\n",
    "            self.saver.save(self.sess, save_path)\n",
    "            print(\"Saved Model, step is \" + str(self.learn_step_counter))\n",
    "        \n",
    "        if self.e_increment:\n",
    "            if self.epsilon < self.e_max:\n",
    "                self.epsilon += self.e_increment \n",
    "            else:\n",
    "                self.epsilon = self.e_max\n",
    "            \n",
    "        self.learn_step_counter += 1\n",
    "        \n",
    "        # physician policy\n",
    "        physcian_actions = eval_act_index\n",
    "        # agent policy\n",
    "        agent_actions = np.argmax(q_eval, axis=1)\n",
    "        \n",
    "        return cost, physcian_actions, agent_actions\n",
    "    \n",
    "    def evaluate(self, val_set):\n",
    "        \n",
    "        self.phase = False\n",
    "        \n",
    "        s, a, r, s_ = val_set[:, :self.num_features], val_set[:, self.num_features].astype(int), \\\n",
    "                      val_set[:, self.num_features + 1], val_set[:, -self.num_features:]\n",
    "        \n",
    "        q_eval = self.sess.run(self.q_eval, {self.s: s})\n",
    "        \n",
    "        agent_actions = np.argmax(q_eval, axis=1)\n",
    "        physician_q = q_eval[range(len(q_eval)), a]\n",
    "        agent_q = q_eval[range(len(q_eval)), agent_actions]\n",
    "        \n",
    "        return a, agent_actions, physician_q, agent_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, histories, num_epoches=10): # val_histories\n",
    "    cost_hist, mean_q = [], []\n",
    "    # shuffle histories\n",
    "    np.random.shuffle(histories)\n",
    "    # insert memory\n",
    "    model.store_transition(histories[:model.memory_size,:])\n",
    "    # learn\n",
    "    for step in range(num_epoches):\n",
    "        loss, phy_actions, agent_actions = model.learn()\n",
    "        if step % 1000 == 0 and step > 0:\n",
    "            print ( 'step:{}, loss:{}'.format(step, loss) )\n",
    "            cost_hist += [ (step, loss) ]\n",
    "            print ( 'physcian actions: ', phy_actions )\n",
    "            print ( 'agent actions: ',  agent_actions )\n",
    "            phy_a, agent_a, physician_q, agent_q = model.evaluate(test_trajectory)\n",
    "            mean_q += [ (step, np.mean(physician_q), np.mean(agent_q)) ]\n",
    "            print ('mean physician Q: ', np.mean(physician_q))\n",
    "            print ('mean agent Q: ', np.mean(agent_q))\n",
    "            print ('----------------------------------')\n",
    "    return phy_a, agent_a, physician_q, agent_q, cost_hist, mean_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    with tf.Session() as sess:\n",
    "        print('Trying to load model...')\n",
    "        try:\n",
    "            restorer = tf.train.import_meta_graph(save_path + '.meta')\n",
    "            restorer.restore(sess, tf.train.latest_checkpoint('./dqn_test/2m'))\n",
    "            print (\"Model restored\")\n",
    "        except IOError:\n",
    "            print (\"No previous model found, running default init\")\n",
    "            sess.run(init)\n",
    "            \n",
    "        dueling_DQN.phase = False\n",
    "        \n",
    "        s, a = x\n",
    "        q_eval = sess.run(dueling_DQN.q_eval, {dueling_DQN.s: s})\n",
    "        \n",
    "        agent_actions = np.argmax(q_eval, axis=1)\n",
    "        physician_q = q_eval[range(len(q_eval)), a]\n",
    "        agent_q = q_eval[range(len(q_eval)), agent_actions]\n",
    "    \n",
    "    return a, agent_actions, physician_q, agent_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### main\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tf.set_random_seed(7)\n",
    "np.random.seed(7)\n",
    "\n",
    "MEMORY_SIZE = train_trajectory.shape[0]\n",
    "ACTION_SPACE = 25\n",
    "FEATURES = 200\n",
    "NUM_EPOCHES = 2500000\n",
    "\n",
    "sess = tf.Session()\n",
    "## speicify output_path while tunning parameters\n",
    "with tf.variable_scope('dueling'):\n",
    "    dueling_DQN = DQN(\n",
    "        num_actions=ACTION_SPACE, num_features=FEATURES, batch_size=32, mem_size=MEMORY_SIZE,\n",
    "        dueling=True, output_graph=True)\n",
    "    \n",
    "sess.run(tf.global_variables_initializer())\n",
    "phy_actions, agent_actions, physician_q, agent_q, cost_hist, mean_q = \\\n",
    "    train(dueling_DQN, train_trajectory, num_epoches=NUM_EPOCHES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sa_seq(df):\n",
    "    s = df.values[:,:200]\n",
    "    vasos, ivs =  df.values[:,200],  df.values[:,201]\n",
    "    a = []\n",
    "    for vaso, iv in zip(vasos, ivs):\n",
    "        a += [action_map[ (iv, vaso) ]]\n",
    "    return s, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_s, test_a = get_sa_seq(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load model...\n",
      "INFO:tensorflow:Restoring parameters from ./dqn_test/2m/ckpt\n",
      "Model restored\n"
     ]
    }
   ],
   "source": [
    "test_a, test_agent_actions, test_physician_q, test_agent_q = predict((test_s, test_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.4440169, 8.2226715)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(test_physician_q), np.mean(test_agent_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x131403e80>"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x131456438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(test_agent_actions).hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_s, train_a = get_sa_seq(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load model...\n",
      "INFO:tensorflow:Restoring parameters from ./dqn_test/2m/ckpt\n",
      "Model restored\n"
     ]
    }
   ],
   "source": [
    "train_a, train_agent_actions, train_physician_q, train_agent_q = predict((train_s, train_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.5419569, 8.2002687)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(train_physician_q), np.mean(train_agent_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x131725d30>"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1317790b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(train_agent_actions).hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(train_agent_actions, open('results/with_end_state/train_agent_actions_2m.pkl', 'wb'))\n",
    "pkl.dump(test_agent_actions, open('results/with_end_state/test_agent_actions_2m.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
