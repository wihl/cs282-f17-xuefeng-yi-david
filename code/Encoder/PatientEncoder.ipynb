{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../utils')\n",
    "\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, LSTM, RepeatVector, Flatten, Masking\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import discretize_sepsis_actions as discretizer\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PatientRecordProcessor:\n",
    "    columns = ['bloc','icustayid','charttime', 'gender', 'age', 'elixhauser',\n",
    "                're_admission', 'SOFA', 'SIRS', 'Weight_kg', 'GCS', 'HR',\n",
    "                'SysBP', 'MeanBP', 'DiaBP', 'Shock_Index', 'RR', 'SpO2',\n",
    "                'Temp_C', 'FiO2_1', 'Potassium', 'Sodium', 'Chloride',\n",
    "                'Glucose', 'BUN', 'Creatinine', 'Magnesium', 'Calcium',\n",
    "                'Ionised_Ca', 'CO2_mEqL', 'SGOT', 'SGPT', 'Total_bili',\n",
    "                'Albumin', 'Hb', 'WBC_count', 'Platelets_count', 'PTT',\n",
    "                'PT', 'INR', 'Arterial_pH', 'paO2', 'paCO2',\n",
    "                'Arterial_BE', 'Arterial_lactate', 'HCO3', 'PaO2_FiO2',\n",
    "                'median_dose_vaso', 'max_dose_vaso', 'input_total_tev',\n",
    "                'input_4hourly_tev', 'output_total', 'output_4hourly',\n",
    "                'cumulated_balance_tev', 'sedation', 'mechvent', 'rrt',\n",
    "                'died_in_hosp', 'mortality_90d']\n",
    "\n",
    "    observ_cols = ['gender', 'age','elixhauser','re_admission', 'SOFA', 'SIRS', 'Weight_kg', 'GCS', 'HR',\n",
    "                'SysBP', 'MeanBP', 'DiaBP', 'RR', 'SpO2',\n",
    "                'Temp_C', 'FiO2_1', 'Potassium', 'Sodium', 'Chloride',\n",
    "                'Glucose', 'BUN', 'Creatinine', 'Magnesium', 'Calcium',\n",
    "                'Ionised_Ca', 'CO2_mEqL', 'SGOT', 'SGPT', 'Total_bili',\n",
    "                'Albumin', 'Hb', 'WBC_count', 'Platelets_count', 'PTT',\n",
    "                'PT', 'INR', 'Arterial_pH', 'paO2', 'paCO2',\n",
    "                'Arterial_BE', 'Arterial_lactate', 'HCO3', 'PaO2_FiO2',\n",
    "                'output_total', 'output_4hourly']\n",
    "    \n",
    "    action_observ_cols = ['gender', 'age','elixhauser','re_admission', 'SOFA', 'SIRS', 'Weight_kg', 'GCS', 'HR',\n",
    "                'SysBP', 'MeanBP', 'DiaBP', 'RR', 'SpO2',\n",
    "                'Temp_C', 'FiO2_1', 'Potassium', 'Sodium', 'Chloride',\n",
    "                'Glucose', 'BUN', 'Creatinine', 'Magnesium', 'Calcium',\n",
    "                'Ionised_Ca', 'CO2_mEqL', 'SGOT', 'SGPT', 'Total_bili',\n",
    "                'Albumin', 'Hb', 'WBC_count', 'Platelets_count', 'PTT',\n",
    "                'PT', 'INR', 'Arterial_pH', 'paO2', 'paCO2',\n",
    "                'Arterial_BE', 'Arterial_lactate', 'HCO3', 'PaO2_FiO2',\n",
    "                'output_total', 'output_4hourly',\n",
    "                'sedation','median_dose_vaso', 'max_dose_vaso', 'input_total_tev',\n",
    "                'input_4hourly_tev','sedation', 'mechvent', 'rrt']\n",
    "    \n",
    "    n_clusters = 2000\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.patient_map = None\n",
    "        self.LONGEST_STAY = 20 \n",
    "        self.OBSER_LEN = 45\n",
    "    \n",
    "    def load_csv(self, raw_path, cluster_path):\n",
    "        print ( 'loading dataset ...' )\n",
    "        self.df = pd.read_csv(raw_path)\n",
    "        print ( 'loading clustered states ...' )\n",
    "        self.clusters = pkl.load(open(cluster_path, 'rb'), encoding='latin1')\n",
    "        print ( 'discretizing actions ...' )\n",
    "        self.discretize_actions()\n",
    "        print ( 'initialization succeeded' )\n",
    "    \n",
    "    def discretize_actions(self):\n",
    "        self.action_sequence, self.vaso_bins, self.iv_bins = \\\n",
    "        discretizer.discretize_actions(self.df.loc[:,'input_4hourly_tev'],\n",
    "                                       self.df.loc[:,'median_dose_vaso'])\n",
    "    def build_patient_map(self):\n",
    "        self.patient_map = {}\n",
    "        for i, row in self.df.iterrows():\n",
    "            icuid = str(row['icustayid'])\n",
    "            state_action_outcome = [self.clusters[i], self.action_sequence[i],\n",
    "                                    row['input_4hourly_tev'],\n",
    "                                    row['median_dose_vaso'],\n",
    "                                    row['died_in_hosp']]\n",
    "            if icuid not in self.patient_map:\n",
    "                # state_id, action, outcome\n",
    "                self.patient_map[icuid] = {\n",
    "                    'age':row['age'], 'gender':row['gender'], 'sa':[state_action_outcome],\n",
    "                    'obser':[row[self.observ_cols].as_matrix()]}\n",
    "            else:\n",
    "                self.patient_map[icuid]['sa'].append(state_action_outcome)\n",
    "                self.patient_map[icuid]['obser'].append(row[self.observ_cols].as_matrix())\n",
    "\n",
    "        return self.patient_map\n",
    "    # FOR DQN\n",
    "    def build_training_history(self):\n",
    "        memory = []\n",
    "        if not self.patient_map:\n",
    "            print ( 'building patient map ...' )\n",
    "            self.patient_map = self.build_patient_map()\n",
    "\n",
    "        for _, patient in self.patient_map.items():\n",
    "\n",
    "            if len(patient['sa']) <= 5:\n",
    "                continue\n",
    "\n",
    "            for i, patient_icu_stay in enumerate(patient['sa']):\n",
    "                _, action, _, _, outcome = patient_icu_stay\n",
    "                s_obser = patient['obser'][i]\n",
    "                next_s_obser = patient['obser'][i + 1]\n",
    "\n",
    "                reward = 0\n",
    "                if (i + 1) == len(patient['sa']) - 1:\n",
    "                    # last stay, check the outcome\n",
    "                    if patient['sa'][i + 1][-1] == 0:\n",
    "                        # survived\n",
    "                        reward = 15\n",
    "                    else:\n",
    "                        reward = -15\n",
    "\n",
    "                memory.append(np.hstack((s_obser, action, reward, next_s_obser)))\n",
    "\n",
    "                if reward != 0:\n",
    "                    break\n",
    "        return np.array(memory)\n",
    "    \n",
    "    def get_patient_data_seq(self):\n",
    "        # icuids\n",
    "        icuids =  self.df['icustayid'].values\n",
    "        # observation matters\n",
    "        # most of the actions are the same, that's why we diltch them\n",
    "        observations = self.df[self.observ_cols].values\n",
    "\n",
    "        self.patient_map = {}\n",
    "        for i, (icuid, row) in enumerate(zip(icuids, observations)):\n",
    "            if icuid not in self.patient_map:\n",
    "                self.patient_map[icuid] = {}\n",
    "                self.patient_map[icuid]['histories'] = [row]\n",
    "            else:\n",
    "                self.patient_map[icuid]['histories'] += [row]\n",
    "        \n",
    "        return self.patient_map\n",
    "    \n",
    "    def get_patient_data_seq_x(self):\n",
    "        patient_seq = self.get_patient_data_seq()\n",
    "        X = np.zeros((len(patient_seq.keys()), self.LONGEST_STAY, self.OBSER_LEN))\n",
    "        for i, icuid in enumerate(patient_seq.keys()):\n",
    "            for j, hist in enumerate(patient_seq[icuid]['histories']):\n",
    "                X[i, j] = hist\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset ...\n",
      "loading clustered states ...\n",
      "discretizing actions ...\n",
      "initialization succeeded\n"
     ]
    }
   ],
   "source": [
    "# 5% varieity\n",
    "prp = PatientRecordProcessor()\n",
    "raw_csv_path = '../../data/train_scaled.csv'\n",
    "state_path = '../../data/states_list.pkl'\n",
    "prp.load_csv(raw_csv_path, state_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = prp.get_patient_data_seq_x()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset ...\n",
      "loading clustered states ...\n",
      "discretizing actions ...\n",
      "initialization succeeded\n"
     ]
    }
   ],
   "source": [
    "prp_test = PatientRecordProcessor()\n",
    "prp_test.load_csv('../../data/test_scaled.csv', state_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = prp_test.get_patient_data_seq_x()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def r2(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred )) \n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_autoencoder(hidden_size=128):\n",
    "    \n",
    "    # inputs = Input(shape=(prp.LONGEST_STAY, prp.OBSER_LEN))\n",
    "    inputs = Input(shape=(prp.LONGEST_STAY, prp.OBSER_LEN))\n",
    "    masked_input = Masking(mask_value=0.0)(inputs)\n",
    "    \n",
    "    encoded = LSTM(hidden_size, input_shape=(prp.LONGEST_STAY, prp.OBSER_LEN))(masked_input)\n",
    "\n",
    "    decoded = RepeatVector(prp.LONGEST_STAY)(encoded)\n",
    "    decoded = LSTM(prp.OBSER_LEN, return_sequences=True)(decoded)\n",
    "    \n",
    "    sequence_autoencoder = Model(inputs, decoded)\n",
    "    sequence_autoencoder.compile(optimizer='adam', loss='mse', metrics=[r2])\n",
    "    \n",
    "    encoder = Model(inputs, encoded)\n",
    "    \n",
    "    return sequence_autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def two_layer_lstm_autoencoder():\n",
    "    \n",
    "    inputs = Input(shape=(prp.LONGEST_STAY, prp.OBSER_LEN))\n",
    "    \n",
    "    encoded = LSTM(256, input_shape=(prp.LONGEST_STAY, prp.OBSER_LEN), return_sequences=True)(inputs)\n",
    "    encoded = LSTM(128, input_shape=(prp.LONGEST_STAY, 256))(encoded)\n",
    "\n",
    "    decoded = RepeatVector(prp.LONGEST_STAY)(encoded)\n",
    "    decoded = LSTM(prp.OBSER_LEN, return_sequences=True)(decoded)\n",
    "    \n",
    "    sequence_autoencoder = Model(inputs, decoded)\n",
    "    sequence_autoencoder.compile(optimizer='adam', loss='mse', metrics=[r2])\n",
    "    \n",
    "    encoder = Model(inputs, encoded)\n",
    "    \n",
    "    return sequence_autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_autoencoder(autoencoder, x, n_epoches=100, batch_size=128):\n",
    "    x_train, x_val, _, _ = train_test_split(x, x, test_size=0.1, random_state=37)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "    early_Stop = EarlyStopping(monitor='loss',\n",
    "                              min_delta=0,\n",
    "                              patience=5,\n",
    "                              verbose=0, mode='auto')\n",
    "    autoencoder.fit(x_train, x_train,\n",
    "                epochs=n_epoches,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_val, x_val), verbose=1, callbacks=[early_Stop, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder, encoder = lstm_autoencoder(hidden_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder_action, encoder_action = lstm_autoencoder(hidden_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder2l, encoder2l = two_layer_lstm_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10408 samples, validate on 1157 samples\n",
      "Epoch 1/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.5317 - r2: 0.1832 - val_loss: 0.4838 - val_r2: 0.2808\n",
      "Epoch 2/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.4298 - r2: 0.3398 - val_loss: 0.4293 - val_r2: 0.3621\n",
      "Epoch 3/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.3966 - r2: 0.3906 - val_loss: 0.4134 - val_r2: 0.3857\n",
      "Epoch 4/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3858 - r2: 0.4075 - val_loss: 0.4065 - val_r2: 0.3960\n",
      "Epoch 5/500\n",
      "10408/10408 [==============================] - 14s 1ms/step - loss: 0.3801 - r2: 0.4162 - val_loss: 0.4017 - val_r2: 0.4031\n",
      "Epoch 6/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3753 - r2: 0.4236 - val_loss: 0.3969 - val_r2: 0.4103\n",
      "Epoch 7/500\n",
      "10408/10408 [==============================] - 13s 1ms/step - loss: 0.3695 - r2: 0.4329 - val_loss: 0.3909 - val_r2: 0.4194\n",
      "Epoch 8/500\n",
      "10408/10408 [==============================] - 13s 1ms/step - loss: 0.3608 - r2: 0.4465 - val_loss: 0.3797 - val_r2: 0.4362\n",
      "Epoch 9/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3496 - r2: 0.4634 - val_loss: 0.3686 - val_r2: 0.4528\n",
      "Epoch 10/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.3407 - r2: 0.4776 - val_loss: 0.3623 - val_r2: 0.4623\n",
      "Epoch 11/500\n",
      "10408/10408 [==============================] - 13s 1ms/step - loss: 0.3345 - r2: 0.4863 - val_loss: 0.3571 - val_r2: 0.4701\n",
      "Epoch 12/500\n",
      "10408/10408 [==============================] - 13s 1ms/step - loss: 0.3303 - r2: 0.4933 - val_loss: 0.3535 - val_r2: 0.4754\n",
      "Epoch 13/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.3265 - r2: 0.4995 - val_loss: 0.3500 - val_r2: 0.4807\n",
      "Epoch 14/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.3236 - r2: 0.5039 - val_loss: 0.3478 - val_r2: 0.4840\n",
      "Epoch 15/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3211 - r2: 0.5076 - val_loss: 0.3449 - val_r2: 0.4884\n",
      "Epoch 16/500\n",
      "10408/10408 [==============================] - 13s 1ms/step - loss: 0.3190 - r2: 0.5108 - val_loss: 0.3436 - val_r2: 0.4902\n",
      "Epoch 17/500\n",
      "10408/10408 [==============================] - 13s 1ms/step - loss: 0.3173 - r2: 0.5136 - val_loss: 0.3425 - val_r2: 0.4918\n",
      "Epoch 18/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.3151 - r2: 0.5168 - val_loss: 0.3406 - val_r2: 0.4947\n",
      "Epoch 19/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3139 - r2: 0.5190 - val_loss: 0.3395 - val_r2: 0.4964\n",
      "Epoch 20/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.3126 - r2: 0.5208 - val_loss: 0.3380 - val_r2: 0.4985\n",
      "Epoch 21/500\n",
      "10408/10408 [==============================] - 14s 1ms/step - loss: 0.3112 - r2: 0.5231 - val_loss: 0.3375 - val_r2: 0.4993\n",
      "Epoch 22/500\n",
      "10408/10408 [==============================] - 14s 1ms/step - loss: 0.3103 - r2: 0.5239 - val_loss: 0.3355 - val_r2: 0.5023\n",
      "Epoch 23/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3099 - r2: 0.5247 - val_loss: 0.3351 - val_r2: 0.5029\n",
      "Epoch 24/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.3088 - r2: 0.5267 - val_loss: 0.3351 - val_r2: 0.5028\n",
      "Epoch 25/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3085 - r2: 0.5269 - val_loss: 0.3336 - val_r2: 0.5052\n",
      "Epoch 26/500\n",
      "10408/10408 [==============================] - 13s 1ms/step - loss: 0.3072 - r2: 0.5290 - val_loss: 0.3324 - val_r2: 0.5069\n",
      "Epoch 27/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3061 - r2: 0.5316 - val_loss: 0.3317 - val_r2: 0.5079\n",
      "Epoch 28/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3051 - r2: 0.5329 - val_loss: 0.3316 - val_r2: 0.5082\n",
      "Epoch 29/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3051 - r2: 0.5323 - val_loss: 0.3307 - val_r2: 0.5094\n",
      "Epoch 30/500\n",
      "10408/10408 [==============================] - 13s 1ms/step - loss: 0.3053 - r2: 0.5320 - val_loss: 0.3300 - val_r2: 0.5105\n",
      "Epoch 31/500\n",
      "10408/10408 [==============================] - 14s 1ms/step - loss: 0.3037 - r2: 0.5352 - val_loss: 0.3314 - val_r2: 0.5083\n",
      "Epoch 32/500\n",
      "10408/10408 [==============================] - 14s 1ms/step - loss: 0.3037 - r2: 0.5343 - val_loss: 0.3296 - val_r2: 0.5111\n",
      "Epoch 33/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3024 - r2: 0.5370 - val_loss: 0.3286 - val_r2: 0.5126\n",
      "Epoch 34/500\n",
      "10408/10408 [==============================] - 13s 1ms/step - loss: 0.3023 - r2: 0.5364 - val_loss: 0.3278 - val_r2: 0.5137\n",
      "Epoch 35/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3015 - r2: 0.5380 - val_loss: 0.3290 - val_r2: 0.5119\n",
      "Epoch 36/500\n",
      "10408/10408 [==============================] - 14s 1ms/step - loss: 0.3010 - r2: 0.5388 - val_loss: 0.3273 - val_r2: 0.5145\n",
      "Epoch 37/500\n",
      "10408/10408 [==============================] - 13s 1ms/step - loss: 0.3013 - r2: 0.5379 - val_loss: 0.3269 - val_r2: 0.5151\n",
      "Epoch 38/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3007 - r2: 0.5390 - val_loss: 0.3262 - val_r2: 0.5161\n",
      "Epoch 39/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2996 - r2: 0.5411 - val_loss: 0.3250 - val_r2: 0.5179\n",
      "Epoch 40/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2995 - r2: 0.5414 - val_loss: 0.3277 - val_r2: 0.5139\n",
      "Epoch 41/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.3004 - r2: 0.5398 - val_loss: 0.3271 - val_r2: 0.5148\n",
      "Epoch 42/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2990 - r2: 0.5413 - val_loss: 0.3258 - val_r2: 0.5168\n",
      "Epoch 43/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2988 - r2: 0.5430 - val_loss: 0.3245 - val_r2: 0.5188\n",
      "Epoch 44/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2978 - r2: 0.5437 - val_loss: 0.3231 - val_r2: 0.5208\n",
      "Epoch 45/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2975 - r2: 0.5440 - val_loss: 0.3227 - val_r2: 0.5214\n",
      "Epoch 46/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2970 - r2: 0.5450 - val_loss: 0.3233 - val_r2: 0.5205\n",
      "Epoch 47/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2968 - r2: 0.5451 - val_loss: 0.3229 - val_r2: 0.5211\n",
      "Epoch 48/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2966 - r2: 0.5458 - val_loss: 0.3223 - val_r2: 0.5220\n",
      "Epoch 49/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2967 - r2: 0.5460 - val_loss: 0.3242 - val_r2: 0.5192\n",
      "Epoch 50/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2960 - r2: 0.5462 - val_loss: 0.3220 - val_r2: 0.5223\n",
      "Epoch 51/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2957 - r2: 0.5468 - val_loss: 0.3223 - val_r2: 0.5219\n",
      "Epoch 52/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2960 - r2: 0.5464 - val_loss: 0.3217 - val_r2: 0.5228\n",
      "Epoch 53/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2958 - r2: 0.5467 - val_loss: 0.3211 - val_r2: 0.5237\n",
      "Epoch 54/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2951 - r2: 0.5482 - val_loss: 0.3214 - val_r2: 0.5233\n",
      "Epoch 55/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2950 - r2: 0.5474 - val_loss: 0.3205 - val_r2: 0.5247\n",
      "Epoch 56/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2949 - r2: 0.5479 - val_loss: 0.3207 - val_r2: 0.5243\n",
      "Epoch 57/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2944 - r2: 0.5490 - val_loss: 0.3215 - val_r2: 0.5232\n",
      "Epoch 58/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2940 - r2: 0.5494 - val_loss: 0.3194 - val_r2: 0.5263\n",
      "Epoch 59/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2941 - r2: 0.5495 - val_loss: 0.3200 - val_r2: 0.5255\n",
      "Epoch 60/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2941 - r2: 0.5495 - val_loss: 0.3204 - val_r2: 0.5247\n",
      "Epoch 61/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2936 - r2: 0.5503 - val_loss: 0.3199 - val_r2: 0.5255\n",
      "Epoch 62/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2938 - r2: 0.5499 - val_loss: 0.3183 - val_r2: 0.5279\n",
      "Epoch 63/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2935 - r2: 0.5496 - val_loss: 0.3185 - val_r2: 0.5277\n",
      "Epoch 64/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2926 - r2: 0.5515 - val_loss: 0.3184 - val_r2: 0.5278\n",
      "Epoch 65/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2925 - r2: 0.5511 - val_loss: 0.3181 - val_r2: 0.5281\n",
      "Epoch 66/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2921 - r2: 0.5519 - val_loss: 0.3193 - val_r2: 0.5264\n",
      "Epoch 67/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2926 - r2: 0.5517 - val_loss: 0.3182 - val_r2: 0.5281\n",
      "Epoch 68/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2925 - r2: 0.5517 - val_loss: 0.3181 - val_r2: 0.5282\n",
      "Epoch 69/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2920 - r2: 0.5526 - val_loss: 0.3174 - val_r2: 0.5293\n",
      "Epoch 70/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2916 - r2: 0.5536 - val_loss: 0.3184 - val_r2: 0.5278\n",
      "Epoch 71/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2914 - r2: 0.5534 - val_loss: 0.3175 - val_r2: 0.5291\n",
      "Epoch 72/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2910 - r2: 0.5546 - val_loss: 0.3181 - val_r2: 0.5283\n",
      "Epoch 73/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2913 - r2: 0.5539 - val_loss: 0.3180 - val_r2: 0.5284\n",
      "Epoch 74/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2911 - r2: 0.5546 - val_loss: 0.3168 - val_r2: 0.5303\n",
      "Epoch 75/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2910 - r2: 0.5540 - val_loss: 0.3175 - val_r2: 0.5291\n",
      "Epoch 76/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2904 - r2: 0.5552 - val_loss: 0.3167 - val_r2: 0.5304\n",
      "Epoch 77/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2906 - r2: 0.5548 - val_loss: 0.3167 - val_r2: 0.5304\n",
      "Epoch 78/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2904 - r2: 0.5555 - val_loss: 0.3165 - val_r2: 0.5307\n",
      "Epoch 79/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2904 - r2: 0.5550 - val_loss: 0.3161 - val_r2: 0.5313\n",
      "Epoch 80/500\n",
      "10408/10408 [==============================] - 13s 1ms/step - loss: 0.2897 - r2: 0.5566 - val_loss: 0.3160 - val_r2: 0.5313\n",
      "Epoch 81/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2894 - r2: 0.5568 - val_loss: 0.3161 - val_r2: 0.5312\n",
      "Epoch 82/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2896 - r2: 0.5570 - val_loss: 0.3160 - val_r2: 0.5315\n",
      "Epoch 83/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2896 - r2: 0.5563 - val_loss: 0.3153 - val_r2: 0.5325\n",
      "Epoch 84/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2892 - r2: 0.5574 - val_loss: 0.3153 - val_r2: 0.5325\n",
      "Epoch 85/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2893 - r2: 0.5575 - val_loss: 0.3153 - val_r2: 0.5324\n",
      "Epoch 86/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2898 - r2: 0.5561 - val_loss: 0.3152 - val_r2: 0.5325\n",
      "Epoch 87/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2889 - r2: 0.5565 - val_loss: 0.3145 - val_r2: 0.5336\n",
      "Epoch 88/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2884 - r2: 0.5581 - val_loss: 0.3150 - val_r2: 0.5329\n",
      "Epoch 89/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2887 - r2: 0.5578 - val_loss: 0.3149 - val_r2: 0.5330\n",
      "Epoch 90/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2885 - r2: 0.5578 - val_loss: 0.3144 - val_r2: 0.5338\n",
      "Epoch 91/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2886 - r2: 0.5579 - val_loss: 0.3145 - val_r2: 0.5336\n",
      "Epoch 92/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2882 - r2: 0.5586 - val_loss: 0.3145 - val_r2: 0.5337\n",
      "Epoch 93/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2881 - r2: 0.5591 - val_loss: 0.3137 - val_r2: 0.5348\n",
      "Epoch 94/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2878 - r2: 0.5595 - val_loss: 0.3148 - val_r2: 0.5332\n",
      "Epoch 95/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2880 - r2: 0.5593 - val_loss: 0.3136 - val_r2: 0.5349\n",
      "Epoch 96/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2877 - r2: 0.5599 - val_loss: 0.3136 - val_r2: 0.5350\n",
      "Epoch 97/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2876 - r2: 0.5592 - val_loss: 0.3134 - val_r2: 0.5353\n",
      "Epoch 98/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2875 - r2: 0.5599 - val_loss: 0.3142 - val_r2: 0.5340\n",
      "Epoch 99/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2875 - r2: 0.5593 - val_loss: 0.3130 - val_r2: 0.5359\n",
      "Epoch 100/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2869 - r2: 0.5604 - val_loss: 0.3137 - val_r2: 0.5348\n",
      "Epoch 101/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2881 - r2: 0.5589 - val_loss: 0.3133 - val_r2: 0.5354\n",
      "Epoch 102/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2877 - r2: 0.5590 - val_loss: 0.3133 - val_r2: 0.5355\n",
      "Epoch 103/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2870 - r2: 0.5602 - val_loss: 0.3129 - val_r2: 0.5360\n",
      "Epoch 104/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2866 - r2: 0.5610 - val_loss: 0.3132 - val_r2: 0.5355\n",
      "Epoch 105/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2868 - r2: 0.5612 - val_loss: 0.3123 - val_r2: 0.5370\n",
      "Epoch 106/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2861 - r2: 0.5614 - val_loss: 0.3129 - val_r2: 0.5359\n",
      "Epoch 107/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2863 - r2: 0.5614 - val_loss: 0.3122 - val_r2: 0.5371\n",
      "Epoch 108/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2863 - r2: 0.5620 - val_loss: 0.3133 - val_r2: 0.5354\n",
      "Epoch 109/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2863 - r2: 0.5609 - val_loss: 0.3128 - val_r2: 0.5362\n",
      "Epoch 110/500\n",
      "10408/10408 [==============================] - 13s 1ms/step - loss: 0.2860 - r2: 0.5616 - val_loss: 0.3124 - val_r2: 0.5367\n",
      "Epoch 111/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2861 - r2: 0.5618 - val_loss: 0.3123 - val_r2: 0.5368\n",
      "Epoch 112/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2856 - r2: 0.5624 - val_loss: 0.3127 - val_r2: 0.5364\n"
     ]
    }
   ],
   "source": [
    "train_autoencoder(autoencoder, train_x, n_epoches=500) # with action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3885/3885 [==============================] - 3s 693us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.011337238124198972, 0.85497363948453808]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder_no_action.evaluate(test_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10448 samples, validate on 1161 samples\n",
      "Epoch 1/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0587 - r2: 0.2347 - val_loss: 0.0465 - val_r2: 0.3919\n",
      "Epoch 2/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0426 - r2: 0.4446 - val_loss: 0.0398 - val_r2: 0.4786\n",
      "Epoch 3/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0338 - r2: 0.5589 - val_loss: 0.0268 - val_r2: 0.6491\n",
      "Epoch 4/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0237 - r2: 0.6909 - val_loss: 0.0221 - val_r2: 0.7110\n",
      "Epoch 5/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0206 - r2: 0.7319 - val_loss: 0.0195 - val_r2: 0.7447\n",
      "Epoch 6/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0184 - r2: 0.7601 - val_loss: 0.0174 - val_r2: 0.7716\n",
      "Epoch 7/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0171 - r2: 0.7765 - val_loss: 0.0179 - val_r2: 0.7663\n",
      "Epoch 8/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0164 - r2: 0.7868 - val_loss: 0.0161 - val_r2: 0.7891\n",
      "Epoch 9/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0151 - r2: 0.8025 - val_loss: 0.0147 - val_r2: 0.8081\n",
      "Epoch 10/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0146 - r2: 0.8094 - val_loss: 0.0139 - val_r2: 0.8178\n",
      "Epoch 11/500\n",
      "10448/10448 [==============================] - 13s 1ms/step - loss: 0.0143 - r2: 0.8130 - val_loss: 0.0138 - val_r2: 0.8192\n",
      "Epoch 12/500\n",
      "10448/10448 [==============================] - 14s 1ms/step - loss: 0.0137 - r2: 0.8210 - val_loss: 0.0138 - val_r2: 0.8189\n",
      "Epoch 13/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0133 - r2: 0.8261 - val_loss: 0.0131 - val_r2: 0.8288\n",
      "Epoch 14/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0131 - r2: 0.8288 - val_loss: 0.0126 - val_r2: 0.8355\n",
      "Epoch 15/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0126 - r2: 0.8358 - val_loss: 0.0123 - val_r2: 0.8387\n",
      "Epoch 16/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0125 - r2: 0.8365 - val_loss: 0.0125 - val_r2: 0.8365\n",
      "Epoch 17/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0121 - r2: 0.8428 - val_loss: 0.0120 - val_r2: 0.8424\n",
      "Epoch 18/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0117 - r2: 0.8469 - val_loss: 0.0139 - val_r2: 0.8184\n",
      "Epoch 19/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0119 - r2: 0.8449 - val_loss: 0.0121 - val_r2: 0.8420\n",
      "Epoch 20/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0116 - r2: 0.8484 - val_loss: 0.0110 - val_r2: 0.8564\n",
      "Epoch 21/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0112 - r2: 0.8541 - val_loss: 0.0120 - val_r2: 0.8434\n",
      "Epoch 22/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0109 - r2: 0.8576 - val_loss: 0.0114 - val_r2: 0.8510\n",
      "Epoch 23/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0111 - r2: 0.8549 - val_loss: 0.0105 - val_r2: 0.8627\n",
      "Epoch 24/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0106 - r2: 0.8618 - val_loss: 0.0105 - val_r2: 0.8620\n",
      "Epoch 25/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0106 - r2: 0.8619 - val_loss: 0.0102 - val_r2: 0.8661\n",
      "Epoch 26/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0104 - r2: 0.8640 - val_loss: 0.0100 - val_r2: 0.8686\n",
      "Epoch 27/500\n",
      "10448/10448 [==============================] - 13s 1ms/step - loss: 0.0103 - r2: 0.8661 - val_loss: 0.0100 - val_r2: 0.8688\n",
      "Epoch 28/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0100 - r2: 0.8695 - val_loss: 0.0111 - val_r2: 0.8547\n",
      "Epoch 29/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0102 - r2: 0.8674 - val_loss: 0.0097 - val_r2: 0.8731\n",
      "Epoch 30/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0098 - r2: 0.8726 - val_loss: 0.0095 - val_r2: 0.8759\n",
      "Epoch 31/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0097 - r2: 0.8731 - val_loss: 0.0096 - val_r2: 0.8737\n",
      "Epoch 32/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0098 - r2: 0.8718 - val_loss: 0.0107 - val_r2: 0.8599\n",
      "Epoch 33/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0094 - r2: 0.8775 - val_loss: 0.0095 - val_r2: 0.8754\n",
      "Epoch 34/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0094 - r2: 0.8769 - val_loss: 0.0093 - val_r2: 0.8781\n",
      "Epoch 35/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0092 - r2: 0.8802 - val_loss: 0.0092 - val_r2: 0.8802\n",
      "Epoch 36/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0091 - r2: 0.8816 - val_loss: 0.0088 - val_r2: 0.8843\n",
      "Epoch 37/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0091 - r2: 0.8809 - val_loss: 0.0087 - val_r2: 0.8859\n",
      "Epoch 38/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0089 - r2: 0.8835 - val_loss: 0.0094 - val_r2: 0.8773\n",
      "Epoch 39/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0087 - r2: 0.8869 - val_loss: 0.0088 - val_r2: 0.8853\n",
      "Epoch 40/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0089 - r2: 0.8845 - val_loss: 0.0087 - val_r2: 0.8857\n",
      "Epoch 41/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0088 - r2: 0.8851 - val_loss: 0.0084 - val_r2: 0.8902\n",
      "Epoch 42/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0085 - r2: 0.8897 - val_loss: 0.0083 - val_r2: 0.8913\n",
      "Epoch 43/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0084 - r2: 0.8902 - val_loss: 0.0081 - val_r2: 0.8934\n",
      "Epoch 44/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0083 - r2: 0.8924 - val_loss: 0.0086 - val_r2: 0.8878\n",
      "Epoch 45/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0082 - r2: 0.8927 - val_loss: 0.0080 - val_r2: 0.8956\n",
      "Epoch 46/500\n",
      "10448/10448 [==============================] - 13s 1ms/step - loss: 0.0082 - r2: 0.8931 - val_loss: 0.0086 - val_r2: 0.8873\n",
      "Epoch 47/500\n",
      "10448/10448 [==============================] - 13s 1ms/step - loss: 0.0082 - r2: 0.8932 - val_loss: 0.0085 - val_r2: 0.8893\n",
      "Epoch 48/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0081 - r2: 0.8939 - val_loss: 0.0081 - val_r2: 0.8939\n",
      "Epoch 49/500\n",
      "10448/10448 [==============================] - 13s 1ms/step - loss: 0.0080 - r2: 0.8959 - val_loss: 0.0081 - val_r2: 0.8939\n",
      "Epoch 50/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0079 - r2: 0.8971 - val_loss: 0.0078 - val_r2: 0.8976\n",
      "Epoch 51/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0077 - r2: 0.8990 - val_loss: 0.0080 - val_r2: 0.8957\n",
      "Epoch 52/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0076 - r2: 0.9008 - val_loss: 0.0084 - val_r2: 0.8895\n",
      "Epoch 53/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0077 - r2: 0.8990 - val_loss: 0.0075 - val_r2: 0.9019\n",
      "Epoch 54/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0076 - r2: 0.9005 - val_loss: 0.0073 - val_r2: 0.9039\n",
      "Epoch 55/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0075 - r2: 0.9029 - val_loss: 0.0081 - val_r2: 0.8942\n",
      "Epoch 56/500\n",
      "10448/10448 [==============================] - 13s 1ms/step - loss: 0.0075 - r2: 0.9023 - val_loss: 0.0073 - val_r2: 0.9046\n",
      "Epoch 57/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0074 - r2: 0.9030 - val_loss: 0.0073 - val_r2: 0.9047\n",
      "Epoch 58/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0074 - r2: 0.9037 - val_loss: 0.0072 - val_r2: 0.9059\n",
      "Epoch 59/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0072 - r2: 0.9056 - val_loss: 0.0071 - val_r2: 0.9073\n",
      "Epoch 60/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0074 - r2: 0.9038 - val_loss: 0.0077 - val_r2: 0.8991\n",
      "Epoch 61/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0072 - r2: 0.9058 - val_loss: 0.0072 - val_r2: 0.9064\n",
      "Epoch 62/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0072 - r2: 0.9064 - val_loss: 0.0073 - val_r2: 0.9050\n",
      "Epoch 63/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0072 - r2: 0.9062 - val_loss: 0.0073 - val_r2: 0.9051\n",
      "Epoch 64/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0069 - r2: 0.9094 - val_loss: 0.0073 - val_r2: 0.9049\n",
      "Epoch 65/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0071 - r2: 0.9075 - val_loss: 0.0074 - val_r2: 0.9036\n",
      "Epoch 66/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0069 - r2: 0.9096 - val_loss: 0.0067 - val_r2: 0.9122\n",
      "Epoch 67/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0070 - r2: 0.9093 - val_loss: 0.0068 - val_r2: 0.9106\n",
      "Epoch 68/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0069 - r2: 0.9098 - val_loss: 0.0070 - val_r2: 0.9081\n",
      "Epoch 69/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0068 - r2: 0.9108 - val_loss: 0.0080 - val_r2: 0.8951\n",
      "Epoch 70/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0068 - r2: 0.9117 - val_loss: 0.0066 - val_r2: 0.9135\n",
      "Epoch 71/500\n",
      "10448/10448 [==============================] - 13s 1ms/step - loss: 0.0067 - r2: 0.9127 - val_loss: 0.0067 - val_r2: 0.9128\n",
      "Epoch 72/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0068 - r2: 0.9119 - val_loss: 0.0068 - val_r2: 0.9104\n",
      "Epoch 73/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0066 - r2: 0.9137 - val_loss: 0.0066 - val_r2: 0.9133\n",
      "Epoch 74/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0067 - r2: 0.9122 - val_loss: 0.0064 - val_r2: 0.9159\n",
      "Epoch 75/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0066 - r2: 0.9141 - val_loss: 0.0067 - val_r2: 0.9119\n",
      "Epoch 76/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0065 - r2: 0.9151 - val_loss: 0.0064 - val_r2: 0.9162\n",
      "Epoch 77/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0066 - r2: 0.9139 - val_loss: 0.0070 - val_r2: 0.9082\n",
      "Epoch 78/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0065 - r2: 0.9158 - val_loss: 0.0063 - val_r2: 0.9171\n",
      "Epoch 79/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0064 - r2: 0.9167 - val_loss: 0.0063 - val_r2: 0.9180\n",
      "Epoch 80/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0064 - r2: 0.9169 - val_loss: 0.0065 - val_r2: 0.9144\n",
      "Epoch 81/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0064 - r2: 0.9167 - val_loss: 0.0065 - val_r2: 0.9146\n",
      "Epoch 82/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0065 - r2: 0.9151 - val_loss: 0.0064 - val_r2: 0.9163\n",
      "Epoch 83/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0063 - r2: 0.9175 - val_loss: 0.0061 - val_r2: 0.9201\n",
      "Epoch 84/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0063 - r2: 0.9185 - val_loss: 0.0063 - val_r2: 0.9173\n",
      "Epoch 85/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0062 - r2: 0.9195 - val_loss: 0.0061 - val_r2: 0.9201\n",
      "Epoch 86/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0062 - r2: 0.9195 - val_loss: 0.0064 - val_r2: 0.9165\n",
      "Epoch 87/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0061 - r2: 0.9206 - val_loss: 0.0061 - val_r2: 0.9198\n",
      "Epoch 88/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0064 - r2: 0.9172 - val_loss: 0.0063 - val_r2: 0.9178\n",
      "Epoch 89/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0062 - r2: 0.9193 - val_loss: 0.0060 - val_r2: 0.9211\n",
      "Epoch 90/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0060 - r2: 0.9224 - val_loss: 0.0059 - val_r2: 0.9226\n",
      "Epoch 91/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0062 - r2: 0.9195 - val_loss: 0.0060 - val_r2: 0.9212\n",
      "Epoch 92/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0062 - r2: 0.9197 - val_loss: 0.0067 - val_r2: 0.9117\n",
      "Epoch 93/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0061 - r2: 0.9207 - val_loss: 0.0062 - val_r2: 0.9194\n",
      "Epoch 94/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0061 - r2: 0.9207 - val_loss: 0.0060 - val_r2: 0.9218\n",
      "Epoch 95/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0061 - r2: 0.9204 - val_loss: 0.0058 - val_r2: 0.9245\n"
     ]
    }
   ],
   "source": [
    "autoencoder_no_action, encoder_no_action = lstm_autoencoder(hidden_size=128)\n",
    "train_autoencoder(autoencoder_no_action, train_x, n_epoches=500) # without action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_no_action.save('encoder.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_repre = patinet_encoder.predict(test_x)\n",
    "train_repre = patinet_encoder.predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('train_patient_repre.txt', 'w') as output:\n",
    "    for i, icuid in enumerate(list(prp.patient_map.keys())):\n",
    "        output.write(str(icuid) + '\\n')\n",
    "        output.write( ','.join(map(str, train_repre[i].tolist())) + '\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('test_patient_repre.txt', 'w') as output:\n",
    "    for i, icuid in enumerate(list(prp_test.patient_map.keys())):\n",
    "        output.write(str(icuid) + '\\n')\n",
    "        output.write( ','.join(map(str, test_repre[i].tolist())) + '\\n' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restoring the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/models.py:251: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "patinet_encoder = load_model('encoder.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_repre = patinet_encoder.predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11563, 20, 45)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('../../data/Sepsis_imp_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ave_sofa = train_set.groupby('icustayid')['SOFA'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.25      ,   6.3       ,   5.94117647, ...,   7.25      ,\n",
       "        10.05      ,   2.46153846])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ave_sofa.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1251654e0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQJIaZQBIUqMMAElqlAEgSY36f+m9lrTYVpViAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x133c20c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "pd.Series(ave_sofa.values).hist(bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 25, 5)\n",
    "groups = np.arange(0, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sofa_labels = pd.cut(ave_sofa, bins, labels=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('repre.tsv', 'w') as output:\n",
    "    for i, repre in enumerate(train_repre):\n",
    "        output.write( '\\t'.join(map(str, repre.tolist())) + '\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('repre_label.tsv', 'w') as output:\n",
    "    for i, label in enumerate(sofa_labels.values):\n",
    "        output.write( str(label) + '\\n' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
