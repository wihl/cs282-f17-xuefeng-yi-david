{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../utils')\n",
    "\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, LSTM, RepeatVector, Flatten, Masking\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import discretize_sepsis_actions as discretizer\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PatientRecordProcessor:\n",
    "    columns = ['bloc','icustayid','charttime', 'gender', 'age', 'elixhauser',\n",
    "                're_admission', 'SOFA', 'SIRS', 'Weight_kg', 'GCS', 'HR',\n",
    "                'SysBP', 'MeanBP', 'DiaBP', 'Shock_Index', 'RR', 'SpO2',\n",
    "                'Temp_C', 'FiO2_1', 'Potassium', 'Sodium', 'Chloride',\n",
    "                'Glucose', 'BUN', 'Creatinine', 'Magnesium', 'Calcium',\n",
    "                'Ionised_Ca', 'CO2_mEqL', 'SGOT', 'SGPT', 'Total_bili',\n",
    "                'Albumin', 'Hb', 'WBC_count', 'Platelets_count', 'PTT',\n",
    "                'PT', 'INR', 'Arterial_pH', 'paO2', 'paCO2',\n",
    "                'Arterial_BE', 'Arterial_lactate', 'HCO3', 'PaO2_FiO2',\n",
    "                'median_dose_vaso', 'max_dose_vaso', 'input_total_tev',\n",
    "                'input_4hourly_tev', 'output_total', 'output_4hourly',\n",
    "                'cumulated_balance_tev', 'sedation', 'mechvent', 'rrt',\n",
    "                'died_in_hosp', 'mortality_90d']\n",
    "\n",
    "    observ_cols = ['gender', 'age','elixhauser','re_admission', 'SOFA', 'SIRS', 'Weight_kg', 'GCS', 'HR',\n",
    "                'SysBP', 'MeanBP', 'DiaBP', 'RR', 'SpO2',\n",
    "                'Temp_C', 'FiO2_1', 'Potassium', 'Sodium', 'Chloride',\n",
    "                'Glucose', 'BUN', 'Creatinine', 'Magnesium', 'Calcium',\n",
    "                'Ionised_Ca', 'CO2_mEqL', 'SGOT', 'SGPT', 'Total_bili',\n",
    "                'Albumin', 'Hb', 'WBC_count', 'Platelets_count', 'PTT',\n",
    "                'PT', 'INR', 'Arterial_pH', 'paO2', 'paCO2',\n",
    "                'Arterial_BE', 'Arterial_lactate', 'HCO3', 'PaO2_FiO2',\n",
    "                'output_total', 'output_4hourly']\n",
    "    \n",
    "    action_observ_cols = ['gender', 'age','elixhauser','re_admission', 'SOFA', 'SIRS', 'Weight_kg', 'GCS', 'HR',\n",
    "                'SysBP', 'MeanBP', 'DiaBP', 'RR', 'SpO2',\n",
    "                'Temp_C', 'FiO2_1', 'Potassium', 'Sodium', 'Chloride',\n",
    "                'Glucose', 'BUN', 'Creatinine', 'Magnesium', 'Calcium',\n",
    "                'Ionised_Ca', 'CO2_mEqL', 'SGOT', 'SGPT', 'Total_bili',\n",
    "                'Albumin', 'Hb', 'WBC_count', 'Platelets_count', 'PTT',\n",
    "                'PT', 'INR', 'Arterial_pH', 'paO2', 'paCO2',\n",
    "                'Arterial_BE', 'Arterial_lactate', 'HCO3', 'PaO2_FiO2',\n",
    "                'output_total', 'output_4hourly',\n",
    "                'sedation','median_dose_vaso', 'max_dose_vaso', 'input_total_tev',\n",
    "                'input_4hourly_tev','sedation', 'mechvent', 'rrt']\n",
    "    \n",
    "    n_clusters = 2000\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.patient_map = None\n",
    "        self.LONGEST_STAY = 20 \n",
    "        self.OBSER_LEN = 45\n",
    "    \n",
    "    def load_csv(self, raw_path, cluster_path):\n",
    "        print ( 'loading dataset ...' )\n",
    "        self.df = pd.read_csv(raw_path)\n",
    "        print ( 'loading clustered states ...' )\n",
    "        self.clusters = pkl.load(open(cluster_path, 'rb'), encoding='latin1')\n",
    "        print ( 'discretizing actions ...' )\n",
    "        self.discretize_actions()\n",
    "        print ( 'initialization succeeded' )\n",
    "    \n",
    "    def discretize_actions(self):\n",
    "        self.action_sequence, self.vaso_bins, self.iv_bins = \\\n",
    "        discretizer.discretize_actions(self.df.loc[:,'input_4hourly_tev'],\n",
    "                                       self.df.loc[:,'median_dose_vaso'])\n",
    "    def build_patient_map(self):\n",
    "        self.patient_map = {}\n",
    "        for i, row in self.df.iterrows():\n",
    "            icuid = str(row['icustayid'])\n",
    "            state_action_outcome = [self.clusters[i], self.action_sequence[i],\n",
    "                                    row['input_4hourly_tev'],\n",
    "                                    row['median_dose_vaso'],\n",
    "                                    row['died_in_hosp']]\n",
    "            if icuid not in self.patient_map:\n",
    "                # state_id, action, outcome\n",
    "                self.patient_map[icuid] = {\n",
    "                    'age':row['age'], 'gender':row['gender'], 'sa':[state_action_outcome],\n",
    "                    'obser':[row[self.observ_cols].as_matrix()]}\n",
    "            else:\n",
    "                self.patient_map[icuid]['sa'].append(state_action_outcome)\n",
    "                self.patient_map[icuid]['obser'].append(row[self.observ_cols].as_matrix())\n",
    "\n",
    "        return self.patient_map\n",
    "    # FOR DQN\n",
    "    def build_training_history(self):\n",
    "        memory = []\n",
    "        if not self.patient_map:\n",
    "            print ( 'building patient map ...' )\n",
    "            self.patient_map = self.build_patient_map()\n",
    "\n",
    "        for _, patient in self.patient_map.items():\n",
    "\n",
    "            if len(patient['sa']) <= 5:\n",
    "                continue\n",
    "\n",
    "            for i, patient_icu_stay in enumerate(patient['sa']):\n",
    "                _, action, _, _, outcome = patient_icu_stay\n",
    "                s_obser = patient['obser'][i]\n",
    "                next_s_obser = patient['obser'][i + 1]\n",
    "\n",
    "                reward = 0\n",
    "                if (i + 1) == len(patient['sa']) - 1:\n",
    "                    # last stay, check the outcome\n",
    "                    if patient['sa'][i + 1][-1] == 0:\n",
    "                        # survived\n",
    "                        reward = 15\n",
    "                    else:\n",
    "                        reward = -15\n",
    "\n",
    "                memory.append(np.hstack((s_obser, action, reward, next_s_obser)))\n",
    "\n",
    "                if reward != 0:\n",
    "                    break\n",
    "        return np.array(memory)\n",
    "    \n",
    "    def get_patient_data_seq(self):\n",
    "        # icuids\n",
    "        icuids =  self.df['icustayid'].values\n",
    "        # observation matters\n",
    "        # most of the actions are the same, that's why we diltch them\n",
    "        observations = self.df[self.observ_cols].values\n",
    "\n",
    "        self.patient_map = {}\n",
    "        for i, (icuid, row) in enumerate(zip(icuids, observations)):\n",
    "            if icuid not in self.patient_map:\n",
    "                self.patient_map[icuid] = {}\n",
    "                self.patient_map[icuid]['histories'] = [row]\n",
    "            else:\n",
    "                self.patient_map[icuid]['histories'] += [row]\n",
    "        \n",
    "        return self.patient_map\n",
    "    \n",
    "    def get_patient_data_seq_x(self):\n",
    "        patient_seq = self.get_patient_data_seq()\n",
    "        X = np.zeros((len(patient_seq.keys()), self.LONGEST_STAY, self.OBSER_LEN))\n",
    "        for i, icuid in enumerate(patient_seq.keys()):\n",
    "            for j, hist in enumerate(patient_seq[icuid]['histories']):\n",
    "                X[i, j] = hist\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset ...\n",
      "loading clustered states ...\n",
      "discretizing actions ...\n",
      "initialization succeeded\n"
     ]
    }
   ],
   "source": [
    "# 5% varieity\n",
    "prp = PatientRecordProcessor()\n",
    "raw_csv_path = '../../data/train_scaled.csv'\n",
    "state_path = '../../data/states_list.pkl'\n",
    "prp.load_csv(raw_csv_path, state_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = prp.get_patient_data_seq_x()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset ...\n",
      "loading clustered states ...\n",
      "discretizing actions ...\n",
      "initialization succeeded\n"
     ]
    }
   ],
   "source": [
    "prp_test = PatientRecordProcessor()\n",
    "prp_test.load_csv('../../data/test_scaled.csv', state_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = prp_test.get_patient_data_seq_x()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def r2(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred )) \n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_autoencoder(hidden_size=128):\n",
    "    \n",
    "    # inputs = Input(shape=(prp.LONGEST_STAY, prp.OBSER_LEN))\n",
    "    inputs = Input(shape=(prp.LONGEST_STAY, prp.OBSER_LEN))\n",
    "    masked_input = Masking(mask_value=0.0)(inputs)\n",
    "    \n",
    "    encoded = LSTM(hidden_size, input_shape=(prp.LONGEST_STAY, prp.OBSER_LEN))(masked_input)\n",
    "\n",
    "    decoded = RepeatVector(prp.LONGEST_STAY)(encoded)\n",
    "    decoded = LSTM(prp.OBSER_LEN, return_sequences=True)(decoded)\n",
    "    \n",
    "    sequence_autoencoder = Model(inputs, decoded)\n",
    "    sequence_autoencoder.compile(optimizer='adam', loss='mse', metrics=[r2])\n",
    "    \n",
    "    encoder = Model(inputs, encoded)\n",
    "    \n",
    "    return sequence_autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def two_layer_lstm_autoencoder():\n",
    "    \n",
    "    inputs = Input(shape=(prp.LONGEST_STAY, prp.OBSER_LEN))\n",
    "    \n",
    "    encoded = LSTM(256, input_shape=(prp.LONGEST_STAY, prp.OBSER_LEN), return_sequences=True)(inputs)\n",
    "    encoded = LSTM(128, input_shape=(prp.LONGEST_STAY, 256))(encoded)\n",
    "\n",
    "    decoded = RepeatVector(prp.LONGEST_STAY)(encoded)\n",
    "    decoded = LSTM(prp.OBSER_LEN, return_sequences=True)(decoded)\n",
    "    \n",
    "    sequence_autoencoder = Model(inputs, decoded)\n",
    "    sequence_autoencoder.compile(optimizer='adam', loss='mse', metrics=[r2])\n",
    "    \n",
    "    encoder = Model(inputs, encoded)\n",
    "    \n",
    "    return sequence_autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_autoencoder(autoencoder, x, n_epoches=100, batch_size=128):\n",
    "    x_train, x_val, _, _ = train_test_split(x, x, test_size=0.1, random_state=37)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "    early_Stop = EarlyStopping(monitor='loss',\n",
    "                              min_delta=0,\n",
    "                              patience=5,\n",
    "                              verbose=0, mode='auto')\n",
    "    autoencoder.fit(x_train, x_train,\n",
    "                epochs=n_epoches,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_val, x_val), verbose=1, callbacks=[early_Stop, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder, encoder = lstm_autoencoder(hidden_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder_action, encoder_action = lstm_autoencoder(hidden_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder2l, encoder2l = two_layer_lstm_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10408 samples, validate on 1157 samples\n",
      "Epoch 1/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.5317 - r2: 0.1832 - val_loss: 0.4838 - val_r2: 0.2808\n",
      "Epoch 2/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.4298 - r2: 0.3398 - val_loss: 0.4293 - val_r2: 0.3621\n",
      "Epoch 3/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.3966 - r2: 0.3906 - val_loss: 0.4134 - val_r2: 0.3857\n",
      "Epoch 4/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3858 - r2: 0.4075 - val_loss: 0.4065 - val_r2: 0.3960\n",
      "Epoch 5/500\n",
      "10408/10408 [==============================] - 14s 1ms/step - loss: 0.3801 - r2: 0.4162 - val_loss: 0.4017 - val_r2: 0.4031\n",
      "Epoch 6/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3753 - r2: 0.4236 - val_loss: 0.3969 - val_r2: 0.4103\n",
      "Epoch 7/500\n",
      "10408/10408 [==============================] - 13s 1ms/step - loss: 0.3695 - r2: 0.4329 - val_loss: 0.3909 - val_r2: 0.4194\n",
      "Epoch 8/500\n",
      "10408/10408 [==============================] - 13s 1ms/step - loss: 0.3608 - r2: 0.4465 - val_loss: 0.3797 - val_r2: 0.4362\n",
      "Epoch 9/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3496 - r2: 0.4634 - val_loss: 0.3686 - val_r2: 0.4528\n",
      "Epoch 10/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.3407 - r2: 0.4776 - val_loss: 0.3623 - val_r2: 0.4623\n",
      "Epoch 11/500\n",
      "10408/10408 [==============================] - 13s 1ms/step - loss: 0.3345 - r2: 0.4863 - val_loss: 0.3571 - val_r2: 0.4701\n",
      "Epoch 12/500\n",
      "10408/10408 [==============================] - 13s 1ms/step - loss: 0.3303 - r2: 0.4933 - val_loss: 0.3535 - val_r2: 0.4754\n",
      "Epoch 13/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.3265 - r2: 0.4995 - val_loss: 0.3500 - val_r2: 0.4807\n",
      "Epoch 14/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.3236 - r2: 0.5039 - val_loss: 0.3478 - val_r2: 0.4840\n",
      "Epoch 15/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3211 - r2: 0.5076 - val_loss: 0.3449 - val_r2: 0.4884\n",
      "Epoch 16/500\n",
      "10408/10408 [==============================] - 13s 1ms/step - loss: 0.3190 - r2: 0.5108 - val_loss: 0.3436 - val_r2: 0.4902\n",
      "Epoch 17/500\n",
      "10408/10408 [==============================] - 13s 1ms/step - loss: 0.3173 - r2: 0.5136 - val_loss: 0.3425 - val_r2: 0.4918\n",
      "Epoch 18/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.3151 - r2: 0.5168 - val_loss: 0.3406 - val_r2: 0.4947\n",
      "Epoch 19/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3139 - r2: 0.5190 - val_loss: 0.3395 - val_r2: 0.4964\n",
      "Epoch 20/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.3126 - r2: 0.5208 - val_loss: 0.3380 - val_r2: 0.4985\n",
      "Epoch 21/500\n",
      "10408/10408 [==============================] - 14s 1ms/step - loss: 0.3112 - r2: 0.5231 - val_loss: 0.3375 - val_r2: 0.4993\n",
      "Epoch 22/500\n",
      "10408/10408 [==============================] - 14s 1ms/step - loss: 0.3103 - r2: 0.5239 - val_loss: 0.3355 - val_r2: 0.5023\n",
      "Epoch 23/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3099 - r2: 0.5247 - val_loss: 0.3351 - val_r2: 0.5029\n",
      "Epoch 24/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.3088 - r2: 0.5267 - val_loss: 0.3351 - val_r2: 0.5028\n",
      "Epoch 25/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3085 - r2: 0.5269 - val_loss: 0.3336 - val_r2: 0.5052\n",
      "Epoch 26/500\n",
      "10408/10408 [==============================] - 13s 1ms/step - loss: 0.3072 - r2: 0.5290 - val_loss: 0.3324 - val_r2: 0.5069\n",
      "Epoch 27/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3061 - r2: 0.5316 - val_loss: 0.3317 - val_r2: 0.5079\n",
      "Epoch 28/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3051 - r2: 0.5329 - val_loss: 0.3316 - val_r2: 0.5082\n",
      "Epoch 29/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3051 - r2: 0.5323 - val_loss: 0.3307 - val_r2: 0.5094\n",
      "Epoch 30/500\n",
      "10408/10408 [==============================] - 13s 1ms/step - loss: 0.3053 - r2: 0.5320 - val_loss: 0.3300 - val_r2: 0.5105\n",
      "Epoch 31/500\n",
      "10408/10408 [==============================] - 14s 1ms/step - loss: 0.3037 - r2: 0.5352 - val_loss: 0.3314 - val_r2: 0.5083\n",
      "Epoch 32/500\n",
      "10408/10408 [==============================] - 14s 1ms/step - loss: 0.3037 - r2: 0.5343 - val_loss: 0.3296 - val_r2: 0.5111\n",
      "Epoch 33/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3024 - r2: 0.5370 - val_loss: 0.3286 - val_r2: 0.5126\n",
      "Epoch 34/500\n",
      "10408/10408 [==============================] - 13s 1ms/step - loss: 0.3023 - r2: 0.5364 - val_loss: 0.3278 - val_r2: 0.5137\n",
      "Epoch 35/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3015 - r2: 0.5380 - val_loss: 0.3290 - val_r2: 0.5119\n",
      "Epoch 36/500\n",
      "10408/10408 [==============================] - 14s 1ms/step - loss: 0.3010 - r2: 0.5388 - val_loss: 0.3273 - val_r2: 0.5145\n",
      "Epoch 37/500\n",
      "10408/10408 [==============================] - 13s 1ms/step - loss: 0.3013 - r2: 0.5379 - val_loss: 0.3269 - val_r2: 0.5151\n",
      "Epoch 38/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.3007 - r2: 0.5390 - val_loss: 0.3262 - val_r2: 0.5161\n",
      "Epoch 39/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2996 - r2: 0.5411 - val_loss: 0.3250 - val_r2: 0.5179\n",
      "Epoch 40/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2995 - r2: 0.5414 - val_loss: 0.3277 - val_r2: 0.5139\n",
      "Epoch 41/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.3004 - r2: 0.5398 - val_loss: 0.3271 - val_r2: 0.5148\n",
      "Epoch 42/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2990 - r2: 0.5413 - val_loss: 0.3258 - val_r2: 0.5168\n",
      "Epoch 43/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2988 - r2: 0.5430 - val_loss: 0.3245 - val_r2: 0.5188\n",
      "Epoch 44/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2978 - r2: 0.5437 - val_loss: 0.3231 - val_r2: 0.5208\n",
      "Epoch 45/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2975 - r2: 0.5440 - val_loss: 0.3227 - val_r2: 0.5214\n",
      "Epoch 46/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2970 - r2: 0.5450 - val_loss: 0.3233 - val_r2: 0.5205\n",
      "Epoch 47/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2968 - r2: 0.5451 - val_loss: 0.3229 - val_r2: 0.5211\n",
      "Epoch 48/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2966 - r2: 0.5458 - val_loss: 0.3223 - val_r2: 0.5220\n",
      "Epoch 49/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2967 - r2: 0.5460 - val_loss: 0.3242 - val_r2: 0.5192\n",
      "Epoch 50/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2960 - r2: 0.5462 - val_loss: 0.3220 - val_r2: 0.5223\n",
      "Epoch 51/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2957 - r2: 0.5468 - val_loss: 0.3223 - val_r2: 0.5219\n",
      "Epoch 52/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2960 - r2: 0.5464 - val_loss: 0.3217 - val_r2: 0.5228\n",
      "Epoch 53/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2958 - r2: 0.5467 - val_loss: 0.3211 - val_r2: 0.5237\n",
      "Epoch 54/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2951 - r2: 0.5482 - val_loss: 0.3214 - val_r2: 0.5233\n",
      "Epoch 55/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2950 - r2: 0.5474 - val_loss: 0.3205 - val_r2: 0.5247\n",
      "Epoch 56/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2949 - r2: 0.5479 - val_loss: 0.3207 - val_r2: 0.5243\n",
      "Epoch 57/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2944 - r2: 0.5490 - val_loss: 0.3215 - val_r2: 0.5232\n",
      "Epoch 58/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2940 - r2: 0.5494 - val_loss: 0.3194 - val_r2: 0.5263\n",
      "Epoch 59/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2941 - r2: 0.5495 - val_loss: 0.3200 - val_r2: 0.5255\n",
      "Epoch 60/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2941 - r2: 0.5495 - val_loss: 0.3204 - val_r2: 0.5247\n",
      "Epoch 61/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2936 - r2: 0.5503 - val_loss: 0.3199 - val_r2: 0.5255\n",
      "Epoch 62/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2938 - r2: 0.5499 - val_loss: 0.3183 - val_r2: 0.5279\n",
      "Epoch 63/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2935 - r2: 0.5496 - val_loss: 0.3185 - val_r2: 0.5277\n",
      "Epoch 64/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2926 - r2: 0.5515 - val_loss: 0.3184 - val_r2: 0.5278\n",
      "Epoch 65/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2925 - r2: 0.5511 - val_loss: 0.3181 - val_r2: 0.5281\n",
      "Epoch 66/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2921 - r2: 0.5519 - val_loss: 0.3193 - val_r2: 0.5264\n",
      "Epoch 67/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2926 - r2: 0.5517 - val_loss: 0.3182 - val_r2: 0.5281\n",
      "Epoch 68/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2925 - r2: 0.5517 - val_loss: 0.3181 - val_r2: 0.5282\n",
      "Epoch 69/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2920 - r2: 0.5526 - val_loss: 0.3174 - val_r2: 0.5293\n",
      "Epoch 70/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2916 - r2: 0.5536 - val_loss: 0.3184 - val_r2: 0.5278\n",
      "Epoch 71/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2914 - r2: 0.5534 - val_loss: 0.3175 - val_r2: 0.5291\n",
      "Epoch 72/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2910 - r2: 0.5546 - val_loss: 0.3181 - val_r2: 0.5283\n",
      "Epoch 73/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2913 - r2: 0.5539 - val_loss: 0.3180 - val_r2: 0.5284\n",
      "Epoch 74/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2911 - r2: 0.5546 - val_loss: 0.3168 - val_r2: 0.5303\n",
      "Epoch 75/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2910 - r2: 0.5540 - val_loss: 0.3175 - val_r2: 0.5291\n",
      "Epoch 76/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2904 - r2: 0.5552 - val_loss: 0.3167 - val_r2: 0.5304\n",
      "Epoch 77/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2906 - r2: 0.5548 - val_loss: 0.3167 - val_r2: 0.5304\n",
      "Epoch 78/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2904 - r2: 0.5555 - val_loss: 0.3165 - val_r2: 0.5307\n",
      "Epoch 79/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2904 - r2: 0.5550 - val_loss: 0.3161 - val_r2: 0.5313\n",
      "Epoch 80/500\n",
      "10408/10408 [==============================] - 13s 1ms/step - loss: 0.2897 - r2: 0.5566 - val_loss: 0.3160 - val_r2: 0.5313\n",
      "Epoch 81/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2894 - r2: 0.5568 - val_loss: 0.3161 - val_r2: 0.5312\n",
      "Epoch 82/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2896 - r2: 0.5570 - val_loss: 0.3160 - val_r2: 0.5315\n",
      "Epoch 83/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2896 - r2: 0.5563 - val_loss: 0.3153 - val_r2: 0.5325\n",
      "Epoch 84/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2892 - r2: 0.5574 - val_loss: 0.3153 - val_r2: 0.5325\n",
      "Epoch 85/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2893 - r2: 0.5575 - val_loss: 0.3153 - val_r2: 0.5324\n",
      "Epoch 86/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2898 - r2: 0.5561 - val_loss: 0.3152 - val_r2: 0.5325\n",
      "Epoch 87/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2889 - r2: 0.5565 - val_loss: 0.3145 - val_r2: 0.5336\n",
      "Epoch 88/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2884 - r2: 0.5581 - val_loss: 0.3150 - val_r2: 0.5329\n",
      "Epoch 89/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2887 - r2: 0.5578 - val_loss: 0.3149 - val_r2: 0.5330\n",
      "Epoch 90/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2885 - r2: 0.5578 - val_loss: 0.3144 - val_r2: 0.5338\n",
      "Epoch 91/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2886 - r2: 0.5579 - val_loss: 0.3145 - val_r2: 0.5336\n",
      "Epoch 92/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2882 - r2: 0.5586 - val_loss: 0.3145 - val_r2: 0.5337\n",
      "Epoch 93/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2881 - r2: 0.5591 - val_loss: 0.3137 - val_r2: 0.5348\n",
      "Epoch 94/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2878 - r2: 0.5595 - val_loss: 0.3148 - val_r2: 0.5332\n",
      "Epoch 95/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2880 - r2: 0.5593 - val_loss: 0.3136 - val_r2: 0.5349\n",
      "Epoch 96/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2877 - r2: 0.5599 - val_loss: 0.3136 - val_r2: 0.5350\n",
      "Epoch 97/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2876 - r2: 0.5592 - val_loss: 0.3134 - val_r2: 0.5353\n",
      "Epoch 98/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2875 - r2: 0.5599 - val_loss: 0.3142 - val_r2: 0.5340\n",
      "Epoch 99/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2875 - r2: 0.5593 - val_loss: 0.3130 - val_r2: 0.5359\n",
      "Epoch 100/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2869 - r2: 0.5604 - val_loss: 0.3137 - val_r2: 0.5348\n",
      "Epoch 101/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2881 - r2: 0.5589 - val_loss: 0.3133 - val_r2: 0.5354\n",
      "Epoch 102/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2877 - r2: 0.5590 - val_loss: 0.3133 - val_r2: 0.5355\n",
      "Epoch 103/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2870 - r2: 0.5602 - val_loss: 0.3129 - val_r2: 0.5360\n",
      "Epoch 104/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2866 - r2: 0.5610 - val_loss: 0.3132 - val_r2: 0.5355\n",
      "Epoch 105/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2868 - r2: 0.5612 - val_loss: 0.3123 - val_r2: 0.5370\n",
      "Epoch 106/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2861 - r2: 0.5614 - val_loss: 0.3129 - val_r2: 0.5359\n",
      "Epoch 107/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2863 - r2: 0.5614 - val_loss: 0.3122 - val_r2: 0.5371\n",
      "Epoch 108/500\n",
      "10408/10408 [==============================] - 11s 1ms/step - loss: 0.2863 - r2: 0.5620 - val_loss: 0.3133 - val_r2: 0.5354\n",
      "Epoch 109/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2863 - r2: 0.5609 - val_loss: 0.3128 - val_r2: 0.5362\n",
      "Epoch 110/500\n",
      "10408/10408 [==============================] - 13s 1ms/step - loss: 0.2860 - r2: 0.5616 - val_loss: 0.3124 - val_r2: 0.5367\n",
      "Epoch 111/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2861 - r2: 0.5618 - val_loss: 0.3123 - val_r2: 0.5368\n",
      "Epoch 112/500\n",
      "10408/10408 [==============================] - 12s 1ms/step - loss: 0.2856 - r2: 0.5624 - val_loss: 0.3127 - val_r2: 0.5364\n"
     ]
    }
   ],
   "source": [
    "train_autoencoder(autoencoder, train_x, n_epoches=500) # with action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3885/3885 [==============================] - 3s 693us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.011337238124198972, 0.85497363948453808]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder_no_action.evaluate(test_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10448 samples, validate on 1161 samples\n",
      "Epoch 1/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0587 - r2: 0.2347 - val_loss: 0.0465 - val_r2: 0.3919\n",
      "Epoch 2/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0426 - r2: 0.4446 - val_loss: 0.0398 - val_r2: 0.4786\n",
      "Epoch 3/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0338 - r2: 0.5589 - val_loss: 0.0268 - val_r2: 0.6491\n",
      "Epoch 4/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0237 - r2: 0.6909 - val_loss: 0.0221 - val_r2: 0.7110\n",
      "Epoch 5/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0206 - r2: 0.7319 - val_loss: 0.0195 - val_r2: 0.7447\n",
      "Epoch 6/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0184 - r2: 0.7601 - val_loss: 0.0174 - val_r2: 0.7716\n",
      "Epoch 7/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0171 - r2: 0.7765 - val_loss: 0.0179 - val_r2: 0.7663\n",
      "Epoch 8/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0164 - r2: 0.7868 - val_loss: 0.0161 - val_r2: 0.7891\n",
      "Epoch 9/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0151 - r2: 0.8025 - val_loss: 0.0147 - val_r2: 0.8081\n",
      "Epoch 10/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0146 - r2: 0.8094 - val_loss: 0.0139 - val_r2: 0.8178\n",
      "Epoch 11/500\n",
      "10448/10448 [==============================] - 13s 1ms/step - loss: 0.0143 - r2: 0.8130 - val_loss: 0.0138 - val_r2: 0.8192\n",
      "Epoch 12/500\n",
      "10448/10448 [==============================] - 14s 1ms/step - loss: 0.0137 - r2: 0.8210 - val_loss: 0.0138 - val_r2: 0.8189\n",
      "Epoch 13/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0133 - r2: 0.8261 - val_loss: 0.0131 - val_r2: 0.8288\n",
      "Epoch 14/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0131 - r2: 0.8288 - val_loss: 0.0126 - val_r2: 0.8355\n",
      "Epoch 15/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0126 - r2: 0.8358 - val_loss: 0.0123 - val_r2: 0.8387\n",
      "Epoch 16/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0125 - r2: 0.8365 - val_loss: 0.0125 - val_r2: 0.8365\n",
      "Epoch 17/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0121 - r2: 0.8428 - val_loss: 0.0120 - val_r2: 0.8424\n",
      "Epoch 18/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0117 - r2: 0.8469 - val_loss: 0.0139 - val_r2: 0.8184\n",
      "Epoch 19/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0119 - r2: 0.8449 - val_loss: 0.0121 - val_r2: 0.8420\n",
      "Epoch 20/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0116 - r2: 0.8484 - val_loss: 0.0110 - val_r2: 0.8564\n",
      "Epoch 21/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0112 - r2: 0.8541 - val_loss: 0.0120 - val_r2: 0.8434\n",
      "Epoch 22/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0109 - r2: 0.8576 - val_loss: 0.0114 - val_r2: 0.8510\n",
      "Epoch 23/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0111 - r2: 0.8549 - val_loss: 0.0105 - val_r2: 0.8627\n",
      "Epoch 24/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0106 - r2: 0.8618 - val_loss: 0.0105 - val_r2: 0.8620\n",
      "Epoch 25/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0106 - r2: 0.8619 - val_loss: 0.0102 - val_r2: 0.8661\n",
      "Epoch 26/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0104 - r2: 0.8640 - val_loss: 0.0100 - val_r2: 0.8686\n",
      "Epoch 27/500\n",
      "10448/10448 [==============================] - 13s 1ms/step - loss: 0.0103 - r2: 0.8661 - val_loss: 0.0100 - val_r2: 0.8688\n",
      "Epoch 28/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0100 - r2: 0.8695 - val_loss: 0.0111 - val_r2: 0.8547\n",
      "Epoch 29/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0102 - r2: 0.8674 - val_loss: 0.0097 - val_r2: 0.8731\n",
      "Epoch 30/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0098 - r2: 0.8726 - val_loss: 0.0095 - val_r2: 0.8759\n",
      "Epoch 31/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0097 - r2: 0.8731 - val_loss: 0.0096 - val_r2: 0.8737\n",
      "Epoch 32/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0098 - r2: 0.8718 - val_loss: 0.0107 - val_r2: 0.8599\n",
      "Epoch 33/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0094 - r2: 0.8775 - val_loss: 0.0095 - val_r2: 0.8754\n",
      "Epoch 34/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0094 - r2: 0.8769 - val_loss: 0.0093 - val_r2: 0.8781\n",
      "Epoch 35/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0092 - r2: 0.8802 - val_loss: 0.0092 - val_r2: 0.8802\n",
      "Epoch 36/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0091 - r2: 0.8816 - val_loss: 0.0088 - val_r2: 0.8843\n",
      "Epoch 37/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0091 - r2: 0.8809 - val_loss: 0.0087 - val_r2: 0.8859\n",
      "Epoch 38/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0089 - r2: 0.8835 - val_loss: 0.0094 - val_r2: 0.8773\n",
      "Epoch 39/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0087 - r2: 0.8869 - val_loss: 0.0088 - val_r2: 0.8853\n",
      "Epoch 40/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0089 - r2: 0.8845 - val_loss: 0.0087 - val_r2: 0.8857\n",
      "Epoch 41/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0088 - r2: 0.8851 - val_loss: 0.0084 - val_r2: 0.8902\n",
      "Epoch 42/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0085 - r2: 0.8897 - val_loss: 0.0083 - val_r2: 0.8913\n",
      "Epoch 43/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0084 - r2: 0.8902 - val_loss: 0.0081 - val_r2: 0.8934\n",
      "Epoch 44/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0083 - r2: 0.8924 - val_loss: 0.0086 - val_r2: 0.8878\n",
      "Epoch 45/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0082 - r2: 0.8927 - val_loss: 0.0080 - val_r2: 0.8956\n",
      "Epoch 46/500\n",
      "10448/10448 [==============================] - 13s 1ms/step - loss: 0.0082 - r2: 0.8931 - val_loss: 0.0086 - val_r2: 0.8873\n",
      "Epoch 47/500\n",
      "10448/10448 [==============================] - 13s 1ms/step - loss: 0.0082 - r2: 0.8932 - val_loss: 0.0085 - val_r2: 0.8893\n",
      "Epoch 48/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0081 - r2: 0.8939 - val_loss: 0.0081 - val_r2: 0.8939\n",
      "Epoch 49/500\n",
      "10448/10448 [==============================] - 13s 1ms/step - loss: 0.0080 - r2: 0.8959 - val_loss: 0.0081 - val_r2: 0.8939\n",
      "Epoch 50/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0079 - r2: 0.8971 - val_loss: 0.0078 - val_r2: 0.8976\n",
      "Epoch 51/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0077 - r2: 0.8990 - val_loss: 0.0080 - val_r2: 0.8957\n",
      "Epoch 52/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0076 - r2: 0.9008 - val_loss: 0.0084 - val_r2: 0.8895\n",
      "Epoch 53/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0077 - r2: 0.8990 - val_loss: 0.0075 - val_r2: 0.9019\n",
      "Epoch 54/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0076 - r2: 0.9005 - val_loss: 0.0073 - val_r2: 0.9039\n",
      "Epoch 55/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0075 - r2: 0.9029 - val_loss: 0.0081 - val_r2: 0.8942\n",
      "Epoch 56/500\n",
      "10448/10448 [==============================] - 13s 1ms/step - loss: 0.0075 - r2: 0.9023 - val_loss: 0.0073 - val_r2: 0.9046\n",
      "Epoch 57/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0074 - r2: 0.9030 - val_loss: 0.0073 - val_r2: 0.9047\n",
      "Epoch 58/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0074 - r2: 0.9037 - val_loss: 0.0072 - val_r2: 0.9059\n",
      "Epoch 59/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0072 - r2: 0.9056 - val_loss: 0.0071 - val_r2: 0.9073\n",
      "Epoch 60/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0074 - r2: 0.9038 - val_loss: 0.0077 - val_r2: 0.8991\n",
      "Epoch 61/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0072 - r2: 0.9058 - val_loss: 0.0072 - val_r2: 0.9064\n",
      "Epoch 62/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0072 - r2: 0.9064 - val_loss: 0.0073 - val_r2: 0.9050\n",
      "Epoch 63/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0072 - r2: 0.9062 - val_loss: 0.0073 - val_r2: 0.9051\n",
      "Epoch 64/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0069 - r2: 0.9094 - val_loss: 0.0073 - val_r2: 0.9049\n",
      "Epoch 65/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0071 - r2: 0.9075 - val_loss: 0.0074 - val_r2: 0.9036\n",
      "Epoch 66/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0069 - r2: 0.9096 - val_loss: 0.0067 - val_r2: 0.9122\n",
      "Epoch 67/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0070 - r2: 0.9093 - val_loss: 0.0068 - val_r2: 0.9106\n",
      "Epoch 68/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0069 - r2: 0.9098 - val_loss: 0.0070 - val_r2: 0.9081\n",
      "Epoch 69/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0068 - r2: 0.9108 - val_loss: 0.0080 - val_r2: 0.8951\n",
      "Epoch 70/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0068 - r2: 0.9117 - val_loss: 0.0066 - val_r2: 0.9135\n",
      "Epoch 71/500\n",
      "10448/10448 [==============================] - 13s 1ms/step - loss: 0.0067 - r2: 0.9127 - val_loss: 0.0067 - val_r2: 0.9128\n",
      "Epoch 72/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0068 - r2: 0.9119 - val_loss: 0.0068 - val_r2: 0.9104\n",
      "Epoch 73/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0066 - r2: 0.9137 - val_loss: 0.0066 - val_r2: 0.9133\n",
      "Epoch 74/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0067 - r2: 0.9122 - val_loss: 0.0064 - val_r2: 0.9159\n",
      "Epoch 75/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0066 - r2: 0.9141 - val_loss: 0.0067 - val_r2: 0.9119\n",
      "Epoch 76/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0065 - r2: 0.9151 - val_loss: 0.0064 - val_r2: 0.9162\n",
      "Epoch 77/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0066 - r2: 0.9139 - val_loss: 0.0070 - val_r2: 0.9082\n",
      "Epoch 78/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0065 - r2: 0.9158 - val_loss: 0.0063 - val_r2: 0.9171\n",
      "Epoch 79/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0064 - r2: 0.9167 - val_loss: 0.0063 - val_r2: 0.9180\n",
      "Epoch 80/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0064 - r2: 0.9169 - val_loss: 0.0065 - val_r2: 0.9144\n",
      "Epoch 81/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0064 - r2: 0.9167 - val_loss: 0.0065 - val_r2: 0.9146\n",
      "Epoch 82/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0065 - r2: 0.9151 - val_loss: 0.0064 - val_r2: 0.9163\n",
      "Epoch 83/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0063 - r2: 0.9175 - val_loss: 0.0061 - val_r2: 0.9201\n",
      "Epoch 84/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0063 - r2: 0.9185 - val_loss: 0.0063 - val_r2: 0.9173\n",
      "Epoch 85/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0062 - r2: 0.9195 - val_loss: 0.0061 - val_r2: 0.9201\n",
      "Epoch 86/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0062 - r2: 0.9195 - val_loss: 0.0064 - val_r2: 0.9165\n",
      "Epoch 87/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0061 - r2: 0.9206 - val_loss: 0.0061 - val_r2: 0.9198\n",
      "Epoch 88/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0064 - r2: 0.9172 - val_loss: 0.0063 - val_r2: 0.9178\n",
      "Epoch 89/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0062 - r2: 0.9193 - val_loss: 0.0060 - val_r2: 0.9211\n",
      "Epoch 90/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0060 - r2: 0.9224 - val_loss: 0.0059 - val_r2: 0.9226\n",
      "Epoch 91/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0062 - r2: 0.9195 - val_loss: 0.0060 - val_r2: 0.9212\n",
      "Epoch 92/500\n",
      "10448/10448 [==============================] - 12s 1ms/step - loss: 0.0062 - r2: 0.9197 - val_loss: 0.0067 - val_r2: 0.9117\n",
      "Epoch 93/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0061 - r2: 0.9207 - val_loss: 0.0062 - val_r2: 0.9194\n",
      "Epoch 94/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0061 - r2: 0.9207 - val_loss: 0.0060 - val_r2: 0.9218\n",
      "Epoch 95/500\n",
      "10448/10448 [==============================] - 11s 1ms/step - loss: 0.0061 - r2: 0.9204 - val_loss: 0.0058 - val_r2: 0.9245\n"
     ]
    }
   ],
   "source": [
    "autoencoder_no_action, encoder_no_action = lstm_autoencoder(hidden_size=128)\n",
    "train_autoencoder(autoencoder_no_action, train_x, n_epoches=500) # without action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_no_action.save('encoder.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_repre = encoder_no_action.predict(test_x)\n",
    "train_repre = encoder_no_action.predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('train_patient_repre.txt', 'w') as output:\n",
    "    for i, icuid in enumerate(list(prp.patient_map.keys())):\n",
    "        output.write(str(icuid) + '\\n')\n",
    "        output.write( ','.join(map(str, train_repre[i].tolist())) + '\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('test_patient_repre.txt', 'w') as output:\n",
    "    for i, icuid in enumerate(list(prp_test.patient_map.keys())):\n",
    "        output.write(str(icuid) + '\\n')\n",
    "        output.write( ','.join(map(str, test_repre[i].tolist())) + '\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
