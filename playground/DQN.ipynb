{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(3)\n",
    "tf.set_random_seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SumTree:\n",
    "    data_pointer = 0\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity  # for all priority values\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)  # for all transitions\n",
    "    \n",
    "    def add(self, p, data):\n",
    "        tree_idx = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data  # update data_frame\n",
    "        self.update(tree_idx, p)  # update tree_frame\n",
    "\n",
    "        self.data_pointer += 1\n",
    "        if self.data_pointer >= self.capacity:  # replace when exceed the capacity\n",
    "            self.data_pointer = 0\n",
    "\n",
    "    def update(self, tree_idx, p):\n",
    "        change = p - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = p\n",
    "        # then propagate the change through tree\n",
    "        while tree_idx != 0:    # this method is faster than the recursive loop in the reference code\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_leaf(self, v):\n",
    "        \"\"\"\n",
    "        Tree structure and array storage:\n",
    "        Tree index:\n",
    "             0         -> storing priority sum\n",
    "            / \\\n",
    "          1     2\n",
    "         / \\   / \\\n",
    "        3   4 5   6    -> storing priority for transitions\n",
    "        Array type for storing:\n",
    "        [0,1,2,3,4,5,6]\n",
    "        \"\"\"\n",
    "        parent_idx = 0\n",
    "        while True:     # the while loop is faster than the method in the reference code\n",
    "            cl_idx = 2 * parent_idx + 1         # this leaf's left and right kids\n",
    "            cr_idx = cl_idx + 1\n",
    "            if cl_idx >= len(self.tree):        # reach bottom, end search\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            else:       # downward search, always search for a higher priority node\n",
    "                if v <= self.tree[cl_idx]:\n",
    "                    parent_idx = cl_idx\n",
    "                else:\n",
    "                    v -= self.tree[cl_idx]\n",
    "                    parent_idx = cr_idx\n",
    "\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
    "\n",
    "    @property\n",
    "    def total_p(self):\n",
    "        return self.tree[0]  # the root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    \"\"\"\n",
    "    epsilon = 0.01  # small amount to avoid zero priority\n",
    "    alpha = 0.6  # [0~1] convert the importance of TD error to priority\n",
    "    beta = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "    beta_increment_per_sampling = 0.001\n",
    "    abs_err_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    def store(self, transition):\n",
    "        max_p = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        if max_p == 0:\n",
    "            max_p = self.abs_err_upper\n",
    "        self.tree.add(max_p, transition)   # set the max p for new p\n",
    "\n",
    "    def sample(self, n):\n",
    "        b_idx, b_memory, ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, self.tree.data[0].size)), np.empty((n, 1))\n",
    "        pri_seg = self.tree.total_p / n       # priority segment\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])  # max = 1\n",
    "\n",
    "        min_prob = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_p     # for later calculate ISweight\n",
    "        for i in range(n):\n",
    "            a, b = pri_seg * i, pri_seg * (i + 1)\n",
    "            v = np.random.uniform(a, b)\n",
    "            idx, p, data = self.tree.get_leaf(v)\n",
    "            prob = p / self.tree.total_p\n",
    "            ISWeights[i, 0] = np.power(prob/min_prob, -self.beta)\n",
    "            b_idx[i], b_memory[i, :] = idx, data\n",
    "        return b_idx, b_memory, ISWeights\n",
    "\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.epsilon  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.abs_err_upper)\n",
    "        ps = np.power(clipped_errors, self.alpha)\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DQN:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions,\n",
    "        num_features,\n",
    "        hidden_units = 128,\n",
    "        learning_rate=0.0001,\n",
    "        reward_discount = 0.95,\n",
    "        epsilon_greedy = 0.9,\n",
    "        lamda = 5,\n",
    "        e_increment = None,\n",
    "        replace_target_iter = 200,\n",
    "        mem_size = 2000,\n",
    "        batch_size = 128,\n",
    "        Rmax = 15,\n",
    "        dueling = False,\n",
    "        prioritized = True,\n",
    "        output_graph = False,\n",
    "        output_path = None,\n",
    "        sess = None\n",
    "    ):\n",
    "        # network layer sizes\n",
    "        self.num_actions, self.num_features = num_actions, num_features\n",
    "        self.hidden_units = hidden_units\n",
    "        # training \n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_discount\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        self.memory_size = mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.lamda = lamda\n",
    "        # epsilon\n",
    "        self.e_increment = e_increment\n",
    "        self.e_max = epsilon_greedy\n",
    "        self.epsilon = 0 if e_increment else (1 - self.e_max)\n",
    "\n",
    "        self.phase = tf.placeholder(tf.bool)\n",
    "        \n",
    "        self.dueling = dueling\n",
    "        self.prioritized = prioritized\n",
    "        self.learn_step_counter = 0\n",
    "        # reward\n",
    "        self.rmax = Rmax\n",
    "        \n",
    "        # s, a, r ,s_\n",
    "        if self.prioritized:\n",
    "            self.memory = Memory(capacity=self.memory_size)\n",
    "        else:\n",
    "            self.memory = np.zeros((self.memory_size, 2 * self.num_features + self.num_actions + 1))\n",
    "        \n",
    "        #build net\n",
    "        self.__build_net__()\n",
    "        # sync target network\n",
    "        self.sync_target_op = [tf.assign(t, e) for t, e in zip(tf.get_collection('target_net_params'), \\\n",
    "                                                               tf.get_collection('eval_net_params'))]\n",
    "        \n",
    "        if not sess:\n",
    "            self.sess = tf.Session()\n",
    "        else:\n",
    "            self.sess = sess\n",
    "        \n",
    "        if output_graph:\n",
    "            if not output_path: output_path = ''\n",
    "            else: output_path += '/'\n",
    "            self.writer = tf.summary.FileWriter(\"logs/\" + output_path, self.sess.graph)\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def __build_net__(self):\n",
    "        \n",
    "        def build_layers(s, c_names, summary=True):\n",
    "            # use xavier_initializer with normal distribution\n",
    "            w_init, b_init = tf.contrib.layers.xavier_initializer(uniform=False), tf.constant_initializer(.1)\n",
    "            # inpyt layer + relu\n",
    "            with tf.variable_scope('l1'):\n",
    "                w1 = tf.get_variable('w1', [self.num_features, self.hidden_units], initializer=w_init, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, self.hidden_units], initializer=b_init, collections=c_names)\n",
    "                l1 = tf.nn.relu(tf.matmul(s, w1) + b1)\n",
    "            \n",
    "#             with tf.variable_scope('l1_bn_ac'):\n",
    "#                 l1_bn = tf.layers.batch_normalization(l1, training=self.phase)\n",
    "#                 l1_ac = tf.maximum(l1_bn, l1_bn * 0.5)\n",
    "            \n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [self.hidden_units, self.hidden_units], initializer=w_init, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.hidden_units], initializer=b_init, collections=c_names)\n",
    "                l2 = tf.nn.relu(tf.matmul(l1, w2) + b2)\n",
    "            \n",
    "#             with tf.variable_scope('l2_bn_ac'):\n",
    "#                 l2_bn = tf.layers.batch_normalization(l2, training=self.phase)\n",
    "#                 l2_ac = tf.maximum(l2_bn, l2_bn * 0.5)\n",
    "            \n",
    "            if self.dueling:\n",
    "                # state value\n",
    "                with tf.variable_scope('Value'):\n",
    "                    w2 = tf.get_variable('w3', [self.hidden_units, 1], initializer=w_init, collections=c_names)\n",
    "                    b2 = tf.get_variable('b3', [1, 1], initializer=b_init, collections=c_names)\n",
    "                    self.V = tf.matmul(l2, w2) + b2\n",
    "                # action value\n",
    "                with tf.variable_scope('Advantage'):\n",
    "                    w2 = tf.get_variable('w3', [self.hidden_units, self.num_actions], initializer=w_init, collections=c_names)\n",
    "                    b2 = tf.get_variable('b3', [1, self.num_actions], initializer=b_init, collections=c_names)\n",
    "                    self.A = tf.matmul(l2, w2) + b2\n",
    "                # output Q value layer\n",
    "                with tf.variable_scope('Q'):\n",
    "                    # Q = V(s) + A(s,a)\n",
    "                    out = self.V + (self.A - tf.reduce_mean(self.A, axis=1, keep_dims=True))\n",
    "            else:\n",
    "                # output Q value layer\n",
    "                with tf.variable_scope('Q'):\n",
    "                    w2 = tf.get_variable('w3', [self.hidden_unitshidden_size, self.num_actions], initializer=w_init, collections=c_names)\n",
    "                    b2 = tf.get_variable('b3', [1, self.num_actions], initializer=b_init, collections=c_names)\n",
    "                    out = tf.matmul(l2, w2) + b2\n",
    "            \n",
    "            if summary:\n",
    "                tf.summary.histogram('V', self.V)\n",
    "                tf.summary.histogram('A', self.A)\n",
    "                tf.summary.histogram('Q', out)\n",
    "            \n",
    "            return out\n",
    "        \n",
    "        ## ------------------ build evaluate_net ------------------\n",
    "        # input, i.e state\n",
    "        self.s = tf.placeholder(tf.float32, [None, self.num_features], name='s')\n",
    "        self.q_target = tf.placeholder(tf.float32, [None, self.num_actions], name='q_target')\n",
    "        \n",
    "        if self.prioritized:\n",
    "            self.ISWeights = tf.placeholder(tf.float32, [None, 1], name='IS_weights')\n",
    "        \n",
    "        with tf.variable_scope('eval_net'):\n",
    "            self.q_eval = build_layers(self.s, ['eval_net_params', tf.GraphKeys.GLOBAL_VARIABLES])\n",
    "        \n",
    "        with tf.variable_scope('loss'):\n",
    "            # clip to +/- Rmax\n",
    "            if self.prioritized:\n",
    "                self.abs_errors = tf.reduce_sum(tf.abs(self.q_target - self.q_eval), axis=1)\n",
    "                self.loss = tf.reduce_mean(self.ISWeights * tf.squared_difference(self.q_target, self.q_eval)) + \\\n",
    "                            self.lamda * tf.reduce_sum(tf.maximum(tf.abs(self.q_eval) - self.rmax, 0))\n",
    "\n",
    "            else:\n",
    "                self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval)) + \\\n",
    "                            self.lamda * tf.reduce_sum(tf.maximum(tf.abs(self.q_eval) - self.rmax, 0))\n",
    "                    \n",
    "        with tf.variable_scope('train'):\n",
    "            self._train_op = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "        \n",
    "        with tf.variable_scope('predict'):\n",
    "            self.predict = tf.argmax(self.q_eval, 1, name='predict')\n",
    "        \n",
    "        ## ------------------ build target_net ------------------\n",
    "        self.s_ = tf.placeholder(tf.float32, [None, self.num_features], name='s_')\n",
    "        \n",
    "        with tf.variable_scope('target_net'):\n",
    "            self.q_next = build_layers(self.s_, ['target_net_params', tf.GraphKeys.GLOBAL_VARIABLES], summary=False)\n",
    "        \n",
    "        ## ------------------ summary ------------------\n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "        self.merged = tf.summary.merge_all()\n",
    "        \n",
    "    def store_transition(self, histories):\n",
    "        if self.prioritized:\n",
    "            for history in histories:\n",
    "                self.memory.store(history)\n",
    "        else:\n",
    "            self.memory = histories\n",
    "            #####################\n",
    "            # if with simulator #\n",
    "            #####################\n",
    "            #     def store_transition(self, s, a, r, s_):\n",
    "            #         if not hasattr(self, 'memory_counter'):\n",
    "            #             self.memory_counter = 0\n",
    "            #         transition = np.hstack((s, a, r, s_))\n",
    "            #         # start to replace when full\n",
    "            #         index = self.memory_counter % self.memory_size\n",
    "            #         self.memory[index, :] = transition\n",
    "            #         self.memory_counter += 1\n",
    "\n",
    "            #     def choose_action(self, observation):\n",
    "            #         observation = observation[np.newaxis, :]\n",
    "            #         if np.random.uniform() > self.epsilon:\n",
    "            #             actions_value = self.sess.run(self.q_eval, feed_dict={self.s: observation})\n",
    "            #             action = np.argmax(actions_value)\n",
    "            #         else:\n",
    "            #             action = np.random.randint(0, self.num_actions)\n",
    "            #         return action\n",
    "            \n",
    "    def learn(self):\n",
    "        \n",
    "        if self.learn_step_counter % self.replace_target_iter == 0 and self.learn_step_counter != 0:\n",
    "            self.sess.run(self.sync_target_op)\n",
    "            print('\\ntarget_params_synced\\n')\n",
    "        \n",
    "        if self.prioritized:\n",
    "            tree_idx, batch_memory, ISWeights = self.memory.sample(self.batch_size)\n",
    "        else:\n",
    "            sample_index = np.random.choice(self.memory_size, size=self.batch_size)\n",
    "            batch_memory = self.memory[sample_index, :]\n",
    "        \n",
    "        # next observation\n",
    "        q_next = self.sess.run(self.q_next, feed_dict={self.s_: batch_memory[:, -self.num_features:]})\n",
    "        q_eval = self.sess.run(self.q_eval, {self.s: batch_memory[:, :self.num_features]})\n",
    "\n",
    "        q_target = q_eval.copy()\n",
    "        q_target[q_target > self.rmax] = self.rmax\n",
    "        q_target[q_target < -self.rmax] = - self.rmax\n",
    "        \n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        eval_act_index = batch_memory[:, self.num_features].astype(int)\n",
    "        reward = batch_memory[:, self.num_features + 1]\n",
    "        \n",
    "        q_target[batch_index, eval_act_index] = reward + self.gamma * np.max(q_next, axis=1)\n",
    "        \n",
    "        if self.prioritized:\n",
    "            _, abs_errors, cost, summary = self.sess.run([self._train_op, self.abs_errors, self.loss, self.merged],\n",
    "                                         feed_dict={self.s: batch_memory[:, :self.num_features],\n",
    "                                                    self.q_target: q_target,\n",
    "                                                    self.ISWeights: ISWeights })\n",
    "            self.memory.batch_update(tree_idx, abs_errors) \n",
    "        else:\n",
    "            _, cost, summary = self.sess.run([self._train_op, self.loss, self.merged],\n",
    "                                         feed_dict={self.s: batch_memory[:, :self.num_features],\n",
    "                                                    self.q_target: q_target })\n",
    "        \n",
    "        self.writer.add_summary(summary, self.learn_step_counter)\n",
    "        \n",
    "        \n",
    "        if self.e_increment:\n",
    "            if self.epsilon < self.e_max:\n",
    "                self.epsilon += self.e_increment \n",
    "            else:\n",
    "                self.epsilon = self.e_max\n",
    "            \n",
    "        self.learn_step_counter += 1\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def evaluate(self, val_set):\n",
    "        \n",
    "        s, a, r, s_ = val_set[sample_index, :], val_set[:, self.num_features].astype(int), \\\n",
    "                      val_set[:, self.num_features + 1], val_set[:, -self.num_features:]\n",
    "        \n",
    "        q_next = self.sess.run(self.q_next, feed_dict={self.s_: s_})\n",
    "        q_eval = self.sess.run(self.q_eval, {self.s: s})\n",
    "        \n",
    "        a_q_eval = sess.run(self.predict, feed_dict={self.s: s_})\n",
    "        \n",
    "        q_next = sess.run(self.q_next, feed_dict={self.s_: s_})\n",
    "        \n",
    "        #double_q_value = q_next[range(self.batch_size), actions_q_eval]\n",
    "        q_target = r + self.gamma * np.max(q_next, axis=1)\n",
    "        \n",
    "        _, abs_errors, cost, summary = self.sess.run([self.abs_errors, self.loss, self.merged],\n",
    "                                         feed_dict={self.s: batch_memory[:, :self.num_features],\n",
    "                                                    self.q_target: q_target,\n",
    "                                                    self.ISWeights: ISWeights })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, histories, val_histories, num_epoches=10):\n",
    "    # shuffle histories\n",
    "    np.random.shuffle(histories)\n",
    "    # insert memory\n",
    "    model.store_transition(histories[:model.memory_size,:])\n",
    "    # learn\n",
    "    for epoch in range(num_epoches):\n",
    "        loss = model.learn()\n",
    "        #if epoch % 10 == 0:\n",
    "        print ( 'epoch:{}, loss:{}'.format(epoch, loss) )\n",
    "        \n",
    "        if epoch % 100:\n",
    "            # eval\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import discretize_sepsis_actions as discretizer\n",
    "import pickle as pkl\n",
    "\n",
    "class PatientRecordProcessor:\n",
    "    \n",
    "    def __init__(self, raw_path, cluster_path):\n",
    "        \n",
    "        print ( 'loading dataset ...' )\n",
    "        self.data = np.genfromtxt(raw_path, dtype=float, delimiter=',', skip_header=1)\n",
    "        print ( 'loading clustered states ...' )\n",
    "        self.clusters = pkl.load(open(cluster_path, 'rb'), encoding='latin1')\n",
    "        print ( 'discretizing actions ...' )\n",
    "        self.discretize_actions()\n",
    "        self.patient_map = None\n",
    "        print ( 'initialization succeed' )\n",
    "        \n",
    "    def discretize_actions(self):\n",
    "        \n",
    "        self.action_sequence, self.vaso_bins, self.iv_bins = \\\n",
    "        discretizer.discretize_actions(self.data[:,50], self.data[:,47])\n",
    "    \n",
    "    def build_patient_map(self):\n",
    "        \n",
    "        self.patient_map = {}\n",
    "        interventions = np.setdiff1d(np.arange(47, 57), [51,52,54,55,56])\n",
    "        turcated_data = np.delete(self.data, interventions, axis=1)\n",
    "        turcated_data = np.delete(turcated_data, [0,1,2], axis=1)\n",
    "        turcated_data = (turcated_data - np.mean(turcated_data, axis=0)) / np.std(turcated_data, axis=0)\n",
    "        for i, row in enumerate(self.data):\n",
    "            icuid = str(row[1])\n",
    "            state_action_outcome = [self.clusters[i], self.action_sequence[i], row[50], row[47], row[-2]]\n",
    "            if icuid not in self.patient_map:\n",
    "                # state_id, action, outcome\n",
    "                self.patient_map[icuid] = {\n",
    "                    'age':row[4], 'gender':row[3], 'sa':[state_action_outcome],\n",
    "                    'obser':[turcated_data[i,:]]}\n",
    "            else:\n",
    "                self.patient_map[icuid]['sa'].append(state_action_outcome)\n",
    "                self.patient_map[icuid]['obser'].append(turcated_data[i,:])\n",
    "        \n",
    "        return self.patient_map\n",
    "    \n",
    "    def build_training_history(self):\n",
    "        memory = []\n",
    "        if not self.patient_map:\n",
    "            print ( 'building patient map ...' )\n",
    "            self.patient_map = self.build_patient_map()\n",
    "        \n",
    "        for _, patient in self.patient_map.items():\n",
    "            \n",
    "            if len(patient['sa']) <= 5:\n",
    "                continue\n",
    "\n",
    "            for i, patient_icu_stay in enumerate(patient['sa']):\n",
    "                _, action, _, _, outcome = patient_icu_stay\n",
    "                s_obser = patient['obser'][i]\n",
    "                next_s_obser = patient['obser'][i + 1]\n",
    "                \n",
    "                reward = 0\n",
    "                if (i + 1) == len(patient['sa']) - 1:\n",
    "                    # last stay, check the outcome\n",
    "                    if patient['sa'][i + 1][-1] == 0: \n",
    "                        # survived\n",
    "                        reward = 15\n",
    "                    else:\n",
    "                        reward = -15\n",
    "                \n",
    "                memory.append(np.hstack((s_obser, action, reward, next_s_obser)))\n",
    "                \n",
    "                if reward != 0:\n",
    "                    break\n",
    "        return np.array(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset ...\n",
      "loading clustered states ...\n",
      "discretizing actions ...\n",
      "initialization succeed\n"
     ]
    }
   ],
   "source": [
    "prefix = '../../Dataset/'\n",
    "prp = PatientRecordProcessor(prefix + 'Sepsis_imp.csv', prefix + 'states_list.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building patient map ...\n"
     ]
    }
   ],
   "source": [
    "histories = prp.build_training_history()\n",
    "np.random.shuffle(histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, dev split\n",
    "train_histories = histories[:int(0.8 * len(histories))]\n",
    "test_histories = histories[int(0.8 * len(histories)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((184936, 104), (46235, 104))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_histories.shape, test_histories.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss:0.6550407409667969\n",
      "epoch:1, loss:0.2583269476890564\n",
      "epoch:2, loss:0.16637790203094482\n",
      "epoch:3, loss:0.1368243247270584\n",
      "epoch:4, loss:0.024408211931586266\n",
      "epoch:5, loss:0.24250836670398712\n",
      "epoch:6, loss:0.23718848824501038\n",
      "epoch:7, loss:0.43445315957069397\n",
      "epoch:8, loss:0.8171199560165405\n",
      "epoch:9, loss:0.37803053855895996\n",
      "epoch:10, loss:0.1290515959262848\n",
      "epoch:11, loss:0.12779027223587036\n",
      "epoch:12, loss:0.15062865614891052\n",
      "epoch:13, loss:0.22090034186840057\n",
      "epoch:14, loss:0.35148221254348755\n",
      "epoch:15, loss:0.5780783295631409\n",
      "epoch:16, loss:0.24954169988632202\n",
      "epoch:17, loss:0.10402794182300568\n",
      "epoch:18, loss:0.1295078843832016\n",
      "epoch:19, loss:0.37378600239753723\n",
      "epoch:20, loss:0.20504017174243927\n",
      "epoch:21, loss:0.21731996536254883\n",
      "epoch:22, loss:0.3668731451034546\n",
      "epoch:23, loss:0.551253616809845\n",
      "epoch:24, loss:0.13052313029766083\n",
      "epoch:25, loss:0.3195194900035858\n",
      "epoch:26, loss:0.40807002782821655\n",
      "epoch:27, loss:0.11454372107982635\n",
      "epoch:28, loss:0.4916286766529083\n",
      "epoch:29, loss:0.6552172899246216\n",
      "epoch:30, loss:0.39808326959609985\n",
      "epoch:31, loss:0.3504616618156433\n",
      "epoch:32, loss:0.23735061287879944\n",
      "epoch:33, loss:0.3806745409965515\n",
      "epoch:34, loss:0.44317835569381714\n",
      "epoch:35, loss:0.21229560673236847\n",
      "epoch:36, loss:0.2675146162509918\n",
      "epoch:37, loss:0.33973079919815063\n",
      "epoch:38, loss:0.44803526997566223\n",
      "epoch:39, loss:0.45465224981307983\n",
      "epoch:40, loss:0.18582451343536377\n",
      "epoch:41, loss:0.4461103081703186\n",
      "epoch:42, loss:0.10308410972356796\n",
      "epoch:43, loss:0.006766908336430788\n",
      "epoch:44, loss:0.09124162793159485\n",
      "epoch:45, loss:0.3681129217147827\n",
      "epoch:46, loss:0.39895761013031006\n",
      "epoch:47, loss:0.08475574851036072\n",
      "epoch:48, loss:0.17342206835746765\n",
      "epoch:49, loss:0.09083486348390579\n",
      "epoch:50, loss:0.16829338669776917\n",
      "epoch:51, loss:0.46413421630859375\n",
      "epoch:52, loss:0.10344385355710983\n",
      "epoch:53, loss:0.17653518915176392\n",
      "epoch:54, loss:0.09205041825771332\n",
      "epoch:55, loss:0.16108398139476776\n",
      "epoch:56, loss:0.16121889650821686\n",
      "epoch:57, loss:0.3894116282463074\n",
      "epoch:58, loss:0.15475314855575562\n",
      "epoch:59, loss:0.1676972210407257\n",
      "epoch:60, loss:0.3045286238193512\n",
      "epoch:61, loss:0.24241718649864197\n",
      "epoch:62, loss:0.09508831799030304\n",
      "epoch:63, loss:0.2220279723405838\n",
      "epoch:64, loss:0.008034137077629566\n",
      "epoch:65, loss:0.3802742660045624\n",
      "epoch:66, loss:0.07049484550952911\n",
      "epoch:67, loss:0.3093569278717041\n",
      "epoch:68, loss:0.23407569527626038\n",
      "epoch:69, loss:0.263913631439209\n",
      "epoch:70, loss:0.2573351263999939\n",
      "epoch:71, loss:0.07012459635734558\n",
      "epoch:72, loss:0.08512132614850998\n",
      "epoch:73, loss:0.23826664686203003\n",
      "epoch:74, loss:0.2566973567008972\n",
      "epoch:75, loss:0.21413619816303253\n",
      "epoch:76, loss:0.15943671762943268\n",
      "epoch:77, loss:0.32245340943336487\n",
      "epoch:78, loss:0.5210516452789307\n",
      "epoch:79, loss:0.2390289306640625\n",
      "epoch:80, loss:0.14546173810958862\n",
      "epoch:81, loss:0.08128604292869568\n",
      "epoch:82, loss:0.23080739378929138\n",
      "epoch:83, loss:0.21797627210617065\n",
      "epoch:84, loss:0.19744578003883362\n",
      "epoch:85, loss:0.09721310436725616\n",
      "epoch:86, loss:0.08783690631389618\n",
      "epoch:87, loss:0.08612470328807831\n",
      "epoch:88, loss:0.21530891954898834\n",
      "epoch:89, loss:0.09594574570655823\n",
      "epoch:90, loss:0.31097766757011414\n",
      "epoch:91, loss:0.07378128170967102\n",
      "epoch:92, loss:0.08518818020820618\n",
      "epoch:93, loss:0.008940892294049263\n",
      "epoch:94, loss:0.06606872379779816\n",
      "epoch:95, loss:0.018162082880735397\n",
      "epoch:96, loss:0.21694380044937134\n",
      "epoch:97, loss:0.13426704704761505\n",
      "epoch:98, loss:0.2808118164539337\n",
      "epoch:99, loss:0.011504709720611572\n",
      "epoch:100, loss:0.2834888696670532\n",
      "epoch:101, loss:0.1458369493484497\n",
      "epoch:102, loss:0.07239612191915512\n",
      "epoch:103, loss:0.08106625825166702\n",
      "epoch:104, loss:0.2017640322446823\n",
      "epoch:105, loss:0.07381885498762131\n",
      "epoch:106, loss:0.13499008119106293\n",
      "epoch:107, loss:0.3674122095108032\n",
      "epoch:108, loss:0.00929737463593483\n",
      "epoch:109, loss:0.24874839186668396\n",
      "epoch:110, loss:0.0687459260225296\n",
      "epoch:111, loss:0.076722651720047\n",
      "epoch:112, loss:0.09131832420825958\n",
      "epoch:113, loss:0.014839153736829758\n",
      "epoch:114, loss:0.25567519664764404\n",
      "epoch:115, loss:0.19430507719516754\n",
      "epoch:116, loss:0.20205523073673248\n",
      "epoch:117, loss:0.15871551632881165\n",
      "epoch:118, loss:0.1860167533159256\n",
      "epoch:119, loss:0.1694694459438324\n",
      "epoch:120, loss:0.06306911259889603\n",
      "epoch:121, loss:0.11919563263654709\n",
      "epoch:122, loss:0.009636443108320236\n",
      "epoch:123, loss:0.18608397245407104\n",
      "epoch:124, loss:0.2794455289840698\n",
      "epoch:125, loss:0.130912646651268\n",
      "epoch:126, loss:0.2454802393913269\n",
      "epoch:127, loss:0.32039549946784973\n",
      "epoch:128, loss:0.069686658680439\n",
      "epoch:129, loss:0.137406125664711\n",
      "epoch:130, loss:0.07406769692897797\n",
      "epoch:131, loss:0.20319345593452454\n",
      "epoch:132, loss:0.07478749006986618\n",
      "epoch:133, loss:0.2176019251346588\n",
      "epoch:134, loss:0.12510301172733307\n",
      "epoch:135, loss:0.11674247682094574\n",
      "epoch:136, loss:0.12359766662120819\n",
      "epoch:137, loss:0.06289667636156082\n",
      "epoch:138, loss:0.18736697733402252\n",
      "epoch:139, loss:0.2761572599411011\n",
      "epoch:140, loss:0.07187808305025101\n",
      "epoch:141, loss:0.3095097839832306\n",
      "epoch:142, loss:0.22521530091762543\n",
      "epoch:143, loss:0.18905006349086761\n",
      "epoch:144, loss:0.14585773646831512\n",
      "epoch:145, loss:0.07069109380245209\n",
      "epoch:146, loss:0.12250740081071854\n",
      "epoch:147, loss:0.11718495190143585\n",
      "epoch:148, loss:0.12419898808002472\n",
      "epoch:149, loss:0.19872459769248962\n",
      "epoch:150, loss:0.071233369410038\n",
      "epoch:151, loss:0.20673716068267822\n",
      "epoch:152, loss:0.09923123568296432\n",
      "epoch:153, loss:0.13077178597450256\n",
      "epoch:154, loss:0.10342764109373093\n",
      "epoch:155, loss:0.06854052096605301\n",
      "epoch:156, loss:0.18858343362808228\n",
      "epoch:157, loss:0.13042785227298737\n",
      "epoch:158, loss:0.009693553671240807\n",
      "epoch:159, loss:0.16958658397197723\n",
      "epoch:160, loss:0.1623643934726715\n",
      "epoch:161, loss:0.11948281526565552\n",
      "epoch:162, loss:0.11845064908266068\n",
      "epoch:163, loss:0.1717284917831421\n",
      "epoch:164, loss:0.06515082716941833\n",
      "epoch:165, loss:0.16505025327205658\n",
      "epoch:166, loss:0.28669869899749756\n",
      "epoch:167, loss:0.06928719580173492\n",
      "epoch:168, loss:0.12620052695274353\n",
      "epoch:169, loss:0.17881357669830322\n",
      "epoch:170, loss:0.11802078038454056\n",
      "epoch:171, loss:0.1746125966310501\n",
      "epoch:172, loss:0.1125173568725586\n",
      "epoch:173, loss:0.06245321407914162\n",
      "epoch:174, loss:0.1625692993402481\n",
      "epoch:175, loss:0.17950889468193054\n",
      "epoch:176, loss:0.059226132929325104\n",
      "epoch:177, loss:0.0642542839050293\n",
      "epoch:178, loss:0.21760293841362\n",
      "epoch:179, loss:0.014655993320047855\n",
      "epoch:180, loss:0.20146691799163818\n",
      "epoch:181, loss:0.10258832573890686\n",
      "epoch:182, loss:0.1814383715391159\n",
      "epoch:183, loss:0.12900738418102264\n",
      "epoch:184, loss:0.2641385793685913\n",
      "epoch:185, loss:0.07080882787704468\n",
      "epoch:186, loss:0.10936865210533142\n",
      "epoch:187, loss:0.1817602664232254\n",
      "epoch:188, loss:0.12077026069164276\n",
      "epoch:189, loss:0.0065071964636445045\n",
      "epoch:190, loss:0.006511138752102852\n",
      "epoch:191, loss:0.1075778603553772\n",
      "epoch:192, loss:0.32849743962287903\n",
      "epoch:193, loss:0.14645561575889587\n",
      "epoch:194, loss:0.06100935488939285\n",
      "epoch:195, loss:0.057556863874197006\n",
      "epoch:196, loss:0.10566000640392303\n",
      "epoch:197, loss:0.06320823729038239\n",
      "epoch:198, loss:0.10511210560798645\n",
      "epoch:199, loss:0.2205132693052292\n",
      "\n",
      "target_params_synced\n",
      "\n",
      "epoch:200, loss:0.13387803733348846\n",
      "epoch:201, loss:0.11822530627250671\n",
      "epoch:202, loss:0.09587978571653366\n",
      "epoch:203, loss:0.0672454908490181\n",
      "epoch:204, loss:0.05982939153909683\n",
      "epoch:205, loss:0.15040957927703857\n",
      "epoch:206, loss:0.2697216272354126\n",
      "epoch:207, loss:0.11975326389074326\n",
      "epoch:208, loss:0.2850308120250702\n",
      "epoch:209, loss:0.2737061679363251\n",
      "epoch:210, loss:0.18103325366973877\n",
      "epoch:211, loss:0.1734432578086853\n",
      "epoch:212, loss:0.0629175454378128\n",
      "epoch:213, loss:0.06479618698358536\n",
      "epoch:214, loss:0.05522017925977707\n",
      "epoch:215, loss:0.21376743912696838\n",
      "epoch:216, loss:0.05261220782995224\n",
      "epoch:217, loss:0.2904367744922638\n",
      "epoch:218, loss:0.16512875258922577\n",
      "epoch:219, loss:0.13404807448387146\n",
      "epoch:220, loss:0.1012745276093483\n",
      "epoch:221, loss:0.16139250993728638\n",
      "epoch:222, loss:0.33338043093681335\n",
      "epoch:223, loss:0.2031852751970291\n",
      "epoch:224, loss:0.2035224884748459\n",
      "epoch:225, loss:0.15921363234519958\n",
      "epoch:226, loss:0.14688538014888763\n",
      "epoch:227, loss:0.10555591434240341\n",
      "epoch:228, loss:0.20725852251052856\n",
      "epoch:229, loss:0.1028621643781662\n",
      "epoch:230, loss:0.2032453566789627\n",
      "epoch:231, loss:0.155276820063591\n",
      "epoch:232, loss:0.11333619803190231\n",
      "epoch:233, loss:0.09834872931241989\n",
      "epoch:234, loss:0.05269341543316841\n",
      "epoch:235, loss:0.050498418509960175\n",
      "epoch:236, loss:0.1400664895772934\n",
      "epoch:237, loss:0.15290075540542603\n",
      "epoch:238, loss:0.06346356123685837\n",
      "epoch:239, loss:0.050420112907886505\n",
      "epoch:240, loss:0.10631853342056274\n",
      "epoch:241, loss:0.0465087890625\n",
      "epoch:242, loss:0.18582192063331604\n",
      "epoch:243, loss:0.09412457793951035\n",
      "epoch:244, loss:0.0470164529979229\n",
      "epoch:245, loss:0.14123696088790894\n",
      "epoch:246, loss:0.14660465717315674\n",
      "epoch:247, loss:0.08961844444274902\n",
      "epoch:248, loss:0.13674315810203552\n",
      "epoch:249, loss:0.004392003640532494\n",
      "epoch:250, loss:0.09829490631818771\n",
      "epoch:251, loss:0.18845005333423615\n",
      "epoch:252, loss:0.04436486214399338\n",
      "epoch:253, loss:0.14104054868221283\n",
      "epoch:254, loss:0.04833318665623665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:255, loss:0.13311931490898132\n",
      "epoch:256, loss:0.2715395390987396\n",
      "epoch:257, loss:0.0024360574316233397\n",
      "epoch:258, loss:0.12720920145511627\n",
      "epoch:259, loss:0.17699138820171356\n",
      "epoch:260, loss:0.04841721057891846\n",
      "epoch:261, loss:0.14510922133922577\n",
      "epoch:262, loss:0.17057348787784576\n",
      "epoch:263, loss:0.003297932678833604\n",
      "epoch:264, loss:0.09684440493583679\n",
      "epoch:265, loss:0.09503313153982162\n",
      "epoch:266, loss:0.08394966274499893\n",
      "epoch:267, loss:0.08137529343366623\n",
      "epoch:268, loss:0.12038502842187881\n",
      "epoch:269, loss:0.17488685250282288\n",
      "epoch:270, loss:0.13977503776550293\n",
      "epoch:271, loss:0.13534261286258698\n",
      "epoch:272, loss:0.09795665740966797\n",
      "epoch:273, loss:0.0749162957072258\n",
      "epoch:274, loss:0.09135134518146515\n",
      "epoch:275, loss:0.0911138728260994\n",
      "epoch:276, loss:0.08520255982875824\n",
      "epoch:277, loss:0.2563992738723755\n",
      "epoch:278, loss:0.04892833158373833\n",
      "epoch:279, loss:0.1679902821779251\n",
      "epoch:280, loss:0.043504659086465836\n",
      "epoch:281, loss:0.044680096209049225\n",
      "epoch:282, loss:0.12314436584711075\n",
      "epoch:283, loss:0.06049327552318573\n",
      "epoch:284, loss:0.09379070997238159\n",
      "epoch:285, loss:0.1320187896490097\n",
      "epoch:286, loss:0.05143769085407257\n",
      "epoch:287, loss:0.15317636728286743\n",
      "epoch:288, loss:0.2054322361946106\n",
      "epoch:289, loss:0.10373535007238388\n",
      "epoch:290, loss:0.23558349907398224\n",
      "epoch:291, loss:0.00409496296197176\n",
      "epoch:292, loss:0.08111865073442459\n",
      "epoch:293, loss:0.07978111505508423\n",
      "epoch:294, loss:0.1199726015329361\n",
      "epoch:295, loss:0.15236175060272217\n",
      "epoch:296, loss:0.07691176235675812\n",
      "epoch:297, loss:0.1464800387620926\n",
      "epoch:298, loss:0.003858066163957119\n",
      "epoch:299, loss:0.07693658024072647\n",
      "epoch:300, loss:0.0811246782541275\n",
      "epoch:301, loss:0.004280189983546734\n",
      "epoch:302, loss:0.12369071692228317\n",
      "epoch:303, loss:0.036337949335575104\n",
      "epoch:304, loss:0.1953168511390686\n",
      "epoch:305, loss:0.10164731740951538\n",
      "epoch:306, loss:0.04158549755811691\n",
      "epoch:307, loss:0.07709226757287979\n",
      "epoch:308, loss:0.004299449734389782\n",
      "epoch:309, loss:0.07917682826519012\n",
      "epoch:310, loss:0.15017087757587433\n",
      "epoch:311, loss:0.10514098405838013\n",
      "epoch:312, loss:0.1845351755619049\n",
      "epoch:313, loss:0.08026191592216492\n",
      "epoch:314, loss:0.03849197179079056\n",
      "epoch:315, loss:0.03711647912859917\n",
      "epoch:316, loss:0.03943032771348953\n",
      "epoch:317, loss:0.08233484625816345\n",
      "epoch:318, loss:0.0391349159181118\n",
      "epoch:319, loss:0.03548790514469147\n",
      "epoch:320, loss:0.07880349457263947\n",
      "epoch:321, loss:0.06840944290161133\n",
      "epoch:322, loss:0.004140989854931831\n",
      "epoch:323, loss:0.10843075811862946\n",
      "epoch:324, loss:0.2179560661315918\n",
      "epoch:325, loss:0.0978005975484848\n",
      "epoch:326, loss:0.075813427567482\n",
      "epoch:327, loss:0.14832563698291779\n",
      "epoch:328, loss:0.07758423686027527\n",
      "epoch:329, loss:0.10419859737157822\n",
      "epoch:330, loss:0.04002852737903595\n",
      "epoch:331, loss:0.10277386754751205\n",
      "epoch:332, loss:0.10367840528488159\n",
      "epoch:333, loss:0.16678467392921448\n",
      "epoch:334, loss:0.20072239637374878\n",
      "epoch:335, loss:0.18269695341587067\n",
      "epoch:336, loss:0.03999631106853485\n",
      "epoch:337, loss:0.039722368121147156\n",
      "epoch:338, loss:0.21641038358211517\n",
      "epoch:339, loss:0.09742815047502518\n",
      "epoch:340, loss:0.0644085705280304\n",
      "epoch:341, loss:0.09271989017724991\n",
      "epoch:342, loss:0.1334797739982605\n",
      "epoch:343, loss:0.03754154592752457\n",
      "epoch:344, loss:0.14044100046157837\n",
      "epoch:345, loss:0.13826438784599304\n",
      "epoch:346, loss:0.03497403487563133\n",
      "epoch:347, loss:0.04202130436897278\n",
      "epoch:348, loss:0.03431984782218933\n",
      "epoch:349, loss:0.09551195800304413\n",
      "epoch:350, loss:0.13200083374977112\n",
      "epoch:351, loss:0.10402700304985046\n",
      "epoch:352, loss:0.7523985505104065\n",
      "epoch:353, loss:0.1039792150259018\n",
      "epoch:354, loss:0.03306002914905548\n",
      "epoch:355, loss:0.09308496117591858\n",
      "epoch:356, loss:0.06932803988456726\n",
      "epoch:357, loss:0.07242060452699661\n",
      "epoch:358, loss:0.005463702604174614\n",
      "epoch:359, loss:0.0040340181440114975\n",
      "epoch:360, loss:0.0638340637087822\n",
      "epoch:361, loss:0.06909585744142532\n",
      "epoch:362, loss:0.061900440603494644\n",
      "epoch:363, loss:0.06912164390087128\n",
      "epoch:364, loss:0.0298239067196846\n",
      "epoch:365, loss:0.031214741989970207\n",
      "epoch:366, loss:0.12932196259498596\n",
      "epoch:367, loss:0.03213764727115631\n",
      "epoch:368, loss:0.03441633656620979\n",
      "epoch:369, loss:0.004539017099887133\n",
      "epoch:370, loss:0.02934112958610058\n",
      "epoch:371, loss:0.06361044198274612\n",
      "epoch:372, loss:0.002137100091204047\n",
      "epoch:373, loss:0.06325285136699677\n",
      "epoch:374, loss:0.004824006464332342\n",
      "epoch:375, loss:0.06336334347724915\n",
      "epoch:376, loss:0.09364496171474457\n",
      "epoch:377, loss:0.09188415855169296\n",
      "epoch:378, loss:0.003469475544989109\n",
      "epoch:379, loss:0.09466906636953354\n",
      "epoch:380, loss:0.0029474382754415274\n",
      "epoch:381, loss:0.08690669387578964\n",
      "epoch:382, loss:0.09467878937721252\n",
      "epoch:383, loss:0.004628842230886221\n",
      "epoch:384, loss:0.15218770503997803\n",
      "epoch:385, loss:0.0037142145447432995\n",
      "epoch:386, loss:0.11107281595468521\n",
      "epoch:387, loss:0.1355155110359192\n",
      "epoch:388, loss:0.03610182926058769\n",
      "epoch:389, loss:0.06151322275400162\n",
      "epoch:390, loss:0.00331650348380208\n",
      "epoch:391, loss:0.0886719822883606\n",
      "epoch:392, loss:0.004092497285455465\n",
      "epoch:393, loss:0.08702843636274338\n",
      "epoch:394, loss:0.06552788615226746\n",
      "epoch:395, loss:0.0872642770409584\n",
      "epoch:396, loss:0.0952492505311966\n",
      "epoch:397, loss:0.13845381140708923\n",
      "epoch:398, loss:0.002626430708914995\n",
      "epoch:399, loss:0.02943897247314453\n",
      "\n",
      "target_params_synced\n",
      "\n",
      "epoch:400, loss:0.16060589253902435\n",
      "epoch:401, loss:0.19061365723609924\n",
      "epoch:402, loss:0.06489260494709015\n",
      "epoch:403, loss:0.03220202028751373\n",
      "epoch:404, loss:0.1716374158859253\n",
      "epoch:405, loss:0.06810786575078964\n",
      "epoch:406, loss:0.09612593054771423\n",
      "epoch:407, loss:0.032910898327827454\n",
      "epoch:408, loss:0.10269073396921158\n",
      "epoch:409, loss:0.2211132049560547\n",
      "epoch:410, loss:0.09163094311952591\n",
      "epoch:411, loss:0.12874270975589752\n",
      "epoch:412, loss:0.06058415770530701\n",
      "epoch:413, loss:0.06109389290213585\n",
      "epoch:414, loss:0.033729784190654755\n",
      "epoch:415, loss:0.03230857476592064\n",
      "epoch:416, loss:0.03023226000368595\n",
      "epoch:417, loss:0.08307649940252304\n",
      "epoch:418, loss:0.0905933603644371\n",
      "epoch:419, loss:0.03556185960769653\n",
      "epoch:420, loss:0.16326354444026947\n",
      "epoch:421, loss:0.05615204945206642\n",
      "epoch:422, loss:0.13604936003684998\n",
      "epoch:423, loss:0.14694799482822418\n",
      "epoch:424, loss:0.11711885780096054\n",
      "epoch:425, loss:0.20904579758644104\n",
      "epoch:426, loss:0.05952554568648338\n",
      "epoch:427, loss:0.06652192771434784\n",
      "epoch:428, loss:0.0844176858663559\n",
      "epoch:429, loss:0.058387622237205505\n",
      "epoch:430, loss:0.08499526232481003\n",
      "epoch:431, loss:0.05039763078093529\n",
      "epoch:432, loss:0.08394022285938263\n",
      "epoch:433, loss:0.027327511459589005\n",
      "epoch:434, loss:0.05483969300985336\n",
      "epoch:435, loss:0.03338945284485817\n",
      "epoch:436, loss:0.08550728112459183\n",
      "epoch:437, loss:0.08375304937362671\n",
      "epoch:438, loss:0.0021761422976851463\n",
      "epoch:439, loss:0.05665817856788635\n",
      "epoch:440, loss:0.030048226937651634\n",
      "epoch:441, loss:0.08552203327417374\n",
      "epoch:442, loss:0.08038180321455002\n",
      "epoch:443, loss:0.061646364629268646\n",
      "epoch:444, loss:0.05474211275577545\n",
      "epoch:445, loss:0.10537233203649521\n",
      "epoch:446, loss:13.977948188781738\n",
      "epoch:447, loss:0.05291331931948662\n",
      "epoch:448, loss:0.0018482202431187034\n",
      "epoch:449, loss:0.03060738369822502\n",
      "epoch:450, loss:0.07826896011829376\n",
      "epoch:451, loss:0.08161153644323349\n",
      "epoch:452, loss:0.031629227101802826\n",
      "epoch:453, loss:0.03333112224936485\n",
      "epoch:454, loss:0.1391209065914154\n",
      "epoch:455, loss:0.07453054189682007\n",
      "epoch:456, loss:0.170832559466362\n",
      "epoch:457, loss:0.11003245413303375\n",
      "epoch:458, loss:0.12898899614810944\n",
      "epoch:459, loss:0.028208289295434952\n",
      "epoch:460, loss:0.029368126764893532\n",
      "epoch:461, loss:0.13309930264949799\n",
      "epoch:462, loss:0.054057855159044266\n",
      "epoch:463, loss:0.05656803026795387\n",
      "epoch:464, loss:0.08058860898017883\n",
      "epoch:465, loss:0.12958106398582458\n",
      "epoch:466, loss:0.07705919444561005\n",
      "epoch:467, loss:0.055132217705249786\n",
      "epoch:468, loss:0.023467490449547768\n",
      "epoch:469, loss:0.03432098776102066\n",
      "epoch:470, loss:0.02440371736884117\n",
      "epoch:471, loss:0.08041153103113174\n",
      "epoch:472, loss:0.0999222919344902\n",
      "epoch:473, loss:0.054371777921915054\n",
      "epoch:474, loss:0.033132877200841904\n",
      "epoch:475, loss:0.08271558582782745\n",
      "epoch:476, loss:0.04347876459360123\n",
      "epoch:477, loss:0.08360597491264343\n",
      "epoch:478, loss:0.07780902832746506\n",
      "epoch:479, loss:0.05170115828514099\n",
      "epoch:480, loss:0.09927358478307724\n",
      "epoch:481, loss:0.15324266254901886\n",
      "epoch:482, loss:0.0536908358335495\n",
      "epoch:483, loss:0.07515831291675568\n",
      "epoch:484, loss:0.04735254868865013\n",
      "epoch:485, loss:0.05312958359718323\n",
      "epoch:486, loss:0.020884091034531593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:487, loss:0.09851408749818802\n",
      "epoch:488, loss:0.12406750023365021\n",
      "epoch:489, loss:0.07502704858779907\n",
      "epoch:490, loss:0.04591481387615204\n",
      "epoch:491, loss:0.07099704444408417\n",
      "epoch:492, loss:0.09752178192138672\n",
      "epoch:493, loss:0.12365209311246872\n",
      "epoch:494, loss:0.11561178416013718\n",
      "epoch:495, loss:0.024396639317274094\n",
      "epoch:496, loss:0.04839065670967102\n",
      "epoch:497, loss:0.02438979409635067\n",
      "epoch:498, loss:0.03784525766968727\n",
      "epoch:499, loss:0.04688504710793495\n",
      "epoch:500, loss:0.07754115760326385\n",
      "epoch:501, loss:0.025441601872444153\n",
      "epoch:502, loss:0.07104778289794922\n",
      "epoch:503, loss:0.06701762229204178\n",
      "epoch:504, loss:0.09894317388534546\n",
      "epoch:505, loss:0.06800040602684021\n",
      "epoch:506, loss:0.02569032832980156\n",
      "epoch:507, loss:0.06782322376966476\n",
      "epoch:508, loss:0.09878097474575043\n",
      "epoch:509, loss:0.0490269660949707\n",
      "epoch:510, loss:0.09125079959630966\n",
      "epoch:511, loss:0.002742262789979577\n",
      "epoch:512, loss:16.53569793701172\n",
      "epoch:513, loss:0.07754848152399063\n",
      "epoch:514, loss:0.026440244168043137\n",
      "epoch:515, loss:0.06197037547826767\n",
      "epoch:516, loss:0.04291178658604622\n",
      "epoch:517, loss:0.11212131381034851\n",
      "epoch:518, loss:0.048476461321115494\n",
      "epoch:519, loss:0.0883958712220192\n",
      "epoch:520, loss:0.04752242937684059\n",
      "epoch:521, loss:0.023349963128566742\n",
      "epoch:522, loss:0.0896797776222229\n",
      "epoch:523, loss:0.06340393424034119\n",
      "epoch:524, loss:0.09429729729890823\n",
      "epoch:525, loss:0.04424240067601204\n",
      "epoch:526, loss:0.045071717351675034\n",
      "epoch:527, loss:0.08543508499860764\n",
      "epoch:528, loss:0.023176684975624084\n",
      "epoch:529, loss:0.002637718804180622\n",
      "epoch:530, loss:0.09613695740699768\n",
      "epoch:531, loss:0.04760991036891937\n",
      "epoch:532, loss:0.02302304096519947\n",
      "epoch:533, loss:0.08802441507577896\n",
      "epoch:534, loss:0.041964128613471985\n",
      "epoch:535, loss:0.08696875721216202\n",
      "epoch:536, loss:0.02161170169711113\n",
      "epoch:537, loss:0.05031479895114899\n",
      "epoch:538, loss:0.06010610610246658\n",
      "epoch:539, loss:0.02552448958158493\n",
      "epoch:540, loss:0.04660660773515701\n",
      "epoch:541, loss:0.05582640320062637\n",
      "epoch:542, loss:0.04009251669049263\n",
      "epoch:543, loss:0.0012368281604722142\n",
      "epoch:544, loss:0.11696925014257431\n",
      "epoch:545, loss:0.020997099578380585\n",
      "epoch:546, loss:0.10814700275659561\n",
      "epoch:547, loss:0.12269386649131775\n",
      "epoch:548, loss:0.02848389558494091\n",
      "epoch:549, loss:0.042773712426424026\n",
      "epoch:550, loss:0.03955797106027603\n",
      "epoch:551, loss:0.04047325626015663\n",
      "epoch:552, loss:0.08477193117141724\n",
      "epoch:553, loss:0.04247720167040825\n",
      "epoch:554, loss:0.044109687209129333\n",
      "epoch:555, loss:0.08083043992519379\n",
      "epoch:556, loss:0.07851780205965042\n",
      "epoch:557, loss:0.03996588662266731\n",
      "epoch:558, loss:0.05781961977481842\n",
      "epoch:559, loss:0.05977790057659149\n",
      "epoch:560, loss:0.020454347133636475\n",
      "epoch:561, loss:0.11832033842802048\n",
      "epoch:562, loss:0.07627712935209274\n",
      "epoch:563, loss:0.0961889997124672\n",
      "epoch:564, loss:0.0985918790102005\n",
      "epoch:565, loss:0.019635789096355438\n",
      "epoch:566, loss:0.04140552505850792\n",
      "epoch:567, loss:0.03936859965324402\n",
      "epoch:568, loss:0.12222794443368912\n",
      "epoch:569, loss:0.01943904720246792\n",
      "epoch:570, loss:0.04099191725254059\n",
      "epoch:571, loss:0.07694074511528015\n",
      "epoch:572, loss:0.0401051864027977\n",
      "epoch:573, loss:0.0022663779091089964\n",
      "epoch:574, loss:0.07749839872121811\n",
      "epoch:575, loss:0.0574825257062912\n",
      "epoch:576, loss:0.0016724520828574896\n",
      "epoch:577, loss:0.10222627967596054\n",
      "epoch:578, loss:0.08774716407060623\n",
      "epoch:579, loss:0.05952785909175873\n",
      "epoch:580, loss:0.0539308600127697\n",
      "epoch:581, loss:0.04050102084875107\n",
      "epoch:582, loss:0.08897983282804489\n",
      "epoch:583, loss:0.018788494169712067\n",
      "epoch:584, loss:0.019931890070438385\n",
      "epoch:585, loss:0.017888138070702553\n",
      "epoch:586, loss:0.03648507595062256\n",
      "epoch:587, loss:0.0539461150765419\n",
      "epoch:588, loss:0.052657417953014374\n",
      "epoch:589, loss:0.05675451457500458\n",
      "epoch:590, loss:0.014993973076343536\n",
      "epoch:591, loss:0.020266080275177956\n",
      "epoch:592, loss:0.03612538427114487\n",
      "epoch:593, loss:0.01797637715935707\n",
      "epoch:594, loss:0.10692542791366577\n",
      "epoch:595, loss:0.0755748450756073\n",
      "epoch:596, loss:0.017882920801639557\n",
      "epoch:597, loss:0.05618954077363014\n",
      "epoch:598, loss:0.04004517197608948\n",
      "epoch:599, loss:0.03588280826807022\n",
      "\n",
      "target_params_synced\n",
      "\n",
      "epoch:600, loss:0.054994527250528336\n",
      "epoch:601, loss:0.0019635555800050497\n",
      "epoch:602, loss:0.0020895004272460938\n",
      "epoch:603, loss:0.05539441108703613\n",
      "epoch:604, loss:0.05856575816869736\n",
      "epoch:605, loss:0.05974284186959267\n",
      "epoch:606, loss:0.07643809169530869\n",
      "epoch:607, loss:0.041141822934150696\n",
      "epoch:608, loss:0.05574443191289902\n",
      "epoch:609, loss:0.05697022005915642\n",
      "epoch:610, loss:0.038415852934122086\n",
      "epoch:611, loss:0.11558196693658829\n",
      "epoch:612, loss:0.0703914538025856\n",
      "epoch:613, loss:0.09469747543334961\n",
      "epoch:614, loss:0.035769857466220856\n",
      "epoch:615, loss:0.01640985533595085\n",
      "epoch:616, loss:0.07377539575099945\n",
      "epoch:617, loss:0.0166837889701128\n",
      "epoch:618, loss:0.0031101107597351074\n",
      "epoch:619, loss:0.09350091964006424\n",
      "epoch:620, loss:0.08916991949081421\n",
      "epoch:621, loss:0.05064407363533974\n",
      "epoch:622, loss:0.03725092113018036\n",
      "epoch:623, loss:0.057891845703125\n",
      "epoch:624, loss:0.05966949835419655\n",
      "epoch:625, loss:0.1271355003118515\n",
      "epoch:626, loss:0.05764072760939598\n",
      "epoch:627, loss:0.055907152593135834\n",
      "epoch:628, loss:0.07089907675981522\n",
      "epoch:629, loss:0.020336270332336426\n",
      "epoch:630, loss:0.03513149172067642\n",
      "epoch:631, loss:0.06718476116657257\n",
      "epoch:632, loss:0.07514318823814392\n",
      "epoch:633, loss:0.08879715204238892\n",
      "epoch:634, loss:0.020073505118489265\n",
      "epoch:635, loss:0.09133230149745941\n",
      "epoch:636, loss:0.025252174586057663\n",
      "epoch:637, loss:0.01895073801279068\n",
      "epoch:638, loss:0.020561696961522102\n",
      "epoch:639, loss:0.019934507086873055\n",
      "epoch:640, loss:0.036349859088659286\n",
      "epoch:641, loss:0.020340193063020706\n",
      "epoch:642, loss:0.05493403598666191\n",
      "epoch:643, loss:0.0015692401211708784\n",
      "epoch:644, loss:0.018829448148608208\n",
      "epoch:645, loss:0.056060608476400375\n",
      "epoch:646, loss:0.05176997184753418\n",
      "epoch:647, loss:0.0363629087805748\n",
      "epoch:648, loss:0.03227818012237549\n",
      "epoch:649, loss:0.09221343696117401\n",
      "epoch:650, loss:0.0019823438487946987\n",
      "epoch:651, loss:0.03438775986433029\n",
      "epoch:652, loss:0.0916559025645256\n",
      "epoch:653, loss:0.03895580768585205\n",
      "epoch:654, loss:0.10479729622602463\n",
      "epoch:655, loss:0.026130311191082\n",
      "epoch:656, loss:0.05590953305363655\n",
      "epoch:657, loss:0.054334841668605804\n",
      "epoch:658, loss:0.0370616689324379\n",
      "epoch:659, loss:0.05842453986406326\n",
      "epoch:660, loss:0.05565152317285538\n",
      "epoch:661, loss:0.03841599076986313\n",
      "epoch:662, loss:0.038489844650030136\n",
      "epoch:663, loss:0.08902228623628616\n",
      "epoch:664, loss:0.03285400569438934\n",
      "epoch:665, loss:0.0016712765209376812\n",
      "epoch:666, loss:0.058095648884773254\n",
      "epoch:667, loss:0.07334017008543015\n",
      "epoch:668, loss:0.019874872639775276\n",
      "epoch:669, loss:0.05521363765001297\n",
      "epoch:670, loss:0.018560077995061874\n",
      "epoch:671, loss:0.035777851939201355\n",
      "epoch:672, loss:0.018101610243320465\n",
      "epoch:673, loss:0.017755554988980293\n",
      "epoch:674, loss:0.07224909961223602\n",
      "epoch:675, loss:0.05418911948800087\n",
      "epoch:676, loss:0.03559300675988197\n",
      "epoch:677, loss:0.018334824591875076\n",
      "epoch:678, loss:0.018494846299290657\n",
      "epoch:679, loss:0.08484435826539993\n",
      "epoch:680, loss:0.04174600541591644\n",
      "epoch:681, loss:0.07613669335842133\n",
      "epoch:682, loss:0.011437475681304932\n",
      "epoch:683, loss:0.015911905094981194\n",
      "epoch:684, loss:0.050867389887571335\n",
      "epoch:685, loss:0.05924910679459572\n",
      "epoch:686, loss:0.10490399599075317\n",
      "epoch:687, loss:0.0014994458761066198\n",
      "epoch:688, loss:0.055047933012247086\n",
      "epoch:689, loss:0.06878671050071716\n",
      "epoch:690, loss:0.09015414863824844\n",
      "epoch:691, loss:0.05211746692657471\n",
      "epoch:692, loss:0.0653892233967781\n",
      "epoch:693, loss:0.019130682572722435\n",
      "epoch:694, loss:0.08584476262331009\n",
      "epoch:695, loss:0.019578445702791214\n",
      "epoch:696, loss:0.0013109014835208654\n",
      "epoch:697, loss:0.05550461262464523\n",
      "epoch:698, loss:0.01990205980837345\n",
      "epoch:699, loss:0.021145939826965332\n",
      "epoch:700, loss:0.033727362751960754\n",
      "epoch:701, loss:0.018774067983031273\n",
      "epoch:702, loss:0.034920867532491684\n",
      "epoch:703, loss:0.0199996680021286\n",
      "epoch:704, loss:0.09986055642366409\n",
      "epoch:705, loss:0.12065669894218445\n",
      "epoch:706, loss:0.0009943874320015311\n",
      "epoch:707, loss:0.0715026929974556\n",
      "epoch:708, loss:0.08985629677772522\n",
      "epoch:709, loss:0.02070622518658638\n",
      "epoch:710, loss:0.0013000258477404714\n",
      "epoch:711, loss:0.0729798972606659\n",
      "epoch:712, loss:0.03916497901082039\n",
      "epoch:713, loss:0.10640954226255417\n",
      "epoch:714, loss:0.0675482526421547\n",
      "epoch:715, loss:0.03359931334853172\n",
      "epoch:716, loss:0.07278702408075333\n",
      "epoch:717, loss:0.037308838218450546\n",
      "epoch:718, loss:0.03848264366388321\n",
      "epoch:719, loss:0.06603502482175827\n",
      "epoch:720, loss:0.06805794686079025\n",
      "epoch:721, loss:0.018555885180830956\n",
      "epoch:722, loss:0.018239878118038177\n",
      "epoch:723, loss:0.020680870860815048\n",
      "epoch:724, loss:0.0696946308016777\n",
      "epoch:725, loss:0.017507120966911316\n",
      "epoch:726, loss:0.03808227553963661\n",
      "epoch:727, loss:0.03640903905034065\n",
      "epoch:728, loss:0.07471142709255219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:729, loss:0.05588691681623459\n",
      "epoch:730, loss:0.04996220022439957\n",
      "epoch:731, loss:0.07256941497325897\n",
      "epoch:732, loss:0.08646894246339798\n",
      "epoch:733, loss:0.055735498666763306\n",
      "epoch:734, loss:0.1002625972032547\n",
      "epoch:735, loss:0.08102128654718399\n",
      "epoch:736, loss:0.035265423357486725\n",
      "epoch:737, loss:0.05764444172382355\n",
      "epoch:738, loss:0.03664056956768036\n",
      "epoch:739, loss:0.019376955926418304\n",
      "epoch:740, loss:0.07063455879688263\n",
      "epoch:741, loss:0.05322069302201271\n",
      "epoch:742, loss:0.05331617221236229\n",
      "epoch:743, loss:0.0013737011468037963\n",
      "epoch:744, loss:0.057674989104270935\n",
      "epoch:745, loss:0.1044001579284668\n",
      "epoch:746, loss:0.09250712394714355\n",
      "epoch:747, loss:0.05155275762081146\n",
      "epoch:748, loss:0.10321429371833801\n",
      "epoch:749, loss:0.036912523210048676\n",
      "epoch:750, loss:0.03713252767920494\n",
      "epoch:751, loss:0.0017841275548562407\n",
      "epoch:752, loss:0.03543808311223984\n",
      "epoch:753, loss:0.0714135393500328\n",
      "epoch:754, loss:0.0017091786721721292\n",
      "epoch:755, loss:0.06781711429357529\n",
      "epoch:756, loss:0.05506518483161926\n",
      "epoch:757, loss:0.01855454407632351\n",
      "epoch:758, loss:0.0016859530005604029\n",
      "epoch:759, loss:0.0010529151186347008\n",
      "epoch:760, loss:0.05127917230129242\n",
      "epoch:761, loss:0.05423411354422569\n",
      "epoch:762, loss:0.10221610218286514\n",
      "epoch:763, loss:0.03708905726671219\n",
      "epoch:764, loss:0.07153123617172241\n",
      "epoch:765, loss:0.05609554797410965\n",
      "epoch:766, loss:0.07206570357084274\n",
      "epoch:767, loss:0.01773074083030224\n",
      "epoch:768, loss:0.04851384460926056\n",
      "epoch:769, loss:0.06711170822381973\n",
      "epoch:770, loss:0.02863561548292637\n",
      "epoch:771, loss:0.03361117094755173\n",
      "epoch:772, loss:0.017680715769529343\n",
      "epoch:773, loss:0.07276064157485962\n",
      "epoch:774, loss:0.037294160574674606\n",
      "epoch:775, loss:0.05866512656211853\n",
      "epoch:776, loss:0.06681250035762787\n",
      "epoch:777, loss:0.018674340099096298\n",
      "epoch:778, loss:0.08181492984294891\n",
      "epoch:779, loss:0.017404429614543915\n",
      "epoch:780, loss:0.0364096499979496\n",
      "epoch:781, loss:0.0013030004920437932\n",
      "epoch:782, loss:0.036100201308727264\n",
      "epoch:783, loss:0.08531586825847626\n",
      "epoch:784, loss:0.07122466713190079\n",
      "epoch:785, loss:0.01809760183095932\n",
      "epoch:786, loss:0.06441976875066757\n",
      "epoch:787, loss:0.0664878636598587\n",
      "epoch:788, loss:0.038345810025930405\n",
      "epoch:789, loss:0.018183693289756775\n",
      "epoch:790, loss:0.03358687087893486\n",
      "epoch:791, loss:0.0012763249687850475\n",
      "epoch:792, loss:0.06874453276395798\n",
      "epoch:793, loss:0.05227121338248253\n",
      "epoch:794, loss:0.038155604153871536\n",
      "epoch:795, loss:0.03565269708633423\n",
      "epoch:796, loss:0.054722681641578674\n",
      "epoch:797, loss:0.051964931190013885\n",
      "epoch:798, loss:0.07037507742643356\n",
      "epoch:799, loss:0.0014718585880473256\n",
      "\n",
      "target_params_synced\n",
      "\n",
      "epoch:800, loss:0.020274080336093903\n",
      "epoch:801, loss:0.053595829755067825\n",
      "epoch:802, loss:0.05437939986586571\n",
      "epoch:803, loss:0.05510038882493973\n",
      "epoch:804, loss:0.07416269928216934\n",
      "epoch:805, loss:0.053624227643013\n",
      "epoch:806, loss:0.02267690747976303\n",
      "epoch:807, loss:0.0616600327193737\n",
      "epoch:808, loss:0.04213588312268257\n",
      "epoch:809, loss:0.019431091845035553\n",
      "epoch:810, loss:0.09093920886516571\n",
      "epoch:811, loss:0.13067960739135742\n",
      "epoch:812, loss:0.0426933616399765\n",
      "epoch:813, loss:0.060620859265327454\n",
      "epoch:814, loss:0.05535854399204254\n",
      "epoch:815, loss:0.002035699784755707\n",
      "epoch:816, loss:0.038275424391031265\n",
      "epoch:817, loss:0.03715881332755089\n",
      "epoch:818, loss:0.002367934677749872\n",
      "epoch:819, loss:0.03673186153173447\n",
      "epoch:820, loss:0.020566072314977646\n",
      "epoch:821, loss:0.06519950181245804\n",
      "epoch:822, loss:0.029485154896974564\n",
      "epoch:823, loss:0.02021826058626175\n",
      "epoch:824, loss:0.03794321417808533\n",
      "epoch:825, loss:0.07124330848455429\n",
      "epoch:826, loss:0.044078875333070755\n",
      "epoch:827, loss:0.03588571399450302\n",
      "epoch:828, loss:0.03657906875014305\n",
      "epoch:829, loss:0.10733596980571747\n",
      "epoch:830, loss:0.043354056775569916\n",
      "epoch:831, loss:0.019740814343094826\n",
      "epoch:832, loss:0.07769850641489029\n",
      "epoch:833, loss:0.019100021570920944\n",
      "epoch:834, loss:0.051503442227840424\n",
      "epoch:835, loss:0.03735838085412979\n",
      "epoch:836, loss:0.07354938983917236\n",
      "epoch:837, loss:0.0017551538767293096\n",
      "epoch:838, loss:0.016530483961105347\n",
      "epoch:839, loss:0.04040323570370674\n",
      "epoch:840, loss:0.04102088138461113\n",
      "epoch:841, loss:0.038018494844436646\n",
      "epoch:842, loss:0.020577071234583855\n",
      "epoch:843, loss:0.07609353959560394\n",
      "epoch:844, loss:0.07561859488487244\n",
      "epoch:845, loss:0.03915875032544136\n",
      "epoch:846, loss:0.03825540095567703\n",
      "epoch:847, loss:0.021338485181331635\n",
      "epoch:848, loss:0.0698096975684166\n",
      "epoch:849, loss:0.1250695288181305\n",
      "epoch:850, loss:0.05799223855137825\n",
      "epoch:851, loss:0.051388129591941833\n",
      "epoch:852, loss:0.03975042328238487\n",
      "epoch:853, loss:0.05524556338787079\n",
      "epoch:854, loss:0.0544130839407444\n",
      "epoch:855, loss:0.020586295053362846\n",
      "epoch:856, loss:0.04729573428630829\n",
      "epoch:857, loss:0.12706100940704346\n",
      "epoch:858, loss:0.5749303698539734\n",
      "epoch:859, loss:0.017661796882748604\n",
      "epoch:860, loss:0.03755633905529976\n",
      "epoch:861, loss:0.05125508829951286\n",
      "epoch:862, loss:0.05365597829222679\n",
      "epoch:863, loss:0.06737930327653885\n",
      "epoch:864, loss:0.06997817009687424\n",
      "epoch:865, loss:0.09261562675237656\n",
      "epoch:866, loss:0.01989312842488289\n",
      "epoch:867, loss:0.05674846097826958\n",
      "epoch:868, loss:0.034212931990623474\n",
      "epoch:869, loss:0.05791594833135605\n",
      "epoch:870, loss:0.07091709226369858\n",
      "epoch:871, loss:0.019128959625959396\n",
      "epoch:872, loss:0.0516747385263443\n",
      "epoch:873, loss:0.03542394936084747\n",
      "epoch:874, loss:0.034427378326654434\n",
      "epoch:875, loss:0.01920974813401699\n",
      "epoch:876, loss:0.036063261330127716\n",
      "epoch:877, loss:0.048986248672008514\n",
      "epoch:878, loss:0.03522901609539986\n",
      "epoch:879, loss:0.0752115473151207\n",
      "epoch:880, loss:0.07304013520479202\n",
      "epoch:881, loss:0.03852568939328194\n",
      "epoch:882, loss:0.0027759515214711428\n",
      "epoch:883, loss:0.022751841694116592\n",
      "epoch:884, loss:0.03813406080007553\n",
      "epoch:885, loss:0.0863405391573906\n",
      "epoch:886, loss:0.01780356653034687\n",
      "epoch:887, loss:0.03621670603752136\n",
      "epoch:888, loss:0.022002218291163445\n",
      "epoch:889, loss:0.019705360755324364\n",
      "epoch:890, loss:0.03528137877583504\n",
      "epoch:891, loss:0.03712962567806244\n",
      "epoch:892, loss:71.22235870361328\n",
      "epoch:893, loss:0.03895922750234604\n",
      "epoch:894, loss:0.07329341769218445\n",
      "epoch:895, loss:0.05398339778184891\n",
      "epoch:896, loss:0.05751622095704079\n",
      "epoch:897, loss:0.048052988946437836\n",
      "epoch:898, loss:0.002885028487071395\n",
      "epoch:899, loss:0.07365787774324417\n",
      "epoch:900, loss:0.03847774490714073\n",
      "epoch:901, loss:0.06218256056308746\n",
      "epoch:902, loss:0.020018354058265686\n",
      "epoch:903, loss:0.056166693568229675\n",
      "epoch:904, loss:0.050739873200654984\n",
      "epoch:905, loss:0.02287454903125763\n",
      "epoch:906, loss:0.0017853602766990662\n",
      "epoch:907, loss:0.10266932845115662\n",
      "epoch:908, loss:0.018809203058481216\n",
      "epoch:909, loss:0.05735073983669281\n",
      "epoch:910, loss:0.11096164584159851\n",
      "epoch:911, loss:0.09681687504053116\n",
      "epoch:912, loss:0.0042142863385379314\n",
      "epoch:913, loss:0.05486491322517395\n",
      "epoch:914, loss:0.08687303215265274\n",
      "epoch:915, loss:0.07925771921873093\n",
      "epoch:916, loss:0.1279458999633789\n",
      "epoch:917, loss:0.05722300708293915\n",
      "epoch:918, loss:0.04930351674556732\n",
      "epoch:919, loss:0.021566525101661682\n",
      "epoch:920, loss:0.08298105001449585\n",
      "epoch:921, loss:0.03667561709880829\n",
      "epoch:922, loss:0.0767534077167511\n",
      "epoch:923, loss:0.08940455317497253\n",
      "epoch:924, loss:0.055164944380521774\n",
      "epoch:925, loss:0.056712280958890915\n",
      "epoch:926, loss:0.034899596124887466\n",
      "epoch:927, loss:0.05323044955730438\n",
      "epoch:928, loss:0.0871962457895279\n",
      "epoch:929, loss:0.11162835359573364\n",
      "epoch:930, loss:0.07349822670221329\n",
      "epoch:931, loss:0.004141127225011587\n",
      "epoch:932, loss:0.018844129517674446\n",
      "epoch:933, loss:0.07918276637792587\n",
      "epoch:934, loss:0.04942977800965309\n",
      "epoch:935, loss:0.051180366426706314\n",
      "epoch:936, loss:0.017816642299294472\n",
      "epoch:937, loss:0.024106215685606003\n",
      "epoch:938, loss:0.03730791434645653\n",
      "epoch:939, loss:0.05811963975429535\n",
      "epoch:940, loss:0.06508047878742218\n",
      "epoch:941, loss:0.05675267055630684\n",
      "epoch:942, loss:0.08283106982707977\n",
      "epoch:943, loss:0.01740701124072075\n",
      "epoch:944, loss:0.019709624350070953\n",
      "epoch:945, loss:0.0520160086452961\n",
      "epoch:946, loss:0.05167413875460625\n",
      "epoch:947, loss:0.0634014680981636\n",
      "epoch:948, loss:0.0885855183005333\n",
      "epoch:949, loss:0.040009330958127975\n",
      "epoch:950, loss:0.07063787430524826\n",
      "epoch:951, loss:0.07396320253610611\n",
      "epoch:952, loss:0.022386686876416206\n",
      "epoch:953, loss:0.057334136217832565\n",
      "epoch:954, loss:0.06506545096635818\n",
      "epoch:955, loss:0.03805791959166527\n",
      "epoch:956, loss:0.017806172370910645\n",
      "epoch:957, loss:0.05768635869026184\n",
      "epoch:958, loss:0.018529746681451797\n",
      "epoch:959, loss:0.03834216296672821\n",
      "epoch:960, loss:0.03710399940609932\n",
      "epoch:961, loss:0.07499685883522034\n",
      "epoch:962, loss:0.0563790425658226\n",
      "epoch:963, loss:0.03860767185688019\n",
      "epoch:964, loss:0.07022060453891754\n",
      "epoch:965, loss:0.022519178688526154\n",
      "epoch:966, loss:0.034516144543886185\n",
      "epoch:967, loss:0.09064531326293945\n",
      "epoch:968, loss:0.02016662433743477\n",
      "epoch:969, loss:0.06817754358053207\n",
      "epoch:970, loss:0.03002423793077469\n",
      "epoch:971, loss:0.0017844508402049541\n",
      "epoch:972, loss:0.002801981521770358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:973, loss:0.05626550689339638\n",
      "epoch:974, loss:0.05539660528302193\n",
      "epoch:975, loss:0.05231013149023056\n",
      "epoch:976, loss:0.03528013452887535\n",
      "epoch:977, loss:0.07224494963884354\n",
      "epoch:978, loss:0.020199723541736603\n",
      "epoch:979, loss:0.03666631877422333\n",
      "epoch:980, loss:0.05047769844532013\n",
      "epoch:981, loss:0.03570803254842758\n",
      "epoch:982, loss:0.05648984760046005\n",
      "epoch:983, loss:0.2626574635505676\n",
      "epoch:984, loss:0.05288871377706528\n",
      "epoch:985, loss:0.03711981698870659\n",
      "epoch:986, loss:0.07394415885210037\n",
      "epoch:987, loss:0.09499017894268036\n",
      "epoch:988, loss:0.0024957398418337107\n",
      "epoch:989, loss:0.035993412137031555\n",
      "epoch:990, loss:0.0191359780728817\n",
      "epoch:991, loss:0.03681506961584091\n",
      "epoch:992, loss:0.05475335195660591\n",
      "epoch:993, loss:0.05860821157693863\n",
      "epoch:994, loss:0.05765843391418457\n",
      "epoch:995, loss:0.07555708289146423\n",
      "epoch:996, loss:0.05259719491004944\n",
      "epoch:997, loss:0.0679333359003067\n",
      "epoch:998, loss:0.0027926659677177668\n",
      "epoch:999, loss:0.0781489685177803\n"
     ]
    }
   ],
   "source": [
    "### main\n",
    "tf.reset_default_graph()\n",
    "\n",
    "MEMORY_SIZE = train_histories.shape[0]\n",
    "ACTION_SPACE = 25\n",
    "FEATURES = 51\n",
    "NUM_EPOCHES = 1000\n",
    "\n",
    "## speicify output_path while tunning parameters\n",
    "with tf.variable_scope('dueling'):\n",
    "    dueling_DQN = DQN(\n",
    "        num_actions=ACTION_SPACE, num_features=FEATURES, batch_size=32, mem_size=MEMORY_SIZE,\n",
    "        dueling=True, output_graph=True)\n",
    "\n",
    "train(dueling_DQN, train_histories, num_epoches=NUM_EPOCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
