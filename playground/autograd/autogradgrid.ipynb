{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 2.77258872224\n",
      "Trained loss: 0.389007543156\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 0.5*(np.tanh(x) + 1)\n",
    "\n",
    "def logistic_predictions(weights, inputs):\n",
    "    # Outputs probability of a label being true according to logistic model.\n",
    "    return sigmoid(np.dot(inputs, weights))\n",
    "\n",
    "def training_loss(weights):\n",
    "    # Training loss is the negative log-likelihood of the training labels.\n",
    "    preds = logistic_predictions(weights, inputs)\n",
    "    label_probabilities = preds * targets + (1 - preds) * (1 - targets)\n",
    "    return -np.sum(np.log(label_probabilities))\n",
    "\n",
    "# Build a toy dataset.\n",
    "inputs = np.array([[0.52, 1.12,  0.77],\n",
    "                   [0.88, -1.08, 0.15],\n",
    "                   [0.52, 0.06, -1.30],\n",
    "                   [0.74, -2.49, 1.39]])\n",
    "targets = np.array([True, True, False, True])\n",
    "\n",
    "# Define a function that returns gradients of training loss using Autograd.\n",
    "training_gradient_fun = grad(training_loss)\n",
    "\n",
    "# Optimize weights using gradient descent.\n",
    "weights = np.array([0.0, 0.0, 0.0])\n",
    "print (\"Initial loss:\", training_loss(weights))\n",
    "for i in range(100):\n",
    "    weights -= training_gradient_fun(weights) * 0.01\n",
    "\n",
    "print  (\"Trained loss:\", training_loss(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# general imports \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "#import autograd.numpy as np\n",
    "\n",
    "import mymacore as ma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALsAAADxCAYAAACTU8ZgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7EZkMLEbkcHEbkQGE7sRGf4f3a+J7oDhG2kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1155ea518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0\n",
      "5 1\n",
      "5 2\n",
      "5 3\n",
      "5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/wihl/Projects/Courses/17Fall/CS282/cs282-f17-xuefeng-yi-david/playground/autograd/rl_functions.py:376: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  estimator = np.sum( individual_trial_estimators ) / normalization\n",
      "/Users/wihl/Projects/Courses/17Fall/CS282/cs282-f17-xuefeng-yi-david/playground/autograd/rl_functions.py:328: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  w = rho / weights_normalization[ t_within_trial ]\n",
      "/Users/wihl/Projects/Courses/17Fall/CS282/cs282-f17-xuefeng-yi-david/playground/autograd/rl_functions.py:468: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  w = rho / weights_normalization[ t_within_trial ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4\n",
      "5 5\n",
      "5 6\n",
      "5 7\n",
      "5 8\n",
      "5 9\n",
      "5 10\n",
      "5 11\n",
      "5 12\n",
      "5 13\n",
      "5 14\n",
      "5 15\n",
      "5 16\n",
      "5 17\n",
      "5 18\n",
      "5 19\n",
      "10 0\n",
      "10 1\n",
      "10 2\n",
      "10 3\n",
      "10 4\n",
      "10 5\n",
      "10 6\n",
      "10 7\n",
      "10 8\n",
      "10 9\n",
      "10 10\n",
      "10 11\n",
      "10 12\n",
      "10 13\n",
      "10 14\n",
      "10 15\n",
      "10 16\n",
      "10 17\n",
      "10 18\n",
      "10 19\n",
      "20 0\n",
      "20 1\n",
      "20 2\n",
      "20 3\n",
      "20 4\n",
      "20 5\n",
      "20 6\n",
      "20 7\n",
      "20 8\n",
      "20 9\n",
      "20 10\n",
      "20 11\n",
      "20 12\n",
      "20 13\n",
      "20 14\n",
      "20 15\n",
      "20 16\n",
      "20 17\n",
      "20 18\n",
      "20 19\n",
      "40 0\n",
      "40 1\n",
      "40 2\n",
      "40 3\n",
      "40 4\n",
      "40 5\n",
      "40 6\n",
      "40 7\n",
      "40 8\n",
      "40 9\n",
      "40 10\n",
      "40 11\n",
      "40 12\n",
      "40 13\n",
      "40 14\n",
      "40 15\n",
      "40 16\n",
      "40 17\n",
      "40 18\n",
      "40 19\n",
      "80 0\n",
      "80 1\n",
      "80 2\n",
      "80 3\n",
      "80 4\n",
      "80 5\n",
      "80 6\n",
      "80 7\n",
      "80 8\n",
      "80 9\n",
      "80 10\n",
      "80 11\n",
      "80 12\n",
      "80 13\n",
      "80 14\n",
      "80 15\n",
      "80 16\n",
      "80 17\n",
      "80 18\n",
      "80 19\n",
      "160 0\n",
      "160 1\n",
      "160 2\n",
      "160 3\n",
      "160 4\n",
      "160 5\n",
      "160 6\n",
      "160 7\n",
      "160 8\n",
      "160 9\n",
      "160 10\n",
      "160 11\n",
      "160 12\n",
      "160 13\n",
      "160 14\n",
      "160 15\n",
      "160 16\n",
      "160 17\n",
      "160 18\n",
      "160 19\n",
      "360 0\n",
      "360 1\n",
      "360 2\n",
      "360 3\n",
      "360 4\n",
      "360 5\n",
      "360 6\n",
      "360 7\n",
      "360 8\n",
      "360 9\n",
      "360 10\n",
      "360 11\n",
      "360 12\n",
      "360 13\n",
      "360 14\n",
      "360 15\n",
      "360 16\n",
      "360 17\n",
      "360 18\n",
      "360 19\n",
      "640 0\n",
      "640 1\n",
      "640 2\n",
      "640 3\n",
      "640 4\n",
      "640 5\n",
      "640 6\n",
      "640 7\n",
      "640 8\n",
      "640 9\n",
      "640 10\n",
      "640 11\n",
      "640 12\n",
      "640 13\n",
      "640 14\n",
      "640 15\n",
      "640 16\n",
      "640 17\n",
      "640 18\n",
      "640 19\n",
      "1280 0\n",
      "1280 1\n",
      "1280 2\n",
      "1280 3\n",
      "1280 4\n",
      "1280 5\n",
      "1280 6\n",
      "1280 7\n",
      "1280 8\n",
      "1280 9\n",
      "1280 10\n",
      "1280 11\n",
      "1280 12\n",
      "1280 13\n",
      "1280 14\n",
      "1280 15\n",
      "1280 16\n",
      "1280 17\n",
      "1280 18\n",
      "1280 19\n",
      "2560 0\n",
      "2560 1\n"
     ]
    }
   ],
   "source": [
    "# created by us \n",
    "import gridworld \n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "import rl_functions as rlfun\n",
    "from gridworld_generate_true_MDP import gridworld_true_MDP\n",
    "\n",
    "np.set_printoptions( precision = 2 )\n",
    "\n",
    "# ---------------------- #\n",
    "#   Different Domains    #\n",
    "# ---------------------- #\n",
    "# You can also create your own!  The interpretation of the different symbols is\n",
    "# the following:\n",
    "#\n",
    "# '#' = wall\n",
    "# 'o' = origin grid cell\n",
    "# '.' = empty grid cell\n",
    "# '*' = goal\n",
    "test_maze = [   # HERE: Make this one bigger, probably! \n",
    "    '#########',\n",
    "    '#..#....#',\n",
    "    '#..#..#.#',\n",
    "    '#..#..#.#',\n",
    "    '#..#.##.#',\n",
    "    '#....*#.#',\n",
    "    '#######.#',\n",
    "    '#o......#',\n",
    "    '#########']\n",
    "\n",
    "cliffworld = [\n",
    "    '#######', \n",
    "    '#.....#', \n",
    "    '#.##..#', \n",
    "    '#o...*#',\n",
    "    '#XXXXX#', \n",
    "    '#######']    \n",
    "\n",
    "short_hallway = [   \n",
    "    '###', # '#' = wall\n",
    "    '#o#', # 'o' = origin grid cell\n",
    "    '#.#', # '.' = empty grid cell\n",
    "    '#*#', # '*' = goal\n",
    "    '###']\n",
    "\n",
    "long_hallway = [   \n",
    "    '###', # '#' = wall\n",
    "    '#o#', # 'o' = origin grid cell\n",
    "    '#.#', # '.' = empty grid cell\n",
    "    '#.#', # '.' = empty grid cell\n",
    "    '#.#', # '.' = empty grid cell\n",
    "    '#.#', # '.' = empty grid cell\n",
    "    '#.#', # '.' = empty grid cell\n",
    "    '#.#', # '.' = empty grid cell\n",
    "    '#.#', # '.' = empty grid cell\n",
    "    '#*#', # '*' = goal\n",
    "    '###']\n",
    "\n",
    "simple_grid = [   \n",
    "    '#######', \n",
    "    '#o....#', \n",
    "    '#..X..#', \n",
    "    '#....*#', \n",
    "    '#######']    \n",
    "\n",
    "# ----------------- #\n",
    "#   Key Functions   # \n",
    "# ----------------- #\n",
    "# The policy outputs the action for each states \n",
    "#def policy( state , Q_table , action_count , epsilon ):\n",
    "#    if np.random.random() < epsilon:\n",
    "#        action = np.random.choice( action_count ) \n",
    "#    else: \n",
    "#        action = np.argmax( Q_table[ state , : ] ) \n",
    "#    return action \n",
    "\n",
    "# Takes in count table and updates it\n",
    "def update_count_table( transition_count_table , reward_value_table , state , action , new_state , reward ):\n",
    "    transition_count_table[ state, action, new_state ] += 1\n",
    "    reward_value_table[ state, action, new_state ] = reward\n",
    "    return transition_count_table , reward_value_table \n",
    "\n",
    "# Takes in counts and builds an MDP using the RMAX approach (unseen rewards set\n",
    "# to rmax, use the empirical counts for the transition frequencies)\n",
    "def build_MDP_RMAX( transition_count_table , reward_value_table , rmax , gamma ):\n",
    "    state_count = np.shape( transition_count_table )[0]\n",
    "    action_count = np.shape( transition_count_table )[1]\n",
    "    state_action_observations = np.sum( transition_count_table, axis = 2 )\n",
    "    state_action_observations = np.reshape( state_action_observations, \n",
    "        ( state_count, action_count, 1) )\n",
    "    transition_matrix = transition_count_table / np.tile(\n",
    "            state_action_observations, [ 1, 1, state_count ])\n",
    "    indices_for_unobserved_transition_probabilities = np.tile(\n",
    "            state_action_observations == 0, [ 1, 1, state_count ])\n",
    "    transition_matrix[ indices_for_unobserved_transition_probabilities ] = \\\n",
    "        1/state_count \n",
    "    rewards_matrix = np.copy(reward_value_table)\n",
    "    sas_not_observed = np.sum( transition_count_table, axis = (0,1) ) == 0\n",
    "    rewards_matrix[ sas_not_observed ] = rmax\n",
    "    MDP = {\n",
    "        'T' : transition_matrix,\n",
    "        'R' : rewards_matrix,\n",
    "        'gamma' : gamma,\n",
    "        'state_count' : state_count,\n",
    "        'action_count' : action_count}\n",
    "    return MDP\n",
    "\n",
    "# Takes in counts and samples and MDP \n",
    "#def sample_MDP( transition_count_table , reward_value_table , Dirichlet_alpha , default_reward ):\n",
    "#    state_count = np.shape( transition_count_table )[0]\n",
    "#    action_count = np.shape( transition_count_table )[1]\n",
    "#    state_action_observations = np.sum( transition_count_table, axis = 2 )\n",
    "#    state_action_observations = np.reshape( state_action_observations, \n",
    "#        ( state_count, action_count, 1) )\n",
    "#    transition_matrix = np.zeros( np.shape( transition_count_table ) )\n",
    "#    for state_ind in range( state_count ):\n",
    "#        for action_ind in range( action_count ):\n",
    "#            transition_matrix[ state_ind, action_ind, : ] = \\\n",
    "#                np.random.dirichlet( transition_count_table[ state_ind, action_ind, : ] + \\\n",
    "#                Dirichlet_alpha )\n",
    "#    rewards_matrix = np.copy(reward_value_table)\n",
    "#    sas_not_observed = np.sum( transition_count_table, axis = (0,1) ) == 0\n",
    "#    rewards_matrix[ sas_not_observed ] = default_reward\n",
    "#    MDP = {\n",
    "#        'T' : transition_matrix,\n",
    "#        'R' : rewards_matrix,\n",
    "#        'gamma' : gamma,\n",
    "#        'state_count' : state_count,\n",
    "#        'action_count' : action_count}\n",
    "#    return MDP\n",
    "\n",
    "# Solve MDP\n",
    "def solve_MDP( MDP ):\n",
    "    state_count = MDP['state_count']\n",
    "    action_count = MDP['action_count']\n",
    "    T = MDP['T']\n",
    "    R = MDP['R']\n",
    "    gamma = MDP['gamma']\n",
    "    Q_table = np.zeros( ( state_count, action_count ) )\n",
    "    V = np.zeros( state_count )\n",
    "    for iter_number in range(200):\n",
    "        for state_ind in range( state_count ):\n",
    "            for action_ind in range( action_count ):\n",
    "                expected_reward = np.sum(T[ state_ind, action_ind, : ] * \\\n",
    "                    R[ state_ind, action_ind, : ] )\n",
    "                expected_value_of_next_state = np.sum( T[ state_ind, action_ind, : ] * V )\n",
    "                Q_table[ state_ind, action_ind ] = expected_reward + \\\n",
    "                    gamma * expected_value_of_next_state\n",
    "            V[ state_ind ] = np.max( Q_table[ state_ind, :] )\n",
    "    return Q_table\n",
    "\n",
    "# -------------------- #\n",
    "#   Create the Task    #\n",
    "# -------------------- #\n",
    "# Task Parameters for gridworld \n",
    "task_name = cliffworld\n",
    "action_error_prob = 0.2\n",
    "pit_reward = -50\n",
    "task = gridworld.GridWorld( task_name ,\n",
    "                            action_error_prob=action_error_prob, \n",
    "                            rewards={'*': 50, 'moved': -1, 'hit-wall': -1,'X':pit_reward} ,\n",
    "                            terminal_markers='*' )        \n",
    "\n",
    "gamma = 0.95\n",
    "state_count = task.num_states  \n",
    "action_count = task.num_actions \n",
    "true_MDP = gridworld_true_MDP( task_name, action_error_prob, pit_reward, gamma )\n",
    "Q_table = solve_MDP( true_MDP )\n",
    "\n",
    "# -------------- #\n",
    "#   Make Plots   #\n",
    "# -------------- #\n",
    "# Note, these are plots that are useful for visualizing the policies\n",
    "# and the value functions, which can help you identify bugs.  You can\n",
    "# also use them as a starting point to create the plots that you will\n",
    "# need for your homework assignment.\n",
    "\n",
    "# Util to make an arrow \n",
    "# The directions are [ 'north' , 'south' , 'east' , 'west' ] \n",
    "def plot_arrow( location , direction , plot ):\n",
    "\n",
    "    arrow = plt.arrow( location[0] , location[1] , dx , dy , fc=\"k\", ec=\"k\", head_width=0.05, head_length=0.1 )\n",
    "    plot.add_patch(arrow) \n",
    "\n",
    "# Useful stats for the plot\n",
    "row_count = len( task_name )\n",
    "col_count = len( task_name[0] ) \n",
    "value_function = np.reshape( np.max( Q_table , 1 ) , ( row_count , col_count ) )\n",
    "policy_function = np.reshape( np.argmax( Q_table , 1 ) , ( row_count , col_count ) )\n",
    "wall_info = .5 + np.zeros( ( row_count , col_count ) )\n",
    "wall_mask = np.zeros( ( row_count , col_count ) )\n",
    "for row in range( row_count ):\n",
    "    for col in range( col_count ):\n",
    "        if task_name[row][col] == '#':\n",
    "            wall_mask[row,col] = 1     \n",
    "wall_info = ma.masked_where( wall_mask==0 , wall_info )\n",
    "value_function *= (1-wall_mask)**2\n",
    "\n",
    "# value function plot \n",
    "plt.subplot( 1 , 2 , 2 ) \n",
    "plt.imshow( value_function , interpolation='none' , cmap=matplotlib.cm.jet )\n",
    "plt.colorbar()\n",
    "plt.imshow( wall_info , interpolation='none' , cmap=matplotlib.cm.gray )\n",
    "plt.title( 'Value Function' )\n",
    "\n",
    "# policy plot \n",
    "# plt.imshow( 1 - wall_mask , interpolation='none' , cmap=matplotlib.cm.gray )    \n",
    "for row in range( row_count ):\n",
    "    for col in range( col_count ):\n",
    "        if wall_mask[row][col] == 1:\n",
    "            continue \n",
    "        if policy_function[row,col] == 0:\n",
    "            dx = 0; dy = -.5\n",
    "        if policy_function[row,col] == 1:\n",
    "            dx = 0; dy = .5\n",
    "        if policy_function[row,col] == 2:\n",
    "            dx = .5; dy = 0\n",
    "        if policy_function[row,col] == 3:\n",
    "            dx = -.5; dy = 0\n",
    "        plt.arrow( col , row , dx , dy , shape='full', fc='w' , ec='w' , lw=3, length_includes_head=True, head_width=.2 )\n",
    "plt.title( 'Policy' )        \n",
    "plt.show( block=False ) \n",
    "\n",
    "# -------------------------- #\n",
    "# Off-policy evaluation part #    \n",
    "# -------------------------- #\n",
    "\n",
    "episode_count__list = [ 5, 10, 20, 40, 80, 160, 360, 640, 1280, 2560 ]\n",
    "num_of_tests = 20\n",
    "epsilon = 0.2\n",
    "pi_optimal = np.argmax( Q_table, axis = 1 )\n",
    "pi_optimal = rlfun.turn_policy_to_stochastic_policy(\n",
    "    pi_optimal, num_of_states = state_count, num_of_actions = action_count )\n",
    "pi_random  = np.ones( (state_count, action_count) ) / action_count\n",
    "pi_eps_greedy = pi_optimal * ( 1 - epsilon ) + epsilon / 4\n",
    "pi_behavior = pi_eps_greedy\n",
    "pi_eval = pi_optimal\n",
    "#pi_behavior = pi_random\n",
    "#pi_eval = pi_random\n",
    "\n",
    "\n",
    "IS__list__list = []\n",
    "PDIS__list__list = []\n",
    "PDDR__list__list = []\n",
    "WIS__list__list = []\n",
    "PDWIS__list__list = []\n",
    "PDWDR__list__list = []\n",
    "\n",
    "max_task_iter = 100\n",
    "rmax = task.get_max_reward()\n",
    "\n",
    "for episode_count in episode_count__list:\n",
    "    IS__list = []\n",
    "    PDIS__list = []\n",
    "    PDDR__list = []\n",
    "    WIS__list = []\n",
    "    PDWIS__list = []\n",
    "    PDWDR__list = []\n",
    "    for test_count in range(num_of_tests):\n",
    "        global_task_iter = 0\n",
    "        states_sequence = []\n",
    "        actions_sequence = []\n",
    "        rewards_sequence = []\n",
    "        fence_posts = []\n",
    "        transition_count_table = np.zeros( ( state_count , action_count , state_count ) )\n",
    "        reward_value_table = np.zeros( ( state_count , action_count , state_count ) )\n",
    "        print(episode_count, test_count)\n",
    "        # Loop until the episode is done \n",
    "        for episode_iter in range( episode_count ):\n",
    "            fence_posts += [ global_task_iter ]\n",
    "            # Start the task \n",
    "            task.reset()\n",
    "            state = task.observe() \n",
    "            action = np.random.choice( 4, 1, p = pi_behavior[ state, : ] )[0]\n",
    "            episode_reward_list = []\n",
    "            task_iter = 0 \n",
    "        \n",
    "            # Loop until done\n",
    "            while task_iter < max_task_iter:\n",
    "                global_task_iter += 1\n",
    "                task_iter = task_iter + 1 \n",
    "                new_state, reward = task.perform_action( action )\n",
    "                new_action = np.random.choice( 4, 1, p = pi_behavior[ new_state, : ] )[0] \n",
    "        \n",
    "                # Cases for the different algorithms \n",
    "                transition_count_table , reward_value_table = update_count_table( transition_count_table , reward_value_table , state , action , new_state , reward )\n",
    "        \n",
    "                # store the data\n",
    "                episode_reward_list.append( reward ) \n",
    "                \n",
    "                states_sequence += [ state ]\n",
    "                actions_sequence += [ action ]\n",
    "                rewards_sequence += [ reward ]\n",
    "\n",
    "                # stop if at goal/else update for the next iteration \n",
    "                if task.is_terminal( state ):\n",
    "                    break\n",
    "                else:\n",
    "                    state = new_state\n",
    "                    action = new_action\n",
    "            \n",
    "                    \n",
    "        MDP = build_MDP_RMAX( transition_count_table , reward_value_table , rmax , gamma )\n",
    "        V, Q = rlfun.policy_evaluation(\n",
    "                MDP['T'] , MDP['R'] , pi_eval , gamma , theta = 0.01 )\n",
    "        \n",
    "        IS, _ = rlfun.off_policy_importance_sampling(\n",
    "            states_sequence, actions_sequence, rewards_sequence,\n",
    "            fence_posts, gamma, pi_eval, pi_behavior, state_count,\n",
    "            action_count)\n",
    "        PDIS, _ = rlfun.off_policy_per_decision_importance_sampling(\n",
    "            states_sequence, actions_sequence, rewards_sequence,\n",
    "            fence_posts, gamma, pi_eval, pi_behavior, state_count,\n",
    "            action_count)\n",
    "        PDDR, _ = rlfun.off_policy_per_decision_doubly_robust(\n",
    "            states_sequence, actions_sequence, rewards_sequence,\n",
    "            fence_posts, gamma, pi_eval, pi_behavior, V, Q, state_count,\n",
    "            action_count)\n",
    "        WIS, ite = rlfun.off_policy_weighted_importance_sampling(\n",
    "            states_sequence, actions_sequence, rewards_sequence,\n",
    "            fence_posts, gamma, pi_eval, pi_behavior, state_count,\n",
    "            action_count)\n",
    "        PDWIS, ite = rlfun.off_policy_per_decision_weighted_importance_sampling(\n",
    "            states_sequence, actions_sequence, rewards_sequence,\n",
    "            fence_posts, gamma, pi_eval, pi_behavior, state_count,\n",
    "            action_count)\n",
    "        PDWDR, ite = rlfun.off_policy_per_decision_weighted_doubly_robust(\n",
    "            states_sequence, actions_sequence, rewards_sequence,\n",
    "            fence_posts, gamma, pi_eval, pi_behavior, V, Q, state_count,\n",
    "            action_count)\n",
    "        IS__list += [ IS ]\n",
    "        PDIS__list += [ PDIS ]\n",
    "        PDDR__list += [ PDDR ]\n",
    "        WIS__list += [ WIS ]\n",
    "        PDWIS__list += [ PDWIS ]\n",
    "        PDWDR__list += [ PDWDR ]\n",
    "    IS__list__list += [ IS__list ]\n",
    "    PDIS__list__list += [ PDIS__list ]\n",
    "    PDDR__list__list += [ PDDR__list ]\n",
    "    WIS__list__list += [ WIS__list ]\n",
    "    PDWIS__list__list += [ PDWIS__list ]\n",
    "    PDWDR__list__list += [ PDWDR__list ]\n",
    "    \n",
    "IS__list__list = np.asarray(IS__list__list)\n",
    "PDIS__list__list = np.asarray(PDIS__list__list)\n",
    "PDDR__list__list = np.asarray(PDDR__list__list)\n",
    "WIS__list__list = np.asarray(WIS__list__list)\n",
    "PDWIS__list__list = np.asarray(PDWIS__list__list)\n",
    "PDWDR__list__list = np.asarray(PDWDR__list__list)\n",
    "\n",
    "V__true, Q__true = rlfun.policy_evaluation(\n",
    "                true_MDP['T'] , true_MDP['R'] , pi_eval , gamma , theta = 0.01 )\n",
    "actual_policy_value = V__true[ task.maze.flat_positions_containing('o')[0] ]\n",
    "plt.figure()\n",
    "plt.errorbar( episode_count__list, np.mean(IS__list__list, axis = 1 ),\n",
    "             yerr = np.std(IS__list__list, axis = 1 ) )\n",
    "plt.errorbar( episode_count__list, np.mean(PDIS__list__list, axis = 1 ),\n",
    "             yerr = np.std(PDIS__list__list, axis = 1 ) )\n",
    "plt.errorbar( episode_count__list, np.mean(PDDR__list__list, axis = 1 ),\n",
    "             yerr = np.std(PDDR__list__list, axis = 1 ) )\n",
    "plt.errorbar( episode_count__list, np.mean(WIS__list__list, axis = 1 ),\n",
    "             yerr = np.std(WIS__list__list, axis = 1 ) )\n",
    "plt.errorbar( episode_count__list, np.mean(PDWIS__list__list, axis = 1 ),\n",
    "             yerr = np.std(PDWIS__list__list, axis = 1 ) )\n",
    "plt.errorbar( episode_count__list, np.mean(PDWDR__list__list, axis = 1 ),\n",
    "             yerr = np.std(PDWDR__list__list, axis = 1 ) )\n",
    "plt.plot(episode_count__list, actual_policy_value*np.ones(\n",
    "        len(episode_count__list)))\n",
    "plt.xscale('log')\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('estimator value')\n",
    "\n",
    "IS__squared_error = (IS__list__list - actual_policy_value)**2\n",
    "PDIS__squared_error = (PDIS__list__list - actual_policy_value)**2\n",
    "PDDR__squared_error = (PDDR__list__list - actual_policy_value)**2\n",
    "WIS__squared_error = (WIS__list__list - actual_policy_value)**2\n",
    "PDWIS__squared_error = (PDWIS__list__list - actual_policy_value)**2\n",
    "PDWDR__squared_error = (PDWDR__list__list - actual_policy_value)**2\n",
    "plt.figure()\n",
    "plt.plot( episode_count__list, np.mean(IS__squared_error, axis = 1 ), label = 'IS' )\n",
    "plt.plot( episode_count__list, np.mean(PDIS__squared_error, axis = 1 ), label = 'PDIS' )\n",
    "plt.plot( episode_count__list, np.mean(PDDR__squared_error, axis = 1 ), label = 'PDDR' )\n",
    "plt.plot( episode_count__list, np.mean(WIS__squared_error, axis = 1 ), label = 'WIS' )\n",
    "plt.plot( episode_count__list, np.mean(PDWIS__squared_error, axis = 1 ), label = 'PDWIS' )\n",
    "plt.plot( episode_count__list, np.mean(PDWDR__squared_error, axis = 1 ), label = 'PDWDR' )\n",
    "#plt.plot( episode_count__list, 50*(np.array(episode_count__list)+0.0)**-1, 'm' )\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend( loc = 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
